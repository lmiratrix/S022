---
title: Reflections on using machine learning (for final projects)
author: "Miratrix"
date: "2023-03-17"
output: pdf_document
---

```{r setup, include=FALSE}
library( tidyverse )

knitr::opts_chunk$set(echo = TRUE, 
                      fig.width = 5,
                      fig.height = 3,
                      out.width = "75%", 
                      fig.align = "center")
options(list(dplyr.summarise.inform = FALSE))
theme_set( theme_classic() )

library( glmnet )
library( caret )
library( dataedu )
```


# Test/train splits, test/train/validate, and cross validation

It is important to track what your machine learning methods are doing in terms of spliting your data inside the call.
For example, the `caret` package's `train()` method will often be doing cross validation (or something similar) inside of it.
You thus do not need to always do a test/train split outside of it!

Also, don't do a test/train split if you don't have a lot of data. It is too expensive! This is where cross validation is particularly useful, since it lets all of your data be used for testing, and it uses most of your data for fitting your models.
It is kind of like a "have your cake and eat it too" situation.

Finally, the full test/train/validate trifecta is only if you are really, really concerned about predicting your future performance of a model.
It turns out that if you use cross-validation to select the best set of tuning parameters, the final estimated accuracy from the cross-validation will generally be pretty close to what a final validation set would tell you.

Let's see how the case study in [Data Science in Ed, Chapter 14](https://datascienceineducation.com/c14.html) went about things.
They first split the data into a training set and a testing set.
They then made a grid of tuning parameters related to tree size and so forth, and then used `train()` to identify what set of tuning parameters was best for the data.
In particular, `train()` used this grid and a variant of cross validation (bootstrap resampling) to repeatedly divide the training data into a part used to fit one random forest for each tuning parameter combo and a part used to evaluate all of those random forests on out of sample data.
The final table produced gives the estimated error rates, and finally `train()` selects the tuning parameter combo with the best rates.

The book then took the final model (via `rf_fit2$finalModel`), which is a last model fit by `train()` to all the training data using the best found parameter combo, as their answer.
To evaluate the quality of their answer, they used `finalModel` to predict outcomes for the test data they set aside at the beginning of their case study.

In our language, their "test" data was used for the final validation step, and the `train()` method was doing little test/train splits inside of it to get the estimates of out of sample error as part of the tuning process.

Note that they also found the predicted error of the finally selected final model was about the same as reported by the out of sample error by `train()`: this is not unusual.
If we are not fitting too many different combos of tuning parameter and so forth, then the estimated error from this process will often be close.

## Cross validation options for caret

The `caret` package does cross validation internally as part of `train()`, if you tell it.
If you don't, it does a different kind of internal repeated test/train splitting that it calls bootstrap.
This is not bootstrap for inference!
What it is doing is resampling the data with replacement to get a training dataset of the same size as it was passed.
It will then have about a third of the data not in the training set, and it will use this for out-of-sample testing to estimate the performance of all the models fit.
Finally, it repeats this a bunch of times and averages: this means each observation will be in most training sets, but will be used for testing some of the time.
This is just like cross validation (except more random)!

Also, `caret`'s cross-validation is, by default, a random cross-validation: repeatedly take 10% of the data as testing, and use the rest for training.
Repeat 10 times.
This means each observation will be used about once for testing and about nine times for training, but not exactly.
Functionally, this will generally be nearly the same as the classic CV where we divide all the data into 10 parts systematically.

It basically does not matter which form of cross-validation or splitting you use.
The number of iterations will impact running time: 25 iterations will be 2.5 times longer to run than 10!



## Bootstrap vs. Cross validation
Don't confuse bootstrapping with cross-validiation.
Bootstrapping is a way of doing statistical inference: you use it to ask how much an estimate would change if you happened to get a different data set from the same source.
This allows you to decide if, for example, a coefficient is "really" positive--if your bootstrapping doesn't really give you any negative estimates, then you can be sure that your estimate is probably not positive due to random chance.

Cross validation, by contrast, is a way of doing a lot of test/train splits because you want to know how a fit model would work on new data.
It is focusing on estimating future predictive accuracy, not statistical inference.







# Determining what variables are important

The Lasso is a sparse regression approach: as part of fitting a lasso model, you are given a subset of your original variables deemed important enough to include in the predictive model.
Random forests, by contrast, allow for a second step of generating a variable importance plot, where you get a measure of how useful each variable was for making predictions.

Both these tools can be usefor for identifying factors particularly tied to your outcome.
In general, you would expect those variables kept by Lasso to also have high scores in a variable importance plot.
Both approaches, however, have some caveats that one should think about when interpreting results.

First, the Lasso approach gives you a final complete model built out of only a few variables. Each kept variable comes with a coefficient, and some of those coefficients will be tiny and others larger.
Before directly comparing coefficients, however, spend a moment to think about how the scale of each variable matters.

For example, in the following fake dataset each variable (other than the last) has the same impact on the outcome.
Put another way, they are all about the same in terms of their correlation with the outcome, as shown by the last column of the output.

```{r}
library( glmnet )

tb = data.frame( X1 = rnorm( 1000 ),
                 X2 = rnorm( 1000, sd = 10 ),
                 X3 = rnorm( 1000, sd = 100 ),
                 X4 = rnorm( 1000 ),
                 X5 = rnorm( 1000 ) )
tb$X4_proxy = tb$X4 + rnorm( 1000, sd=0.2 )
tb$Y = with(tb, X1 + 0.1 * X2 + 0.01 * X3 + X4 + rnorm( 1000 ) )

cor( tb ) %>%
    knitr::kable( digits=2 )
```

We can fit a Lasso model as so:
```{r}
Xmat = model.matrix( Y ~ ., data=tb )

mod <- cv.glmnet( x = Xmat[,-1], y = tb$Y )

coef( mod )
```
Note how we have the proxy, a little bit, and also X3.  The proxy looks more important than X3 (but actually isn't important).
X3 looks unimportant because the scale of X3 is so large.
One fix is to standardize your covariates before putting them into your lasso:
```{r}
tb2 <- tb %>% 
    mutate( X2 = scale( X2 ),
            X3 = scale( X3 ) )
Xmat2 = model.matrix( Y ~ ., data=tb )
mod2 <- cv.glmnet( x = Xmat2[,-1], y = tb2$Y )
coef( mod2 )
```

<!--
```{r}
final_fit = glmnet( Xmat, tb$Y ) 
plot( final_fit )
```
-->

If we use random forests and a variable importance plot, we get this:
```{r, cache=TRUE}
set.seed(2020)
library( caret )

rf_fit2_imp <- train( Y ~ .,
        data = tb,
        method = "ranger",
        importance = "permutation" )

varImp(rf_fit2_imp) %>%
    pluck(1) %>%
    rownames_to_column("var") %>%
    ggplot(aes(x = reorder(var, Overall), y = Overall)) +
    geom_col(fill = dataedu_colors("darkblue")) +
    coord_flip() +
    theme_dataedu()
```

The variable importance plot correctly puts X1, X2, and X3 all as about the same importance (they are: the larger variance perfectly balances out the smaller coefficient).  The `X4` variable gets dinged a bit because the `X4_proxy` variable can serve almost as well for prediction, so they both are deemed important.




# Sensitivity check on trees
Say you want to present a tree in your final project to show how an outcome relates to different variables of interest.
Trees, unfortunately, are notoriously unstable and thus you should see if you would get a very different tree with slight changes to your data.

Fortunately, it is easy to check this with the following: 

Once you have your final tuning parameters and so forth set, repeat the following 5 times:
    * Bootstrap your data.
    * Fit a tree to the bootstrapped data, using the tuning parameters you selected.
    * Make a plot of your bootstrapped tree.
    
Now check to see if your five trees are basically similar, or entirely different, from each other. This is called a _stability check_ and is a useful way of seeing if your results are particularly sensitive to mild changes in data.





# A cool and easy way to do causal inference type stuff

Consider the problem of comparing Lyft vs Uber prices given a dataset of rides.  You can't just compare the average price in each type because it is possible that, for example, Lyft rides are systematically shorter than Uber.

One direction to take is to fit two random forests, one for your Lyft data and one for your Uber data, to predict price given distance, time, weather, starting location, and anything else you have as a covariate.
You then use each random forest to predict price for each observation in your entire dataset.
You now have two predictions, one from your Lyft model and one from your Uber model, for each observation.
Finally, you compare to see if the predictions are different on average.
This gives you a "controlled comparison" where you are asking if there are systematic shifts in cost for otherwise similar things.

<!--An alternate direction to take is to fit a random forest predicting price with all of your EXCEPT ride agency. Then calculate the residuals of all of your observations, and see if your residuals are systematically different between Lyft and Uber.
This "subtracts out" the systematic effect of price, but 
-->



