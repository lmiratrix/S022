[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Handouts for S022 (Data Science in Education)",
    "section": "",
    "text": "Preface\nThis “book” is basically a set of handouts generated for a Data Science in Education course taught at the Harvard Graduate School of Education by Luke Miratrix. They are loosely organized, but the primary purpose of this “book” is to make all the handouts easy to access and find.\nThe handouts were generated in response to student questions, and aim to short-circuit staring at the possibly confusing world of Stack Overflow. They are curated in the sense that these activities tend to come up over and over in the course of fundamental data science work.\nI hope you find this resource useful. Please send feedback to lmiratrix@g.harvard.edu.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#a-note-on-duplicated-material",
    "href": "index.html#a-note-on-duplicated-material",
    "title": "Handouts for S022 (Data Science in Education)",
    "section": "A note on duplicated material",
    "text": "A note on duplicated material\nSome of this material is taken from a sister “textbook” for my MLM course (S043/Stat151). That textbook is here. In particular, you will find entire chapters of that textbook replicated here.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-statistical-programming",
    "href": "index.html#about-statistical-programming",
    "title": "Handouts for S022 (Data Science in Education)",
    "section": "About Statistical Programming",
    "text": "About Statistical Programming\nStatistical programming entails using a computer to analyze data in a way where you design and control the analysis and sometimes aspects of data generation (simulation). This is a step (or possibly several steps) beyond simply running built-in commands and interpreting output. It includes cleaning and transforming data to suit your needs, designing visualizations and plots to illustrate points you want to make, and conducting analyses that are tailor-made to your questions of interest.\nS022 will make extensive use of the statistical software package R, which runs on both PCs and Macs. The software is free and available online. R is straightforward to learn, but is sufficiently powerful and versatile to be useful for real projects that you might carry out after this course. It is used widely in many other statistics courses and also in research in such fields as education, psychology, economics, medical research, epidemiology, public health, and political science. See below for further argument as to why you should learn R.\nWe highly recommend using RStudio, which makes using R easier. RStudio is an Integrated Development Environment (IDE) that structures your experience, helps keep things organized, and offers multiple time-saving features to make your programming experience better. You might also consider R Markdown. R Markdown allows for generating documents with embedded R code and R output in a clean format, which can greatly help report generation.\nWe will teach R and the associated tools as part of this course. We assume no background knowledge with statistical programming or any other programming.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-r-mystique",
    "href": "index.html#the-r-mystique",
    "title": "Handouts for S022 (Data Science in Education)",
    "section": "The R Mystique",
    "text": "The R Mystique\nMany people seem to believe that R is particularly technically challenging and difficult to master. This probably stems from its extreme flexibility; it is a fully functional programming language as well as a statistical analysis package. R can do things that many other software packages (I’m looking at you, Stata) essentially can’t. These more involved things are frequently hard to do because they require you to think like a programmer rather than a data analyst. As a result, R is perceived as a “hard” language to use.\nHowever, for straightforward, off-the-shelf analyses, R is arguably easier to learn and use in many ways than programs such as Stata or SAS. This is especially true when R is combined with RStudio for a better programming experience.\nDon’t believe me? Take S022, have us teach you some R, and find out for yourself.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "self_assess.html",
    "href": "self_assess.html",
    "title": "Do I take S-022? A self-assessment",
    "section": "",
    "text": "A self-assessment quiz\nTrying to figure out how S022 will be for you? ​Add up the points across the following categories:",
    "crumbs": [
      "Do I take S-022? A self-assessment"
    ]
  },
  {
    "objectID": "self_assess.html#a-self-assessment-quiz",
    "href": "self_assess.html#a-self-assessment-quiz",
    "title": "Do I take S-022? A self-assessment",
    "section": "",
    "text": "Work Experience\n+0        I have no work experience with quantitative data, or\n+1        I have some work experience with quantitative data, or\n+2        I have substantial work experience with quantitative data\n\n\nStat Courses\n-2        I have no prior stat experience under my belt, or\n+1        I have taken a very intro stat course (e.g., S012) or have a little stat knowledge, or\n+3        I have taken a linear regression course (e.g., S030, S040), or\n+5        I have taken an intermediate or advanced stats course (e.g., S052 or beyond)\n+1 bonus if concurrently enrolling in S052\n+1 bonus if prior quantatative classes were comfortable for you\n\n\nProgramming\n+0        I have basically no experience doing computer programming, or\n+1        I have some experience doing computer programming, or\n+2        I have a lot of experience doing computer programming\n\n\nR Skills\n+0        I have no real experience with R, or\n+1        I have a small bit of experience with R (e.g., ran scripts of it in S040), or\n+2        I have some experience with R (e.g., played around with scripts a bit in S040), or\n+3        I have substantial experience with R (e.g., write my own R code for my work)\n+1 bonus if learning R has been comfortable for you\n+1 bonus if you are comfortable with something like STATA\n\n\nMath\n+0        I don’t recall much math from my past education, or\n+1        I have taken (and somewhat remember) Calculus, or\n+2        I have taken classes beyond Calculus\n+1 bonus if classes were comfortable for you\n\n\nOther\n+2        I am content with getting a B or taking the class SAT/UNSAT",
    "crumbs": [
      "Do I take S-022? A self-assessment"
    ]
  },
  {
    "objectID": "self_assess.html#total-the-above",
    "href": "self_assess.html#total-the-above",
    "title": "Do I take S-022? A self-assessment",
    "section": "Total the above",
    "text": "Total the above\nA VERY ROUGH recommendation is:\n&lt; 4:      Danger! Please email the instructor to talk about how this might go for you.\n4-6:      It will be really hard! You could take this course, but will likely find it will take much more time than a typical course. We would support you through this, but be warned that this could feel like a lot to take on.\n7-8:      It will be hard! There are lots of folks like you who are taking this course. The course will likely be a fair bit of work and could feel confusing/overwhelming at times. We would support you through this. At the end you will have learned a lot if you stick with it.\n9-12:     It will probably feel like a normal course. No reservations. You can either work a reasonable amount and learn a lot about data science, or dig deeper to really go far with the skills we cover.\n13+:     It will probably be a cake-walk. You will learn some concepts but not have to work particularly hard in this course. We would still love to have you!\nStudents have historically said this course teaches you a lot; the question is just whether you have the time to allocate for the course. This quiz helps assess the time.",
    "crumbs": [
      "Do I take S-022? A self-assessment"
    ]
  },
  {
    "objectID": "assignment_guidelines.html",
    "href": "assignment_guidelines.html",
    "title": "Assignment Formatting Guidelines",
    "section": "",
    "text": "Formatting",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "assignment_guidelines.html#formatting",
    "href": "assignment_guidelines.html#formatting",
    "title": "Assignment Formatting Guidelines",
    "section": "",
    "text": "Start each question on a new page.\nProvide at least a brief title for the question or sub-question along with the question number. E.g. “(a). Association between graduation rates and school type.” You can copy the entire prompt for your future reference if you want, but abbreviated prompts are fine with us!\nUse headings or other means to highlight problem titles (e.g., bold, italic, etc.).\nMake sure you answer all parts of each question, and do so under the proper labeled subpart. For each question or sub-question, include any R code, R output and answer.\nUse different fonts/formatting for your R code, R output and answers and use font formatting consistently throughout each assignment.\nUse single line spacing and normal 1-inch margins.\nInclude page numbers.\nLet people say of your work: “There is no bombast, no similes, flowers, digressions, or unnecessary descriptions. Everything tends directly to the catastrophe.” —Horace Walpole, The Castle of Otranto",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "assignment_guidelines.html#numbers",
    "href": "assignment_guidelines.html#numbers",
    "title": "Assignment Formatting Guidelines",
    "section": "Numbers",
    "text": "Numbers\n\nRound your final answers, not intermediary steps.\nPut a zero in front of a decimal place (e.g. 0.2 instead of .2). This is optional for bounded numbers (e.g., p-values or proportions).\nRound to the nearest meaningful digit. What this means is a little hard to say, and different people can have different standards. However, if you have a statistic with a standard error of 1, it’s not meaningful to report decimals because the estimate simply isn’t precise enough to estimate numbers that small. Similarly, if you’re reporting average salaries, it’s usually not meaningful to report past the hundreds place regardless of your precision, because tens of dollars are too small to matter. Because this is so imprecise, we’ll give a lot of lee-way, but spurious precision will annoy. If you’re not sure what this means in practice, feel free to ask a TF.\nFormat your p-values! Round p-values to 3 places, and never report a p-value of 0. Instead say, for example, “p &lt; 0.001” or “p &lt; 10^-r” for some r.\n“Numbers have life; they’re not just symbols on paper.” —Shakuntala Devi",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "assignment_guidelines.html#plots",
    "href": "assignment_guidelines.html#plots",
    "title": "Assignment Formatting Guidelines",
    "section": "Plots",
    "text": "Plots\n\nMake sure your plots are well labelled, including title, axis, legends and any other elements you choose to include.\nYour plots should be self-explanatory.\nInclude notes and captions as necessary.\nTry to make plots easier to compare when you have multiple plots. For example, it is nice to have the same \\(X\\)-axis bounds if giving two histograms.\nDo not include best fit lines unless you have some reason, e.g. a significant \\(p\\)-value or a scientific basis for understanding an association.\n“Above all else show the data.” —Edward Tufte",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "assignment_guidelines.html#avoiding-bias",
    "href": "assignment_guidelines.html#avoiding-bias",
    "title": "Assignment Formatting Guidelines",
    "section": "Avoiding bias",
    "text": "Avoiding bias\n\nUse plural phrases, nouns or pronouns, e.g. “children and their toys” for “a child and his toy.” You may use the singular “they” pronoun (“a child and their toy”), but be warned that broader academic communities are still in flux regarding this usage.\nTry to avoid biased forms of language concerning race, gender, disability and sexuality. Walden University has a reasonable list of case studies to consider on this topic. The APA also has a guide for writing about race.\n“I have yet to see a piece of writing, political or non-political, that does not have a slant. All writing slants the way a writer leans, and no man is born perpendicular.” — E.B. White",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "assignment_guidelines.html#conciseness",
    "href": "assignment_guidelines.html#conciseness",
    "title": "Assignment Formatting Guidelines",
    "section": "Conciseness",
    "text": "Conciseness\n\n“Brevity is the soul of wit.” —Hamlet",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "self_grading.html",
    "href": "self_grading.html",
    "title": "Self Grading Instructions",
    "section": "",
    "text": "Acknowledgement:\nThis idea of self-grading was inspired by Cora Wigger. She used to have some stuff on twitter, but she appears to have removed her account.",
    "crumbs": [
      "Self Grading Instructions"
    ]
  },
  {
    "objectID": "resources_on_learning_R.html",
    "href": "resources_on_learning_R.html",
    "title": "Resources on Learning R",
    "section": "",
    "text": "To get help, try to stick with this textbook as there are so many ways of doing things in R that you can be led into strange lands that look very different from the land of S022. This index may help you find certain helpful documents (chapters) in this textbook quickly. It also lists other places you might go for help.\nSo, where can you get help with R?\n\nLive Class Code and Sections\nThe labs and classes usually have R scripts that shows how to do the stuff from that class. See the Packets for this code. Sections will typically have a hands-on component which will give you a chance to try things out yourself. These sections will also publish the final R code for future reference. See the section pages to get this information.\n\n\nR for Data Science: The Very Important Online Textbook\nSee R for Data Science (2e).. This textbook provides important information on wrangling data, making plots, and doing statistical programming. It is full of examples and code snippets you can steal. Note that we are using the second edition of this textbook (2e).\nThere are solutions to this guide. See https://mine-cetinkaya-rundel.github.io/r4ds-solutions/.. It can be quite useful to do the exercises in this text!\nA supplementary textbook Also check out Modern Dive’s “Statistical Inference via Data Science”, especially for some beginning R information in Chapter 1.\n\n\nOffice Hours\nOffice hours are fine time to get help troubleshooting a specific or script you are working on.\n\n\nGSE Stat Help Desk\nEducation students can write to stathelp@gse.harvard.edu to get help getting started with R. If groups of 3 or more want some tutorials, they can ask for them as well.\nThey sometimes have intro to R workshops. For example, grab some old workshop materials here:http://its.gse.harvard.edu/gentle-introduction-r\n\n\nR Cheat Sheets\nR Cheat Sheets are one page sheets that give lots of R commands for specific tasks. VERY USEFUL! See https://rstudio.com/resources/cheatsheets/ for a full compendium of cheat sheets for work in RStudio, such as the ggplot sheet.\nWe’ve covered/touched on a lot of the cheat sheet topics in class or section like using the dplyr package, R-Markdown, or RegEx, but there are also things that we haven’t discussed, such as creating survival plots in ggplot or developing a web-based app.\n\n\nSelected Handouts\nWARNING: The following links have not yet been updated, but all chapters should be in the textbook\nChapter 11  Basic Data Manipulation with tidyverse – a quick review of basic data manipulation. This chapter shows the core tidyverse dplyr commands for manipulating data.\nCoding Style Guide This gives advice on how to format your code in a consistent manner. style_guide.pdf\nSimple Plot Control simple_plotting_tips.pdf (https://canvas.harvard.edu/courses/152037/files/21404225/download) A handout on making nice plots, controlling their size, some simple make it beautiful advice, and saving them at a high resolution!\nPrediction and Plotting Using predict() with a fit model on fake data that has systematically spaced values can be a nice road for beautiful plots. prediction_and_plotting.pdf\nAggregation and Plotting Aggregate data and make plots. Illustrates some data wrangling to get data ready. pivot_and_line_plotting.pdf\nDoing things over and over (map, rerun, replicate) Useful in particular for implementing resampling methods like the bootstrap, or doing tasks repeatidly, like web scraping. doing_things_over_and_over.pdf\nData cleaning excel files This “handout” (actually an R script) loads in a csv file made from a dirty excel file and cleans it. This is a data cleaning problem I have seen a few final project groups face. Download these files, look at the original csv file, and then source the script and see what you get! data_cleaning_example.R sample_ugly_data.csv\nOverview of Machine Learning Tools Predicting-birthweight.pdf\nIntro to Quarto (and Quarto Resources) using_quarto.pdf\nPlotting Distributions from Aggregate Data This handout covers how to plot univariate distributions of data (i.e. histograms and bar charts) if your data have already been aggregated (e.g. you have the counts for each value of a variable but not the individual observations). hist_and_bar_with_aggregated_data.pdf\nDownloading and Merging Publicly Posted Datasets This handout walks through an example case of downloading datasets posted on a government website, cleaning them, merging them together, and using the merged dataset for an analysis. It addresses some of the common challenges of getting acquainted with a data portal. ipeds_data-1.pdf\n\n\nFurther information on using R Markdown\nIn addition to ?sec-intro_markdown (which has some initial thoughts and how-tos tailored to the course), there are some additional resources on using R Markdown:\n\nrmarkdown-cheatsheet-2.0.pdf\nrmarkdown-reference.pdf\nSee the async lecture on configuring code chunks in Week 1, on Canvas, as well.\n\n\n\nUseful links to materials on the web\n\nR: https://www.r-project.org/\nRStudio: https://www.rstudio.com/\nColumbia RStudio overview: here\nTryR code school: http://tryr.codeschool.com\nData Camp: https://www.datacamp.com/courses/free-introduction-to-r\nPrinceton R tutorials: http://data.princeton.edu/R/\nD-Lab R training: https://github.com/dlab-berkeley/R-for-Data-Science\nGGPlot2: http://ggplot2.org/\nGoogle/stackoverflow\n\n\n\nWriting your own math\n\nHandout: math reference – This handout, from my handout textbook for S043 (multilevel modeling) has the Latex you need to write equations easily for your reports. But what you really want is the Rmd version that has the code to make this handout, which is linked in the handout itself.",
    "crumbs": [
      "Resources on Learning R"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting Started with R",
    "section": "",
    "text": "Some Resources and Directions\nThe following are ways of getting help with R.",
    "crumbs": [
      "Getting Started",
      "Getting Started with R"
    ]
  },
  {
    "objectID": "getting_started.html#some-resources-and-directions",
    "href": "getting_started.html#some-resources-and-directions",
    "title": "Getting Started with R",
    "section": "",
    "text": "Handouts\nThis textbook has many handouts written by the teaching team that illustrates how to use R for a variety of tasks. Also see the Resources page on Canvas for further commentary and organization.\n\n\nClass Code\nEach class has a R script that shows how to do the stuff from that class. See the Packets on Canvas for this code.\n\n\nA Very Important Online Textbook\nThe online textbook R for Data Science. provides important information on wrangling data, making plots, and doing statistical programming. It is full of examples and code snippets you can steal. There are two versions:\n\nThe First Edition – this what we often refer to in these materials. But we are slowly updating to…\nThe Second Edition – this one has a different organization, less weird modeling, and covers recent changes to the “tidyverse.”\n\n\n\nClass Sections\nSections will typically have a hands-on component which will give you a chance to try things out yourself. These sections will also publish the final R code for future reference. See the section pages on Canvas to get this information.\n\n\nOffice Hours\nOffice hours are fine time to get help troubleshooting a specific or script you are working on.\n\n\nGSE Stat Help Desk\nEducation students can write to stathelp@gse.harvard.edu to get help getting started with R. If groups of 3 or more want some tutorials, they can ask for them as well.\nThe Help Desk sometimes has Intro to R workshops. For example, grab some old workshop materials here: http://its.gse.harvard.edu/gentle-introduction-r",
    "crumbs": [
      "Getting Started",
      "Getting Started with R"
    ]
  },
  {
    "objectID": "style_guide.html",
    "href": "style_guide.html",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "",
    "text": "1.1 Why have coding style?\nMany style decisions in coding are arbitrary, so why bother adhering to them? The primary reasons are that good style makes your code more readable, allows you to focus on writing quality code rather than worrying about inconsistencies, and helps you avoid being judged negatively for poor style.\nMore importantly, if you write in a clean style, you will be able to read your own code a few weeks later, when you have forgotten what you have been thinking. It also makes it a lot easier for the teaching team to help you understand possible problems in your code.\nIn this chapter, I lay out some tips for writing clean code. Much of this was initially taken from the Tidyverse style guide at http://style.tidyverse.org; we are primarily focusing on Chapters 2 and 4. But I will note that I depart substantially from some of their recommendations.\nThe cartoon is xkcd; read those if you want to be an awesome nerd.\nFor further reading about coding practice, also see Code and Data for the Social Sciences: A Practitioner’s Guide, Gentzkow and Shapiro. Worth a skim!",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "style_guide.html#the-naming-of-things",
    "href": "style_guide.html#the-naming-of-things",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "1.2 The naming of things",
    "text": "1.2 The naming of things\n\n“There are only two hard things in Computer Science: cache invalidation and naming things.” —Phil Karlton\n\n\nVariable and function names should be lowercase.\nUse an underscore to separate words within a name.\nGenerally, variable names should be nouns and function names should be verbs.\nIdeally, your names should be self-explanatory and your code should be “self-documenting.”\nAvoid using numbers to store versions of a data frame (e.g., df1, df2, df3, …).\nNames for variables, functions, files, etc. should generally consist of complete words.\nNaming is hard. more art than science.\n\n\n# Good\nday_one\nfirst_day\n\n# Bad\nfirst_day_of_the_month\nDayOne\ndayone\ndjm1\n\n\n1.2.1 Don’t use common names\nIf R uses a name already, just avoid it.\n\n# Bad\nTRUE &lt;- FALSE\npi &lt;- 10\nmean &lt;- function(x) sum(x)\n\n\n\n1.2.2 Name constants for clarity\n\n# Good\nwinsor_upper &lt;- 0.99\nwinsor_lower &lt;- 0.01\ndiamonds &lt;-\n  diamonds %&gt;%\n  mutate(y_winsor = winsorize(y, probs = c(winsor_lower, winsor_upper)))\n\n# Mediocre\ndiamonds_clean &lt;-\n  diamonds %&gt;%\n  mutate(y = winsorize(y, probs = c(0.01, 0.99)))",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "style_guide.html#syntax",
    "href": "style_guide.html#syntax",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "1.3 Syntax",
    "text": "1.3 Syntax\n\n1.3.1 Appropriate spaces make code readable\n\nPut a space before and after = when naming arguments in function calls.\nAlways put a space after a comma, and never before (just like in regular English).\n\n\n# Good\naverage &lt;- mean(x, na.rm = TRUE)\n\n# Also good\naverage &lt;- mean( x, na.rm = TRUE )\n\n# Bad\naverage&lt;-mean(x, na.rm = TRUE)\naverage &lt;- mean(x ,na.rm = TRUE)\n\n\n\n1.3.2 Judicious spaces group elements to increase readability\n\nMost infix operators (==, +, -, &lt;-, etc.) should be surrounded by spaces.\nThe exception are those with relatively high precedence: ^, :, ::, and :::. (“High precedence” means that these operators are evaluated first, like multiplication goes before addition.)\n\n\n# Good\nheight &lt;- (feet * 12) + inches\nsqrt(x^2 + y^2)\nx &lt;- 1:10\nbase::get\n\n# Bad\nheight&lt;-feet*12 + inches\nsqrt(x ^ 2 + y ^ 2)\nx &lt;- 1 : 10\nbase :: get\n\n\n\n1.3.3 Lots of spaces to line things up is nice\nExtra spacing (i.e., more than one space in a row) is ok if it improves alignment of equal signs or assignments (&lt;-).\n\n# Good\nlist(\n  total = a + b + c,\n  mean  = (a + b + c) / n\n)\n\n# Less good, but livable\nlist(\n  total = a + b + c,\n  mean = (a + b + c) / n\n)\n\n\n\n1.3.4 Including names when calling functions\nA function typically takes several arguments, namely the data to compute on and further arguments giving the details of the desired computation.\nOmit names of common arguments (e.g. data, aes).\nIf you override the default value of an argument, use the full name:\n\n# Good\nmean(1:10, na.rm = TRUE)\n\n# Bad\nmean(x = 1:10, , FALSE)\nmean(, TRUE, x = c(1:10, NA))\n\n\n\n1.3.5 No line of code should be longer than 80 characters\n\nuse one line each for the function name, each argument, and the closing ‘)’.\nuse indentation to make it clear that the arguments are part of the function call.\n\n\n# Good\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\"\n)\n\n# Very bad\ndo_something_very_complicated(something = \"that\", requires = many, arguments = \"some of which may be long\")\n\n# Still bad\ndo_something_very_complicated(\n  something = \"that\", requires = many,\n  arguments = \"some of which may be long\"\n)\n\n# Yup, still bad\ndo_something_very_complicated(\n  something = \"that\", requires = many, arguments =\n  \"some of which may be long\"\n)\n\nException to the above: short unnamed arguments can also go on the same line as the function name, even if the whole function call spans multiple lines.\n\n# Good\nmap(x, f,\n  extra_argument_a = 10,\n  extra_argument_b = c(1, 43, 390, 210209)\n)\n\n\n\n1.3.6 Use &lt;- for assignment (if you are prissy)\nUse &lt;-, not =, for assignment.\n\n# Good\nx &lt;- 5\n\n# Upsets the nitpickers\nx = 5\n\n\n\n1.3.7 Double quotes are standard\nUse \", not ', for quoting text. The only exception is when the text already contains double quotes and no single quotes.\n\n# Good\n\"Text\"\n'Text with \"quotes\"'\n'&lt;a href=\"http://style.tidyverse.org\"&gt;A link&lt;/a&gt;'\n\n# Bad\n\"Text\"\n'Text with \"double\" and \\'single\\' quotes'\n\n\n\n1.3.8 Comments are good\nComments can really help with code, and help a reader assess if the code is achiving the desired effect. Comments should give the purpose of the code, not just restate the code in English.\nThat said, commends can be dangerous because if you change your code without changing the comments, you can cause confusion. This happens a lot, potentially making comments a source of confusion rather than clarity.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "style_guide.html#pipes-with-magrittr",
    "href": "style_guide.html#pipes-with-magrittr",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "1.4 Pipes (%>%) with magrittr",
    "text": "1.4 Pipes (%&gt;%) with magrittr\nUse %&gt;% (or |&gt;, if you are modern) to emphasise a sequence of actions, rather than the object that the actions are being performed on.\nAvoid using the pipe when:\n\nYou need to manipulate more than one object at a time. Reserve pipes for a sequence of steps applied to one primary object.\nThere are meaningful intermediate objects that could be given informative names (cf rule 2.9).\n\n\n1.4.1 Surround the pipe with whitespace\n\n%&gt;% should always have a space before it, and should usually be followed by a new line.\nAfter the first step, each line should be indented by two spaces.\n\nAdhering to the above structure makes it easier to add new steps (or rearrange existing steps) and makes it harder to overlook a step.\n\n# Good\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize_if(is.numeric, mean) %&gt;%\n  ungroup() %&gt;%\n  gather(measure, value, -Species) %&gt;%\n  arrange(value)\n\n# Bad\niris %&gt;% group_by(Species) %&gt;% summarize_all(mean) %&gt;% \nungroup() %&gt;% gather(measure, value, -Species) %&gt;%\narrange(value)\n\n\n\n1.4.2 Formatting short pipes\nIt is ok to keep a one-step pipe in one line, if you want. But not doing so is also fine.\n\n# Good\niris %&gt;% arrange(Species)\n\n# Also good\niris %&gt;%\n  arrange(Species)\n\narrange(iris, Species)\n\n\n\n1.4.3 Do not pack pipes inside of other pipes\nThis is brain-hurty:\n\n# Bad\nx %&gt;%\n  select(a, b, w) %&gt;%\n  left_join(\n    y %&gt;% filter(!u) %&gt;% gather(a, v, -b) %&gt;% select(a, b, v),\n    by = c(\"a\", \"b\")\n  )\n\nInstead, separate the pipes into their own lines:\n\n# Good\n\nx_join &lt;- x %&gt;%\n  select(a, b, w)\ny_join &lt;- y %&gt;%\n  filter(!u) %&gt;%\n  gather(a, v, -b) %&gt;%\n  select(a, b, v)\nleft_join(x_join, y_join, by = c(\"a\", \"b\"))\n\nGenerally when you are doing something to two datasets, put both inside the parenthesis, rather than having one in and one piped in.\n\n\n1.4.4 Keep your parenthesis even if you have no arguments\nmagrittr allows you to omit () on functions that don’t have arguments. Avoid this. This way data objects never have parentheses and functions always do.\n\n# Good\nx %&gt;%\n  unique() %&gt;%\n  sort()\n\n# Bad\nx %&gt;%\n  unique %&gt;%\n  sort\n\n\n\n1.4.5 Rule 4.6: Assignment\nThe first line can have the assignment and also the starting dataframe of a pipe sequence.\n\n# Good\niris_long &lt;- iris %&gt;%\n  gather(measure, value, -Species) %&gt;%\n  arrange(-value)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "style_guide.html#a-few-final-remarks",
    "href": "style_guide.html#a-few-final-remarks",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "1.5 A few final remarks",
    "text": "1.5 A few final remarks\n\nStyle is awesome. Save a future researcher from spending months trying to disentangle your spaghetti!\nYou don’t need to memorize these rules! Just as you have spell check and grammarly on your computer for prose, there is a package styler to help you follow the code style guide.\nJust as you still need to learn to spell (since spell checker doesn’t capture everything), you need to learn these rules as well.\n\nIn closing:\n\n“Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.” —Hadley Wickham",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html",
    "href": "intro_markdown.html",
    "title": "2  Intro to R Markdown #{sec-intro-markdown}",
    "section": "",
    "text": "2.1 Overview\nThis document will show you how to use R Markdown to create documents which draw directly on your data to produce reports.\nR Markdown (and its newer cousin Quarto) is a simple but powerful markdown language which you can use to create documents with inline R code and results. This makes it much easier for you to complete homework assignments and reports; makes it much less likely that your work will include errors; and makes your work much easier to reproduce. For example, if you find you have to drop cases from your dataset, you can simply add that line of code to your document, and recompile your document. Any text that’s drawn directly from your analyses will be automatically updated.\nTo get started with R Markdown, watch Miratrix’s video on RMarkdown. You can find it on this page, under the “Do it for Lab” tab.\nTo use a markdown document, you generally open it and work inside of R Studio. To compile (or “knit”) the document—meaning have the computer run all the code in the document, and then generate a new document with the results of the code and all the formatting specified carried out—click on the button that says ‘Knit’. You can make a new document by saying “new markdown document” and then immediately “knit” it to see what it does.\nOther R packages, such as Sweave and knitr, allow you to do the same things, but R Markdown has the added advantage of being relatively simple to use.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown #{sec-intro-markdown}</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#getting-started",
    "href": "intro_markdown.html#getting-started",
    "title": "2  Intro to R Markdown #{sec-intro-markdown}",
    "section": "2.2 Getting started",
    "text": "2.2 Getting started\nEvery R Markdown document starts with a header, called the YAML header. YAML headers look like this:\n---\ntitle: \"My perfect homework\"\nauthor: \"R master\"\noutput: pdf_document\n---\nA header can contain more or less information, as you see fit. Your computer needs to have a copy of LaTex installed in order to output .pdf documents. If you don’t, you should change output: pdf_document to output: html_document or output: word_document.\nYou can specify a lot of options in the header. The most important ones are the title of your document, the author, and the output format.\nIn the main part of your document, you identify sections of the document using hashtags; more hashtags indicate less important sections.\nFor example, this:\n# A big section\nproduces a big header (large font, etc.)\nwhile this\n## A small section\nproduces a smaller header (still a large font, but less large).\nAlso, if your document includes a table of contents, the sections get used to automatically generate the table of contents.\nYou can italicize words by writing *italicize* or _italicize_. You can bold words with **bold** or __bold__.\nYou can add superscripts (E=mc2) by writing E=mc^2^.\nYou can create unordered lists:\n- Item 1\n- Item 2\n- Item 3\nto get\n\nItem 1\nItem 2\nItem 3\n\nOr ordered lists:\n1. Item 1\n2. Item 2\n3. Item 3\nto get\n\nItem 1\nItem 2\nItem 3\n\nTo start a new page, just type \\newpage (not relevant for HTML output).\nAs you may have noticed, one of the driving ideas behind R Markdown is that the text should be interpretable even if it’s not compiled. A person should be able to read the initial text file and understand the basic organization and what all of the symbols denote.\nYou can also add links and images, and do many other things beyond what we’ll show you in this class. There are many resources out there, but here’s one place you can start.\nWe also note that newer versions of Markdown and Quarto have a visual editor that allows you to format things in the usual way, e.g., control-B for bold. Some people prefer to take that approach.\nRegardless, to compile or knit the document, click on the button that says Knit or Render, or Shift + Ctrl/Cmd + K.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown #{sec-intro-markdown}</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#embedding-r-code",
    "href": "intro_markdown.html#embedding-r-code",
    "title": "2  Intro to R Markdown #{sec-intro-markdown}",
    "section": "2.3 Embedding R code",
    "text": "2.3 Embedding R code\nThe power of R Markdown is you can include R code and that code will get run and the output included in your report automatically. There are two main ways to embed R code in R Markdown, code chunks (this is the primary way) or inline (this is if you want to get fancy).\n\n2.3.1 Code chunks\nTo insert a code chunk click on Insert on the top right corner of your R Markdown file and select R. Or use keyboard shortcuts: Ctrl + Alt + I for PC and Cmd + Option + I for Mac.\nA code chunk starts with three backticks and the letter r in curly braces, and ends with three more backticks on their own line. In between these lines you put the R code.\nCode chunks have a number of different options you can put after the r in the opening line. The most important ones for us are:\n\neval = TRUE, which means every time you knit the file, the code inside the R code chunk will get evaluated. This is the default.\necho = TRUE, which means every time you knit the file, the code inside the R code chunk will be rendered, and you can see both the code itself and the results from evaluating the code.\n\nFor class, you should keep echo = TRUE, so that we can see your code and be able to tell what went wrong, if something did. You can set echo = FALSE for code chunks that load and manipulate data in the setup of your analysis, if you don’t want people to look at that part of your work.\nOther code chunks options you may see in class are:\n\nwarnings = FALSE, which means warning messages generated by the code will not be displayed.\nresults = 'asis', which means results will not be reformatted when the file is compiled (useful if results return raw HTML).\nfig.height and fig.width, which specify the height and width (in inches) of plots created by the chunk.\n\nInstead of specifying code chunks options every time, you can specify them globally in the setup chunk by using\n\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE)\n\nYou can then add additional options only to relevant chunks. For example, you want to exclude specific chunks, you can re-set echo = FALSE and eval = FALSE for those specific chunks.\nRunning code chunks: A good practice for writing your code is to run individual code chunks to make sure they are doing what you want them to do as you write them. You can do this by executing individual lines of code, or whole chunks. Go to Run in the upper right corner and select what chunks to execute, e.g. Run Current Chunk, Run Next Chunk, etc.\n\n\n2.3.2 Inline code\nCode results can also be inserted directly in the text of your R Markdown file. This is particularly useful when you are extracting and interpreting model parameters. You can extract the coefficient from the model and use inline code to report it. If the data or model change, the text will also change when you knit the document.\nTo add inline code, enclose it in `r `. For example, to report the mean reading score, you can use\n-0.0443549\nWhich will produce -0.0443549. That’s a few too many decimals, let’s round it off, using\n-0.04\nwhich produces “-0.04.”\nHere we used two commands: round and mean. You can use more commands and write more complex inline code, depending on what you want to report.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown #{sec-intro-markdown}</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#embedding-plots",
    "href": "intro_markdown.html#embedding-plots",
    "title": "2  Intro to R Markdown #{sec-intro-markdown}",
    "section": "2.4 Embedding plots",
    "text": "2.4 Embedding plots\nPlots are easy to embed. For example,\n\nlibrary(ggplot2)\n\ndat$male &lt;- factor(dat$male, levels = c(0, 1), labels = c(\"Female\", \"Male\"))\n  \nggplot(data=dat, aes(p7vrq, attain, colour=male)) + \n  geom_point() + \n  labs(title=\"Attainment as a function of verbal reasoning\",\n       x = \"Verbal reasoning quotient\", \n       y = \"Educational attainment\", colour=\"Gender\") +\n  geom_smooth(method=\"lm\", formula = y ~ x, se=FALSE, colour=\"darkorchid3\")\n\n\n\n\n\n\n\n\nGirls are rendered as coral, boys are rendered in turquoise, and the line of best fit is drawn in darkorchid3 (because why not). Just because you have a lot of colors and plotting characters to work with doesn’t mean you need to use them all. In the options, I specified fig.width = 7 and fig.height = 7. Notice that this command draws on dat, which we loaded in a previous chunk. When knitting the document, code chunks get executed in order and the results persist throughout the R Markdown document.\nFor the purposes of class, we want to see both your plot code and the plot itself. It’s not uncommon to use wrong code to create a plot that looks correct (at least visually).",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown #{sec-intro-markdown}</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#embedding-tables",
    "href": "intro_markdown.html#embedding-tables",
    "title": "2  Intro to R Markdown #{sec-intro-markdown}",
    "section": "2.5 Embedding tables",
    "text": "2.5 Embedding tables\nYou can directly render tables in R Markdown. The idea is, inside an R chunk, you call a command that prints out a table. The report then takes this printout and integrates it into your overall report. There are many different packages to make tables, but in class we’ll mostly use knitr, texreg, stargazer, and the tab_model() function in sjPlot.\nYou can use these packages to create a descriptive table. For example:\n\nhead( dat ) %&gt;%\n  knitr::kable( digits = 2 )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nneighid\nschid\nattain\np7vrq\np7read\ndadocc\ndadunemp\ndaded\nmomed\nmale\ndeprive\n\n\n\n\n675\n0\n0.74\n21.97\n12.13\n2.32\n0\n0\n0\nMale\n-0.18\n\n\n647\n0\n0.26\n-7.03\n-12.87\n16.20\n0\n0\n1\nFemale\n0.21\n\n\n650\n0\n-1.33\n-11.03\n-31.87\n-23.45\n1\n0\n0\nMale\n0.53\n\n\n650\n0\n0.74\n3.97\n3.13\n2.32\n0\n0\n0\nMale\n0.53\n\n\n648\n0\n-0.13\n-2.03\n0.13\n-3.45\n0\n0\n0\nFemale\n0.19\n\n\n648\n0\n0.56\n-5.03\n-0.87\n-3.45\n0\n0\n0\nFemale\n0.19\n\n\n\n\nSee Chapter 5 for more on making various tables.\nWe can also use texreg or stargazer to create a taxonomy of regression models.\nFor example:\n\nlibrary(texreg)\n\n# fit some models \nm1 &lt;- lm(attain ~ male, data=dat)\nm2 &lt;- lm(attain ~ male + momed, data=dat)\nm3 &lt;- lm(attain ~ male + momed + daded, data=dat)\n\nscreenreg(list(m1,m2,m3), \n          custom.coef.names=c(\"Intercept\", \"Male\", \n                              \"Maternal education\", \"Paternal education\"))\n\n\n=========================================================\n                    Model 1      Model 2      Model 3    \n---------------------------------------------------------\nIntercept              0.15 ***     0.03        -0.02    \n                      (0.03)       (0.03)       (0.03)   \nMale                  -0.12 **     -0.12 **     -0.12 ** \n                      (0.04)       (0.04)       (0.04)   \nMaternal education                  0.49 ***     0.24 ***\n                                   (0.05)       (0.05)   \nPaternal education                               0.54 ***\n                                                (0.06)   \n---------------------------------------------------------\nR^2                    0.00         0.05         0.09    \nAdj. R^2               0.00         0.05         0.08    \nNum. obs.           2310         2310         2310       \n=========================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nBoth packages include a lot of options and make it easy to produce publication-quality tables with little effort.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown #{sec-intro-markdown}</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#embedding-math",
    "href": "intro_markdown.html#embedding-math",
    "title": "2  Intro to R Markdown #{sec-intro-markdown}",
    "section": "2.6 Embedding math",
    "text": "2.6 Embedding math\nWe’ll be writing some mathematical models in class. R Markdown can use LaTeX style math-writing to display mathematical script.  Similar to code chunks and inline code, you can use LaTeX for single or multiple equations, or for individual parameters embedded in the text.\nFor example, the following statement\n$$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$$\ncompiles to\n\\[Y_i = \\beta_0 + \\beta_1 Y_i + \\epsilon_i\\]\nAnd the following statement $\\mu$ compiles to \\(\\mu\\).",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown #{sec-intro-markdown}</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#help-r-markdown-report-generation-doesnt-work",
    "href": "intro_markdown.html#help-r-markdown-report-generation-doesnt-work",
    "title": "2  Intro to R Markdown #{sec-intro-markdown}",
    "section": "2.7 Help! R Markdown report generation doesn’t work",
    "text": "2.7 Help! R Markdown report generation doesn’t work\nIf your markdown will not render, first try restarting R and then selecting run all chunks from the menu. If that does not work, then try fixing your code.\nIf that does work, but knitting still does not work, then something stranger is going on. First, check to see if you have a “View()” command in your Markdown file. This will cause your document to not be able to knit.\nAlso watch for the skim() command–it can crash report generation as well.\nIf you can’t knit PDFs you need to install latex (tex). Once you do, reboot your computer. If things don’t work, then knit to Microsoft word (or, failing that, html as a last resort), print to pdf, and turn that in. But then ask a teaching fellow to help get things set up, since PDFs make for much more readable reports.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown #{sec-intro-markdown}</span>"
    ]
  },
  {
    "objectID": "using_quarto.html",
    "href": "using_quarto.html",
    "title": "3  Creating Reports in Quarto",
    "section": "",
    "text": "3.1 What is Quarto?\nQuarto is the “next-generation” of RMarkdown. Most tutorials on Quarto are intended for people who have experience with RMarkdown, so a great place to start is to read the prior chapter before reading this one.\nOnce you’re familiar with RMarkdown, the tricks you know will almost always work in Quarto. The major difference is that Quarto will render inside RStudio, as you write your document, which makes it a little nicer to work with. There are other new (and very cool) features, but they aren’t essential for anything in this course.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Creating Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "using_quarto.html#using-visual-editor-with-rmd",
    "href": "using_quarto.html#using-visual-editor-with-rmd",
    "title": "3  Creating Reports in Quarto",
    "section": "3.2 Using Visual Editor with Rmd",
    "text": "3.2 Using Visual Editor with Rmd\nAn obvious, cool thing about Quarto is the fancy visual editor, but you can turn this feature on for regular old RMarkdown files, too. Just click on this , and click “use visual editor.” You can also add editor: visual to the yaml (top-matter) of an rmd file and it’ll open in the visual editor by default.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Creating Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "using_quarto.html#creating-a-quarto-report",
    "href": "using_quarto.html#creating-a-quarto-report",
    "title": "3  Creating Reports in Quarto",
    "section": "3.3 Creating a Quarto Report",
    "text": "3.3 Creating a Quarto Report\nIn RStudio, click this icon in the upper left: \nThat’ll give you the following drop-down menu, where you can select “Quarto Document”:\n\nWhen you click on “Quarto Document…” RStudio might take a few seconds to load. Then you’ll see this pop-up:\n\n\nFill out the document title and author (just like for RMarkdown). You can always change the title and author later. You’ll want to render your reports as PDFs, so select that option. Finally, hit the “Create” button at the bottom.\nRStudio will load up a new Quarto doc (with some boilerplate markdown in it). From here, you can treat it like an RMarkdown file.\nIf you want a more thorough introduction to Quarto, check out this tutorial.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Creating Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "using_quarto.html#rendering-to-pdf",
    "href": "using_quarto.html#rendering-to-pdf",
    "title": "3  Creating Reports in Quarto",
    "section": "3.4 Rendering to PDF",
    "text": "3.4 Rendering to PDF\nTo render your report to PDF, you’ll need to have an installation of LaTex. You can set this up from within RStudio.\nDown by your console, there’s a tab called “Terminal.” Click on it to open the terminal. Inside the terminal type the following:\nquarto install tool tinytex\nI recommend restarting your computer after this.\nNow you should be able to click  and get a pdf version of your report. By default, a copy of the pdf will be saved in the same folder as your Quarto document.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Creating Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "using_quarto.html#a-couple-quick-tricks",
    "href": "using_quarto.html#a-couple-quick-tricks",
    "title": "3  Creating Reports in Quarto",
    "section": "3.5 A Couple Quick Tricks",
    "text": "3.5 A Couple Quick Tricks\n\n3.5.0.1 Making Code Chunks\nUse the keyboard shortcut ctrl+alt+i or command + option + i to create a new R code chunk.\n\n\n3.5.0.2 Adding Images\nYou can copy-paste or drag-and-drop images into a Quarto doc. That’s how I put the above screenshots into this document.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Creating Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "using_quarto.html#additional-quarto-resources",
    "href": "using_quarto.html#additional-quarto-resources",
    "title": "3  Creating Reports in Quarto",
    "section": "3.6 Additional Quarto Resources",
    "text": "3.6 Additional Quarto Resources\nHere are a couple more links if you’d like to learn more:\n\nThe RMarkdown Cheatsheet has great information. Remember, most things that work in RMarkdown work in Quarto. Also, there are a lot of other great cheatsheets at this site as well.\nA Quarto Intro Video that walks through using Quarto for the first time.\nA In Depth Quarto Intro video that covers many amazing new features (like making interactive html reports).",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Creating Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "code_chunks.html",
    "href": "code_chunks.html",
    "title": "4  Working with Rmarkdown chunks",
    "section": "",
    "text": "4.1 Markdown chunks",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with Rmarkdown chunks</span>"
    ]
  },
  {
    "objectID": "code_chunks.html#markdown-chunks",
    "href": "code_chunks.html#markdown-chunks",
    "title": "4  Working with Rmarkdown chunks",
    "section": "",
    "text": "4.1.1 Options for including/suppressing code and output\ninclude: Should chunk be included in knit file? Defaults to TRUE. If FALSE, code chunk is run, but chunk and any output is not included in the knit file.\neval: Should chunk be evaluated by R? Defaults to TRUE. If FALSE, code chunk is included in the knit file, but not run.\necho: Should the code from this chunk be included in knit file along with output? Defaults to TRUE. If FALSE, the output from the chunk is included, but the code that created it is not. Most useful for plots.\n\n\n4.1.2 Options for including/suppressing R messages\nR has “errors,” meaning it could not run your code, “warnings,” meaning that the code was wrong, but there are some potential issues with it, and “messages,” which are simply information about what your code ran. You can include or suppress each of these types of message.\nerror: Should R continue knitting if code produces an error? Defaults to FALSE. Generally don’t want to change this because it means you can miss serious issues with your code.\nwarning: Should R include warnings in knit file? Defaults to TRUE.\nmessage: Should R include informational messages in knit file? Defaults to TRUE. Easy way to clean up your markdowns.\nFor example, the following code chunk produces an error, but because the header is error = TRUE, warning=FALSE, message=FALSE, only the error is shown:\n\n#This code produces an error\ndat %&gt;%\n  filter(dest = 1)\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `dest == 1`?\n\n#Example warning\nparse_number(c(\"1\", \"$3432\", \"tomato\"))\n\n[1]    1 3432   NA\nattr(,\"problems\")\n# A tibble: 1 × 4\n    row   col expected actual\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt; \n1     3    NA a number tomato\n\n#Example message\nlibrary(gridExtra)\n\n\n\n4.1.3 Options for modifying figure outputs\nout.width: What percentage of the page width should output take?\nfig.height: What should be the height of figures?\nfig.width: What should be the width of figures?\nfig.asp: What should be the aspect ratio of figures?\nfig.align: How should figures be aligned?\nWe might want a bigger plot for this:\n\n\n\n\n\n\n\n\n\nAnd a smaller plot for this:",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with Rmarkdown chunks</span>"
    ]
  },
  {
    "objectID": "code_chunks.html#changing-your-defaults",
    "href": "code_chunks.html#changing-your-defaults",
    "title": "4  Working with Rmarkdown chunks",
    "section": "4.2 Changing your defaults",
    "text": "4.2 Changing your defaults\nAt the beginning of your code, you can set custom defaults so all your chunks will render the same way (unless you override by specifically adding arguments to a chunk itself). This is handy in that you will then not need to repeat the custom arguments in each code chunk. For example, you can set a default figure size.\nHere is an example:\n\n knitr::opts_chunk$set(echo = TRUE, \n                       fig.width = 5,\n                       fig.height = 3,\n                       out.width = \"5in\", \n                       out.height = \"3in\", fig.align = \"center\")",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with Rmarkdown chunks</span>"
    ]
  },
  {
    "objectID": "making_tables.html",
    "href": "making_tables.html",
    "title": "5  Making tables in Markdown",
    "section": "",
    "text": "5.1 Making a “table one”\nThe “table one” is the first table in a lot of papers that show general means of different variables for different groups. Perhaps not surprisingly, the tableone package is useful for making such tables:\nlibrary(tableone)\n\n# sample mean  \nCreateTableOne(data = dat,\n               vars = c(\"G\", \"Z\", \"X\"))\n\n               \n                Overall      \n  n               100        \n  G (%)                      \n     A             20 (20.0) \n     B             17 (17.0) \n     C             17 (17.0) \n     D             24 (24.0) \n     E             22 (22.0) \n  Z = tx (%)       46 (46.0) \n  X (mean (SD)) -0.31 (0.97) \n\n# you can also stratify by a variables of interest\ntb &lt;- CreateTableOne(data = dat,\n                     vars = c(\"X\", \"G\", \"Y\"), \n                     strata = c(\"Z\"))\ntb\n\n               Stratified by Z\n                co            tx            p      test\n  n                54            46                    \n  X (mean (SD)) -0.31 (0.95)  -0.31 (1.00)   0.987     \n  G (%)                                      0.927     \n     A             11 (20.4)      9 (19.6)             \n     B             10 (18.5)      7 (15.2)             \n     C              8 (14.8)      9 (19.6)             \n     D             12 (22.2)     12 (26.1)             \n     E             13 (24.1)      9 (19.6)             \n  Y (mean (SD))  0.00 (0.97)   0.02 (0.98)   0.904\nYou can then use kable on your table as so:\nprint(tb$ContTable, printToggle = FALSE) %&gt;%\n    knitr::kable()\n\n\n\n\n\nco\ntx\np\ntest\n\n\n\n\nn\n54\n46\n\n\n\n\nX (mean (SD))\n-0.31 (0.95)\n-0.31 (1.00)\n0.987\n\n\n\nY (mean (SD))\n0.00 (0.97)\n0.02 (0.98)\n0.904",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making tables in Markdown</span>"
    ]
  },
  {
    "objectID": "making_tables.html#the-stargazer-package",
    "href": "making_tables.html#the-stargazer-package",
    "title": "5  Making tables in Markdown",
    "section": "5.2 The stargazer package",
    "text": "5.2 The stargazer package\nYou can easily make pretty tables using the stargazer package. You need to ensure the data is a data.frame, not tibble, because stargazer is old school. It appears to only do continuous variables. Stargazer is probably best known for making regression tables, but it can make other kinds of tables as well, such as data summaries.\nWhen using stargazer to summarize a dataset, you can specify that it should include only some of the variables and you can omit stats that are not of interest:\n\n# to include only variables of interest\nstargazer(as.data.frame(dat), header=FALSE, \n          omit.summary.stat = c(\"p25\", \"p75\", \"min\", \"max\"), \n          # to omit percentiles\n          title = \"Table 1: Descriptive statistics\",\n          type = \"text\")\n\n\nTable 1: Descriptive statistics\n=============================\nStatistic  N   Mean  St. Dev.\n-----------------------------\nX         100 -0.310  0.970  \nY         100 0.011   0.967  \n-----------------------------\n\n\nSee the stargazer help file for how to set/change more of the options: https://cran.r-project.org/web/packages/stargazer/stargazer.pdf\nWarning: stargazer does not work well with tibbles (the data frames you get from tidyverse commands), so you need to convert your data to a data.frame before using it. In particular, you have to “cast” your data to a data.frame to make it work:\n\n  library(stargazer)\n  \n  # to include all variables\n  stargazer( as.data.frame(dat), header = FALSE, type=\"text\")\n\nTo use stargazer in a PDF or HTML report, you will want the report to format the table so it doesn’t look like raw output. To do so, you would not set type=\"text\" but rather type=\"latex\" or type=\"html\", and then in the markdown chunk header (the thing that encloses all your R code) you would say “results=‘asis’” in your code chunk header like so:\n  ::: {.cell layout-align=\"center\" messages='false'}\n  \n  :::\nThis will ensure the output of stargazer gets formatted properly in your R Markdown.\nUnfortunately, it is hard to dynamically make a report that can render to either html or a pdf, so you will have to choose one or the other. If you are making a PDF, you will want to use type=\"latex\" and if you are making an HTML report, you will want to use type=\"html\".",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making tables in Markdown</span>"
    ]
  },
  {
    "objectID": "making_tables.html#the-xtable-package",
    "href": "making_tables.html#the-xtable-package",
    "title": "5  Making tables in Markdown",
    "section": "5.3 The xtable package",
    "text": "5.3 The xtable package\nThe xtable package is another great package for making tables. It is particularly good for LaTeX documents. It is a bit more complicated to use than stargazer, but it is very powerful. Here is an example of how to use it:\n\nlibrary(xtable)\nxtable(sdat, caption = \"A table of fake data\" )\n\nHere you would again use the “results=‘asis’” in the chunk header to get the table to render properly in your R Markdown document.",
    "crumbs": [
      "On markdown",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Making tables in Markdown</span>"
    ]
  },
  {
    "objectID": "intro_ggplot.html",
    "href": "intro_ggplot.html",
    "title": "6  Intro to ggplot",
    "section": "",
    "text": "6.1 Summarizing\nYou can also automatically add various statistical summaries, such as simple regression lines:\nggplot( dat, aes(y=mathach, x=ses, col=sector ) ) + \n    geom_point() + \n    stat_smooth( method=\"lm\", se = FALSE )\nNotice how it automatically realized you have two subgroups of data defined by sector. It gives you a regression line for each group.\nThe elements of the plot are stacked, and if you remove one of the elements, it will not appear:\nggplot( dat, aes(y=mathach, x=ses, col=sector ) ) + \n  stat_smooth( method=\"lm\" )\nHere we also added some uncertainty bars around the regression lines by not saying se = FALSE. (Including uncertainty is the default; this uncertainty is not to be trusted, especially in this course, as it is not taking clustering into account.)",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to `ggplot`</span>"
    ]
  },
  {
    "objectID": "intro_ggplot.html#grouping",
    "href": "intro_ggplot.html#grouping",
    "title": "6  Intro to ggplot",
    "section": "6.2 Grouping",
    "text": "6.2 Grouping\nCombining these ideas we can make a trend line for each school:\n\nmy.plot = ggplot( dat, aes(y=mathach, x=ses, col=sector, group=schoolid ) ) + \n    stat_smooth( method=\"lm\", alpha=0.5, se = FALSE )\n\nmy.plot\n\n\n\n\n\n\n\n\nThe trendlines automatically extend to the limits of the data they are run on, hence the different lengths.\nAlso, notice we “saved” the plot in the variable my.plot. Only when we “print” the plot will the plot appear on your display. When we type the name of a variable, it prints. Once you have a plot stored in a variable you can augment it very easily.\nAs you may now realize, ggplot2 is very, very powerful.",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to `ggplot`</span>"
    ]
  },
  {
    "objectID": "intro_ggplot.html#customization",
    "href": "intro_ggplot.html#customization",
    "title": "6  Intro to ggplot",
    "section": "6.3 Customization",
    "text": "6.3 Customization\nWe next show some other things you can do. For example, you can make lots of little plots:\n\nmy.plot + \n  facet_grid( ~ female ) + \n    ggtitle(\"School-level trend lines for their male and female students\") +\n    labs(x=\"SES\",y=\"Math Achievement\") \n\n\n\n\n\n\n\n\nOr,\n\n# random subset of schoolid\nsch &lt;- sample( unique( dat$schoolid ), 6 )\n\n# pipe into ggplot \nsch.six &lt;- dat |&gt; \n  filter(schoolid %in% sch)\n\nmy.six.plot &lt;- ggplot( sch.six, aes(y=mathach, x=ses, col=sector ) ) + \n    facet_wrap( ~ schoolid, ncol=3 ) + \n    geom_point() + stat_smooth( method=\"lm\" )\n\nmy.six.plot\n\n\n\n\n\n\n\n\nAlso shown in the above are adding titles.",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to `ggplot`</span>"
    ]
  },
  {
    "objectID": "intro_ggplot.html#themes",
    "href": "intro_ggplot.html#themes",
    "title": "6  Intro to ggplot",
    "section": "6.4 Themes",
    "text": "6.4 Themes\nYou can very quickly change the entire presentation of your plot using themes. There are pre-packaged ones, and you can make your own that you use over and over. Here we set up a theme to be used moving forward\n\nlibrary( ggthemes )\nmy_t = theme_calc() + theme( legend.position=\"bottom\", \n                             legend.direction=\"horizontal\", \n                             legend.key.width=unit(1,\"cm\")  )\ntheme_set( my_t )\n\nCompare the same plot from above, now with a new theme.\n\nmy.six.plot\n\n\n\n\n\n\n\n\nCool, no?",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to `ggplot`</span>"
    ]
  },
  {
    "objectID": "intro_ggplot.html#next-steps",
    "href": "intro_ggplot.html#next-steps",
    "title": "6  Intro to ggplot",
    "section": "6.5 Next steps",
    "text": "6.5 Next steps\nThere is a lot of information out there on ggplot and my best advice is to find code examples, and then modify them as needed. There are tutorials and blogs that walk through building plots (search for “ggplot tutorial” for example), but seeing examples seems to be the best way to learn the stuff. For example, you could use the above code for your project one quite readily. And don’t be afraid to ask how to modify plots on Piazza!\nIn particular, check out the excellent “R for Data Science’’ textbook. It extensively uses ggplot, starting here.",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to `ggplot`</span>"
    ]
  },
  {
    "objectID": "simple_plotting_tips.html",
    "href": "simple_plotting_tips.html",
    "title": "7  Some tips for making plots",
    "section": "",
    "text": "This document provides several simple plotting tips for using RStudio to make plots (primarily using the ggplot2 package in tidyverse). These are all very simple tricks that can radically enhance the legibility of a plot. This document also covers how to save your plots so you can use them in presentations and other documents, and how to control the size of your plot in a R Markdown document.\n\n8 Setting a theme\nThemes in ggplot give you a bunch of default options for plotting. For example, you can get rid of the grey background of plots with the theme_minimal() command.\n\nairquality$day_cntr = 1:nrow(airquality)\nggplot( airquality, aes( day_cntr, Ozone ) ) +\n    geom_point(na.rm=TRUE) + geom_line()\n\n\n\n\n\n\n\nggplot( airquality, aes( day_cntr, Ozone ) ) +\n    geom_point(na.rm=TRUE) + geom_line() +\n    theme_minimal()\n\n\n\n\n\n\n\n\nI also quite like Tufte theme in the ggthemes bonus library (this library is a library of various themes with styles good and bad):\n\nggplot( airquality, aes( day_cntr, Ozone ) ) +\n    geom_point(na.rm=TRUE) + geom_line() +\n    ggthemes::theme_tufte()\n\n\n\n\n\n\n\n\nYou can also set the theme globally:\n\ntheme_set( theme_minimal() )\n\n\n\n9 Formatting axes and making labels\nThe labs() command is critical for getting labels for your plot. The scale_x_continuous() (or y) allow you to control the scales on each axis. Also, there is a nice package, scales that will format your x and y-axes. Witness!\n\nlibrary( scales ) # for label_dollar(), below\nggplot( sat93, aes( expend, tot_sat ) ) +\n  geom_point() +\n  scale_x_continuous( labels = label_dollar() ) +\n  labs( title = \"Cost per pupil vs. Average SAT of state\",\n        y = \"Average SAT\",\n        x = \"Expend / Pupil\" ) +\n  theme_minimal() +\n  scale_y_continuous( limits = c( 750, 1250), \n                      breaks = c( 750, 1000, 1250 ) )\n\n\n\n\n\n\n\n\n\n\n10 Making legends using aes()\nIf you are making a bunch of lines and want to color them, you can do so by giving them all names and then letting R pick the colors for you. This also lets you have a nice legend telling you which color is which. Also note how you can put a name for your legend inside labs(). Witness!\n\nlibrary( scales ) # for label_dollar(), below\nggplot( sat93, aes( expend, tot_sat ) ) +\n  geom_point() +\n  scale_x_continuous( labels = label_dollar() ) +\n  labs( title = \"Cost per pupil vs. Average SAT of state\",\n        y = \"Average SAT\",\n        x = \"Expend / Pupil\",\n        color = \"smoother\") +\n  geom_smooth( aes( col = \"linear\" ), method=\"lm\", se=FALSE ) +\n  geom_smooth( aes( col = \"loess\" ), se=FALSE ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\" )\n\n\n\n\n\n\n\n\n\n\n11 Cropping vs. zooming in\nIf you set the scale of your plot via scale_x_continuous() you will drop data from your analysis. If you use coord_cartesian() you will zoom in on the specified window, but it will be using all the data for smoothers, etc. This can be an important difference:\n\nggplot( sat93, aes( expend, tot_sat ) ) +\n  geom_point() +\n  geom_smooth( se=FALSE ) +\n  coord_cartesian( xlim=c(4000, 5000 ) )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot( sat93, aes( expend, tot_sat ) ) +\n  geom_point() +\n  geom_smooth( se=FALSE ) +\n  scale_x_continuous( limits = c(4000, 5000 ) )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 36 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 36 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n12 Controlling size in markdown files\nYou can add fig.height=2, fig.width=7 into the header of a chunk. For example, the chunk above had:\n\n{r,r my_figure, fig.height=2, fig.width=7}\n\nYou can also set overall size in your setup chunk via\n\nknitr::opts_chunk$set(echo = TRUE, \n                      fig.width = 5,\n                      fig.height = 3,\n                      out.width = \"75%\", \n                      fig.align = \"center\")\n\nThe fig.width controls how big the figure is when plotting. The figure is then rescaled to fit into the area dictated by out.width. The “75%” can be replaced with, e.g., “4in” for 4 inches. The percent is the percent of text width of the page.\n\n\n13 Making the fonts of labels, etc., larger\nIf you make the figure smaller, the axes labels and line thicknesses and everything get larger, relatively speaking. Compare the following, with the header chunks printed out for clarity:\n\n{r,r, fig.height=2, fig.width=7, out.width=“100%”, out.height=“100%”}\nggplot( airquality, aes( day_cntr, Ozone ) ) +\n    geom_point(na.rm=TRUE) + geom_line(na.rm=TRUE)\n\n\n\n\n\n\n\n\n\n{r,r, fig.height=2, fig.width=16}\nggplot( airquality, aes( day_cntr, Ozone ) ) +\n    geom_point(na.rm=TRUE) + geom_line()\n\n\n\n\n\n\n\n\nSee how the second figure is a bigger figure rescaled to fit into a smaller space (width-wise). Everything thus looks tiny. The other plots in this document are fig.height=2, fig.width=7.\nIf the plot is small, but out.width is large, it will not scale up but instead keep to the desired size. The out.width only ensures plots are no larger than given. E.g. fig.width=3, fig.height=2, out.width=\"100%\" gives:\n\n{r,r, warning = FALSE, fig.width=3, fig.height=2, out.width=“100%”}\nggplot( airquality, aes( day_cntr, Ozone ) ) +\n    geom_point(na.rm=TRUE) + geom_line()\n\n\n\n\n\n\n\n\n\n\n14 Saving high resolution plots\nThere are three ways to export plots from RStudio. You can click on the export button on RStudio. If you say “Copy to Clipboard” you will get a bitmap plot (basically a digital photo of your plot), which will be fuzzy if you blow it up in your report. One way around that is specify a larger width and height in the dialog box before you save. “Save as Image” has the same concern.\nThe “Save to PDF” will save a vector image of your plot. Vector images remember the dots and lines used for your plot, and will be crisp if you make them large or small.\nYou can also use ggsave as follows:\n\nggplot( airquality, aes( day_cntr, Ozone ) ) +\n    geom_point(na.rm=TRUE) + geom_line()\nggsave( \"my_ac_plot.pdf\", width = 8, height=2 )\n\nThis allows you to specify the width and height and then, if you want to re-make your plot, it will save the exact same size. This is also good if you have a series of plots you want to make the same size.\nI recommend saving to PDF. The image bitmap plots always look a bit lousy.\n\n\n\n15 Think about the height of a plot.\nVery often, making your plot shorter will make it better. Big vertical distance amplifies variation and makes it hard to follow trends.\nCompare\n\n{r,r, fig.height=1.5, fig.width=7, out.width=“100%”, out.height=“100%”}\nggplot( airquality, aes( day_cntr, Ozone ) ) +\n    geom_point(na.rm=TRUE) + geom_line(na.rm=TRUE)\n\n\n\n\n\n\n\n\n\n{r,r, fig.height=5, fig.width=7, out.width=“100%”, out.height=“100%”}\nggplot( airquality, aes( day_cntr, Ozone ) ) +\n    geom_point(na.rm=TRUE) + geom_line(na.rm=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n16 Think about the x and y axis of your plot\nSimilarly, swapping the x and y axes of a plot can make labels a lot easier to read.\nConsider the following two plots:\n\nggplot( dat, aes( x = Category, y=per_students ) ) +\n    geom_bar(stat=\"identity\") +\n    theme(axis.text.x = element_text(angle = 90))  +\n    scale_y_continuous( limits = c(0,60), breaks = c(0,10,20,30,40,50,60) ) +\n    labs( y = \"Percent Student Population\", x=\"\" )\n\n\n\n\n\n\n\n\n\n{r,r, fig.height=2, fig.width = 6, out.width=“100%”}\nggplot( dat, aes( x = Category, y=per_students ) ) +\n    geom_bar(stat=\"identity\") +\n    theme(axis.text.x = element_text(angle = 90))  +\n    labs( y = \"Percent Student Population\", x=\"\" ) +\n    scale_y_continuous( limits = c(0,60), breaks = c(0,10,20,30,40,50,60) ) +\n    coord_flip() +\n    theme(axis.text.x = element_text(angle = 0))  \n\n\n\n\n\n\n\n\nFlipping is easy to do. Just add coord_flip() to your ggplot command! The choice of vertical height of the plot also matters here. Make your bars not too thick, but still a nice amount thick so your labels are well spaced.\nFor the first plot, we had to rotate the labels so they didn’t overlap. Other choices are likely possible.\nAlso notice the rotating of the axes labels. With coord_flip be careful as to what element you are controlling. Usually fiddling around will get it right in no time!\nNote: these data are from the Massachusetts Department of Elementary and Secondary Education, school year 2019-20, for the state.\n\n\n17 Controlling colors, etc\nFor this, read the “Graphics for Communication” chapter of R for DS: https://r4ds.had.co.nz/graphics-for-communication.html",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Some tips for making plots</span>"
    ]
  },
  {
    "objectID": "double_plot.html",
    "href": "double_plot.html",
    "title": "8  Plotting Two Datasets at Once",
    "section": "",
    "text": "It’s easy (though not always advisable) to plot two data sets at once with ggplot. First, we load tidyverse and our HSB data. We then create a school-level aggregate data set of just the mean SES values.\n\nlibrary(tidyverse)\nlibrary(haven)\n\n# clear memory\nrm(list = ls())\n\ntheme_set(theme_classic())\n\n# load HSB data\nhsb &lt;- read_dta(\"data/hsb.dta\") |&gt; \n  select(mathach, ses, schoolid)\n\nsch &lt;- hsb |&gt; \n  group_by(schoolid) |&gt; \n  summarise(mean_ses = mean(ses),\n            mean_mathach = mean(mathach))\n\nLet’s say we wanted to plot both the individual students and the school means. This is easy enough to do separately:\n\nggplot(hsb, aes(x = ses, y = mathach)) +\n  geom_point(alpha = 0.1)\n\n\n\n\n\n\n\nggplot(sch, aes(x = mean_ses, y = mean_mathach)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe can superimpose both plots as follows. Essentially, the first argument in ggplot provides the data, and by default, this is passed to all subsequent layers of the plot. We can override this behavior by specifying a different data set (and aesthetic mappings, if desired) within an individual layer of ggplot, such as geom_point.\n\nggplot(hsb, aes(x = ses, y = mathach)) +\n  geom_point(alpha = 0.1) +\n  geom_point(data = sch, aes(x = mean_ses, y = mean_mathach), color = \"red\")",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting Two Datasets at Once</span>"
    ]
  },
  {
    "objectID": "hist_and_bar_with_aggregated_data.html",
    "href": "hist_and_bar_with_aggregated_data.html",
    "title": "9  Plotting aggregate data with ggPlot",
    "section": "",
    "text": "9.0.1 Preface\nBefore we get started, let’s load the packages we’ll need.\nlibrary(tidyverse)\nlibrary(ggplot2)",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plotting aggregate data with ggPlot</span>"
    ]
  },
  {
    "objectID": "hist_and_bar_with_aggregated_data.html#categorical-data-and-bar-charts",
    "href": "hist_and_bar_with_aggregated_data.html#categorical-data-and-bar-charts",
    "title": "9  Plotting aggregate data with ggPlot",
    "section": "10.1 Categorical data and bar charts",
    "text": "10.1 Categorical data and bar charts\nWe can do the same thing with a categorical variable, course grades. First we’ll plot using the raw data, and then we’ll switch to the aggregated version. For the aggregated case, we’ll again rely on the stat argument of the geom_bar() function.\n\n# Plot from raw data\n# For a categorical variable, we use geom_bar\ndf %&gt;% filter(course=='ela') %&gt;%\n  ggplot(aes(x=grade)) +\n  geom_bar()\n\n\n\n\n\n\n\n# Now let's aggregate the data\ngrade_agg &lt;- df %&gt;% filter(course=='ela') %&gt;% \n  group_by(grade) %&gt;%\n  summarize(n=n())\n\n# Now reproduce the bar plot from the aggregated data\nggplot(grade_agg, aes(x=grade, y=n)) + geom_bar(stat='identity')\n\n\n\n\n\n\n\n\nLooks exactly the same! (Except for the y-axis label.)",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plotting aggregate data with ggPlot</span>"
    ]
  },
  {
    "objectID": "hist_and_bar_with_aggregated_data.html#a-closing-thought",
    "href": "hist_and_bar_with_aggregated_data.html#a-closing-thought",
    "title": "9  Plotting aggregate data with ggPlot",
    "section": "10.2 A Closing Thought",
    "text": "10.2 A Closing Thought\nSometimes the data we get have already been aggregated. In this case, we might still want to visualize distributions of the variables that have been grouped. The above guide introduces the stat argument of geom_bar() as a way to get this done.",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plotting aggregate data with ggPlot</span>"
    ]
  },
  {
    "objectID": "heatmap_jitter_alpha.html",
    "href": "heatmap_jitter_alpha.html",
    "title": "10  Jitter and heatmap examples",
    "section": "",
    "text": "11 Jitter examples\n\n# Histogram of ouput variable (INTEG)\nhist(Schooldata$INTEG)\n\n\n\n\n\n\n\n# Histogram of predictor variable (PER_FSM)\nhist(Schooldata$PER_FSM)\n\n\n\n\n\n\n\n# Scatterplot of ouput variable (INTEG) on predictor (FSM)\nggplot(Schooldata, aes(x = PER_FSM, y = INTEG)) +\n  geom_point() +\n  labs(title= \"Model 1 - using the variance from perfect represenation vs Free School Meals\",\n       x = \"Percentage of student in the school on Free School Meals (FSM)\",\n       y = \"Percentage deviance from perfectly reflective of ethnic groups in local area\")\n\n\n\n\n\n\n\nfit1 &lt;- lm(INTEG ~ PER_FSM, data=Schooldata)\nsummary(fit1)\n\n\nCall:\nlm(formula = INTEG ~ PER_FSM, data = Schooldata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.901 -17.485  -3.918  12.352 115.029 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 54.10637    0.86768  62.358   &lt;2e-16 ***\nPER_FSM     -0.09395    0.03680  -2.553   0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.4 on 2485 degrees of freedom\nMultiple R-squared:  0.002616,  Adjusted R-squared:  0.002215 \nF-statistic: 6.518 on 1 and 2485 DF,  p-value: 0.01074\n\n\nWe can try jittering more (jittering here is not a good choice, by the way). The width and height control how much jitter to do both left-right and up-down.\n\nggplot(Schooldata, aes(x = PER_FSM, y = INTEG)) +\n  geom_jitter(width = 5, height=5) +\n  labs(title= \"Model 1 - using the variance from perfect represenation vs Free School Meals\",\n       x = \"Percentage of student in the school on Free School Meals (FSM)\",\n       y = \"Percentage deviance from perfectly reflective of ethnic groups in local area\")\n\n\n\n\n\n\n\nggplot(Schooldata, aes(x = PER_FSM, y = INTEG)) +\n  geom_jitter(width = 20, height=20) +\n  labs(title= \"Model 1 - using the variance from perfect represenation vs Free School Meals\",\n       x = \"Percentage of student in the school on Free School Meals (FSM)\",\n       y = \"Percentage deviance from perfectly reflective of ethnic groups in local area\")\n\n\n\n\n\n\n\nggplot(Schooldata, aes(x = PER_FSM, y = INTEG)) +\n  geom_jitter(width = 100, height=100) +\n  labs(title= \"Model 1 - using the variance from perfect represenation vs Free School Meals\",\n       x = \"Percentage of student in the school on Free School Meals (FSM)\",\n       y = \"Percentage deviance from perfectly reflective of ethnic groups in local area\")\n\n\n\n\n\n\n\n\nYou can also generate jittered data with jitter():\n\na = 1:10\na\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\njitter( a )\n\n [1] 0.9101964 2.0495048 2.8895016 4.0503323 5.1828552 5.9413801 6.9465185\n [8] 8.1875840 9.1974467 9.9710692\n\njitter( a, factor = 0.5 )\n\n [1] 1.043054 2.085757 2.941163 3.998704 5.006021 5.900659 7.070870 8.037672\n [9] 8.958997 9.981389\n\njitter( a, factor = 0.0005 )\n\n [1] 0.9999841 1.9999137 2.9999772 3.9999379 5.0000341 6.0000208 6.9999914\n [8] 8.0000790 8.9999071 9.9999341\n\n## Try alpha\n\nggplot(Schooldata, aes(x = PER_FSM, y = INTEG)) +\n  geom_point(alpha = 0.5) +\n  labs(title= \"Model 1 - using the variance from perfect represenation vs Free School Meals\",\n       x = \"Percentage of student in the school on Free School Meals (FSM)\",\n       y = \"Percentage deviance from perfectly reflective of ethnic groups in local area\")\n\n\n\n\n\n\n\nggplot(Schooldata, aes(x = PER_FSM, y = INTEG)) +\n  geom_point(alpha = 0.25) +\n  labs(title= \"Model 1 - using the variance from perfect represenation vs Free School Meals\",\n       x = \"Percentage of student in the school on Free School Meals (FSM)\",\n       y = \"Percentage deviance from perfectly reflective of ethnic groups in local area\")\n\n\n\n\n\n\n\nggplot(Schooldata, aes(x = PER_FSM, y = INTEG)) +\n  geom_point(alpha = 0.05) +\n  labs(title= \"Model 1 - using the variance from perfect represenation vs Free School Meals\",\n       x = \"Percentage of student in the school on Free School Meals (FSM)\",\n       y = \"Percentage deviance from perfectly reflective of ethnic groups in local area\")\n\n\n\n\n\n\n\n\n\n\n12 Making a heat map\n\nggplot(Schooldata,aes(x = PER_FSM, y = INTEG))+\n  stat_density2d(aes(alpha=..level..), geom=\"polygon\") +\n  labs(title= \"Model 1 - using the variance from perfect represenation vs Free School Meals\",\n       x = \"Percentage of student in the school on Free School Meals (FSM)\",\n       y = \"Percentage deviance from perfectly reflective of ethnic groups in local area\") +\n  geom_point(colour=\"red\",alpha=0.05)+\n  theme_bw()\n\nWarning: The dot-dot notation (`..level..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(level)` instead.",
    "crumbs": [
      "On Plotting",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Jitter and heatmap examples</span>"
    ]
  },
  {
    "objectID": "manipulation_examples.html",
    "href": "manipulation_examples.html",
    "title": "11  Basic Data Manipulation with tidyverse",
    "section": "",
    "text": "11.1 filter() (Grab the rows you want, 5.1)\nfilter( table1, year &gt; 1999 )\n\n# A tibble: 3 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  2000   2666   20595360\n2 Brazil       2000  80488  174504898\n3 China        2000 213766 1280428583\nRemember, if you want to save the results of your command, you need to put it in a new variable, like so:\nmy.table &lt;- filter( table1, year &gt; 1999 )\nmy.table\n\n# A tibble: 3 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  2000   2666   20595360\n2 Brazil       2000  80488  174504898\n3 China        2000 213766 1280428583\nWhen you do something like this, you should see it appear in your workplace.",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Basic Data Manipulation with tidyverse</span>"
    ]
  },
  {
    "objectID": "manipulation_examples.html#arrange-sort-your-rows-the-way-you-want-5.2",
    "href": "manipulation_examples.html#arrange-sort-your-rows-the-way-you-want-5.2",
    "title": "11  Basic Data Manipulation with tidyverse",
    "section": "11.2 arrange() (Sort your rows the way you want, 5.2)",
    "text": "11.2 arrange() (Sort your rows the way you want, 5.2)\n\narrange( table1, cases )\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\narrange( table1, desc( country ), population )\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 China        1999 212258 1272915272\n2 China        2000 213766 1280428583\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 Afghanistan  1999    745   19987071\n6 Afghanistan  2000   2666   20595360",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Basic Data Manipulation with tidyverse</span>"
    ]
  },
  {
    "objectID": "manipulation_examples.html#select-grab-the-columns-you-want-5.3",
    "href": "manipulation_examples.html#select-grab-the-columns-you-want-5.3",
    "title": "11  Basic Data Manipulation with tidyverse",
    "section": "11.3 select() (Grab the columns you want, 5.3)",
    "text": "11.3 select() (Grab the columns you want, 5.3)\n\nselect( table1, country, population )\n\n# A tibble: 6 × 2\n  country     population\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Afghanistan   19987071\n2 Afghanistan   20595360\n3 Brazil       172006362\n4 Brazil       174504898\n5 China       1272915272\n6 China       1280428583",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Basic Data Manipulation with tidyverse</span>"
    ]
  },
  {
    "objectID": "manipulation_examples.html#mutate-make-new-variables-out-of-your-old-ones-5.4",
    "href": "manipulation_examples.html#mutate-make-new-variables-out-of-your-old-ones-5.4",
    "title": "11  Basic Data Manipulation with tidyverse",
    "section": "11.4 mutate() (Make new variables out of your old ones, 5.4)",
    "text": "11.4 mutate() (Make new variables out of your old ones, 5.4)\n\ntable1 &lt;- mutate( table1, case.per.1000 = 1000 * cases / population )\ntable1\n\n# A tibble: 6 × 5\n  country      year  cases population case.per.1000\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071        0.0373\n2 Afghanistan  2000   2666   20595360        0.129 \n3 Brazil       1999  37737  172006362        0.219 \n4 Brazil       2000  80488  174504898        0.461 \n5 China        1999 212258 1272915272        0.167 \n6 China        2000 213766 1280428583        0.167 \n\n\n(We will use this new variable later, so I am saving it in our table)",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Basic Data Manipulation with tidyverse</span>"
    ]
  },
  {
    "objectID": "manipulation_examples.html#group_by-and-summarize-summarize-your-data-by-subgroup-5.6",
    "href": "manipulation_examples.html#group_by-and-summarize-summarize-your-data-by-subgroup-5.6",
    "title": "11  Basic Data Manipulation with tidyverse",
    "section": "11.5 group_by() and summarize (Summarize your data by subgroup, 5.6)",
    "text": "11.5 group_by() and summarize (Summarize your data by subgroup, 5.6)\n\ntbl &lt;- group_by( table1, country )\nsummarize( tbl, av.pop = mean( population ), av.cases = mean( cases ) )\n\n# A tibble: 3 × 3\n  country          av.pop av.cases\n  &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n1 Afghanistan   20291216.    1706.\n2 Brazil       173255630    59112.\n3 China       1276671928.  213012 \n\n\nSame thing, with the pipe!\n\ntable1 %&gt;% group_by( country ) %&gt;%\n    summarize( av.pop = mean( population ), av.cases = mean( cases ) )\n\n# A tibble: 3 × 3\n  country          av.pop av.cases\n  &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n1 Afghanistan   20291216.    1706.\n2 Brazil       173255630    59112.\n3 China       1276671928.  213012",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Basic Data Manipulation with tidyverse</span>"
    ]
  },
  {
    "objectID": "manipulation_examples.html#special-grouped-mutates-making-new-variables-within-subgroups-5.6",
    "href": "manipulation_examples.html#special-grouped-mutates-making-new-variables-within-subgroups-5.6",
    "title": "11  Basic Data Manipulation with tidyverse",
    "section": "11.6 Special: grouped mutates (Making new variables within subgroups, 5.6)",
    "text": "11.6 Special: grouped mutates (Making new variables within subgroups, 5.6)\nThis combo is for doing things like group mean centering your data:\n\ntable1 %&gt;% group_by( year ) %&gt;% mutate( case.per.1000.cent = case.per.1000 - mean( case.per.1000 ) )\n\n# A tibble: 6 × 6\n# Groups:   year [2]\n  country      year  cases population case.per.1000 case.per.1000.cent\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;              &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071        0.0373            -0.104 \n2 Afghanistan  2000   2666   20595360        0.129             -0.123 \n3 Brazil       1999  37737  172006362        0.219              0.0783\n4 Brazil       2000  80488  174504898        0.461              0.209 \n5 China        1999 212258 1272915272        0.167              0.0256\n6 China        2000 213766 1280428583        0.167             -0.0856\n\n\nAnd a plot\n\nggplot( table1, aes( x=year, y=case.per.1000, col=country ) ) + geom_line() + geom_point()",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Basic Data Manipulation with tidyverse</span>"
    ]
  },
  {
    "objectID": "intro_linear_regression.html",
    "href": "intro_linear_regression.html",
    "title": "12  Intro to Regression",
    "section": "",
    "text": "12.1 Simple Regression\nWe now give a walk-through on how to fit simple linear regression models in R. Linear regression is the main way researchers tend to examine the relationships between multiple variables. This document runs through some code without too much discussion, with the assumption that you are already familiar with interpretation of such models. If you want some help with the interpretation, see the suggested readings above!\nFor our walk-through, we are going to use an example dataset, RestaurantTips, that records tip amounts for a series of bills. Let’s first regress Tip on Bill. Before doing regression, we should plot the data to make sure using simple linear regression is reasonable. For kicks, we add in an automatic regression line as well by taking advantage of ggplot’s geom_smooth() method:\n# load the data into memory\ndata(RestaurantTips)\n\n# plot Tip on Bill\nggplot( RestaurantTips, aes(x = Bill, y = Tip) ) +\n    geom_point() +\n    geom_smooth( method=\"lm\", se=FALSE ) +\n    geom_smooth( method=\"loess\", se=FALSE, col=\"orange\" ) +\n    labs(title = \"Tip given Bill\")\nThat looks pretty darn linear! There are a few unusually large tips, but no extreme outliers, and variability appears to be constant at all levels of Bill , so we proceed:\n# fit the linear model\nmod &lt;- lm(Tip ~ Bill, data = RestaurantTips)\nsummary(mod)\n\n\nCall:\nlm(formula = Tip ~ Bill, data = RestaurantTips)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.391 -0.489 -0.111  0.284  5.974 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.29227    0.16616   -1.76    0.081 .  \nBill         0.18221    0.00645   28.25   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.98 on 155 degrees of freedom\nMultiple R-squared:  0.837, Adjusted R-squared:  0.836 \nF-statistic:  798 on 1 and 155 DF,  p-value: &lt;2e-16\nThe first line tells R to fit the regression. The thing on the left of the ~ is our outcome, the things on the right are our covariates or predictors. R then saves the results of all that work under the name mod (short for model - you can call it anything you want). Once we fit the model, we used summary() command to print the output to the screen.\nResults relevant to the intercept are in the (Intercept) row and results relevant to the slope are in the Bill row (Bill is the explanatory variable). The Estimate column gives the estimated coefficients, the Std. Error column gives the standard error for these estimates, the t value is simply estimate/SE, and the p-value is the result of a hypothesis test testing whether that coefficient is significantly different from 0.\nWe also see the RMSE as Residual standard error and \\(R^2\\) as Multiple R-squared. The last line of the regression output gives details relevant to an ANOVA table for testing our model against no model. It has the F-statistic, degrees of freedom, and p-value.\nYou can pull the coefficients of your model out with the coef() command:\ncoef(mod)\n\n(Intercept)        Bill \n     -0.292       0.182 \n\ncoef(mod)[1] # intercept\n\n(Intercept) \n     -0.292 \n\ncoef(mod)[2] # slope\n\n Bill \n0.182 \n\ncoef(mod)[\"Bill\"] # alternate way.\n\n Bill \n0.182\nAlternatively, you can use the tidy() function from broom to turn the regression results into a tidy data frame, which makes it easier to work with:\ntidy(mod)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -0.292   0.166       -1.76 8.06e- 2\n2 Bill           0.182   0.00645     28.2  5.24e-63\n\ntidy(mod)[[2,2]] # slope\n\n[1] 0.182\nWe can plot our regression line on top of the scatterplot manually using the geom_abline() layer in ggplot:\nggplot( RestaurantTips, aes( Bill, Tip ) ) +\n  geom_point() +\n  geom_abline( intercept = -0.292, slope =  0.182, col=\"red\" )",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Intro to Regression</span>"
    ]
  },
  {
    "objectID": "intro_linear_regression.html#multiple-regression",
    "href": "intro_linear_regression.html#multiple-regression",
    "title": "12  Intro to Regression",
    "section": "12.2 Multiple Regression",
    "text": "12.2 Multiple Regression\nWe now include the additional explanatory variables of number in party (Guests) and whether or not they pay with a credit card (Credit):\n\ntip.mod &lt;- lm(Tip ~ Bill + Guests + Credit, data=RestaurantTips )\nsummary(tip.mod)\n\n\nCall:\nlm(formula = Tip ~ Bill + Guests + Credit, data = RestaurantTips)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.384 -0.478 -0.108  0.272  5.984 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.25468    0.20273   -1.26     0.21    \nBill         0.18302    0.00846   21.64   &lt;2e-16 ***\nGuests      -0.03319    0.10282   -0.32     0.75    \nCredity      0.04217    0.18282    0.23     0.82    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.985 on 153 degrees of freedom\nMultiple R-squared:  0.838, Adjusted R-squared:  0.834 \nF-statistic:  263 on 3 and 153 DF,  p-value: &lt;2e-16\n\n\nThis output should look very similar to the output for one variable, except now there is a row corresponding to each explanatory variable. Our two-category (y, n) Credit variable was automatically converted to a 0-1 dummy variable (with “y” being 1 and “n” our baseline).\nYou can make plots and tables of your fit models. For one easy kind of regression graph, try ggeffects:\n\n# graph model 2, with Bill on X, Credit as color, and Guests held constant at the mean\nggeffect(tip.mod, terms = c(\"Bill\", \"Credit\")) |&gt; \n  plot(show_data = TRUE, show_ci = FALSE)\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n\n\n\n\n\nFor making tables, ?sec-make-regression-tables.",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Intro to Regression</span>"
    ]
  },
  {
    "objectID": "intro_linear_regression.html#categorical-variables-and-factors",
    "href": "intro_linear_regression.html#categorical-variables-and-factors",
    "title": "12  Intro to Regression",
    "section": "12.3 Categorical Variables (and Factors)",
    "text": "12.3 Categorical Variables (and Factors)\nYou can include any explanatory categorical variable in a multiple regression model, and R will automatically create corresponding 0/1 variables. For example, if you were to include gender coded as male/female, R would create a variable GenderMale that is 1 for males and 0 for females.\n\n12.3.1 Numbers Coding Categories.\nIf you have multiple levels of a category, but your levels are coded with numbers you have to be a bit careful because R can treat this as a quantitative (continuous) variable by mistake in some cases. You will know it did this if you only see the single variable on one line of your output. For categorical variables with \\(k\\) categories, you should see \\(k-1\\) lines.\nTo make a variable categorical, even if the levels are numbers, convert the variable to a factor with as.factor or factor:\n\n# load the US states data\ndata( USStates )\n\n# convert Region to a factor\nUSStates &lt;- USStates |&gt; \n  mutate(Region = factor(Region))\n\n\n\n12.3.2 Setting new baselines.\nWe can reorder the levels if desired (the first is our baseline).\n\nlevels( USStates$Region )\n\n[1] \"MW\" \"NE\" \"S\"  \"W\" \n\nUSStates$Region = relevel(USStates$Region, \"S\" )\nlevels( USStates$Region )\n\n[1] \"S\"  \"MW\" \"NE\" \"W\" \n\n\nNow any regression will use the south as baseline.\n\n\n12.3.3 Testing for significance of a categorical variable.\nWhen deciding whether to keep a categorical variable, we need to test how important all the dummy variables for that category are to the model all at once. We do this with ANOVA. Here we examine whether region is useful for predicting the percent vote for Clinton in 2016:\n\nmlm = lm( ClintonVote ~ Region, data=USStates)\nanova( mlm )\n\nAnalysis of Variance Table\n\nResponse: ClintonVote\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nRegion     3   1643     548    6.99 0.00057 ***\nResiduals 46   3603      78                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt is quite important.\nWe can also compare for region beyond some other variable:\n\nmlm2 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath, data=USStates)\n\nmlm3 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath + Region, data=USStates)\nanova( mlm2, mlm3 )\n\nAnalysis of Variance Table\n\nModel 1: ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n    EighthGradeMath\nModel 2: ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n    EighthGradeMath + Region\n  Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)  \n1     46 3287                           \n2     43 2649  3       638 3.45  0.025 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRegion is still important, beyond including some further controls. Interpreting this mess of a regression is not part of this document; this document shows you how to run regressions but it doesn’t discuss whether you should or not.\n\n\n12.3.4 Missing levels in a factor\nR often treats categorical variables as factors. This is often useful, but sometimes annoying. A factor has different levels which are the different values it can be. For example:\n\ndata(FishGills3)\nlevels(FishGills3$Calcium)\n\n[1] \"\"       \"High\"   \"Low\"    \"Medium\"\n\ntable(FishGills3$Calcium)\n\n\n         High    Low Medium \n     0     30     30     30 \n\n\nNote the weird nameless level; it also has no actual observations in it. Nevertheless, if you make a boxplot, you will get an empty plot in addition to the other three. This error was likely due to some past data entry issue. You can drop the unused level:\n\nFishGills3$Calcium = droplevels(FishGills3$Calcium)\n\nYou can also turn a categorical variable into a numeric one like so:\n\nsummary( FishGills3$Calcium )\n\n  High    Low Medium \n    30     30     30 \n\nasnum = as.numeric( FishGills3$Calcium )\nasnum\n\n [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3\n[39] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[77] 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nRegression on only a categorical variable is fine:\n\nmylm = lm( GillRate ~ Calcium, data=FishGills3 )\nmylm\n\n\nCall:\nlm(formula = GillRate ~ Calcium, data = FishGills3)\n\nCoefficients:\n  (Intercept)     CalciumLow  CalciumMedium  \n         58.2           10.3            0.5  \n\n\nR has made you a bunch of dummy variables automatically. Here “high” is the baseline, selected automatically. We can also force it so there is no baseline by removing the intercept, in which case the coefficients are the means of each group.\n\nmymm = lm( GillRate ~ 0 + Calcium, data=FishGills3 )\nmymm\n\n\nCall:\nlm(formula = GillRate ~ 0 + Calcium, data = FishGills3)\n\nCoefficients:\n  CalciumHigh     CalciumLow  CalciumMedium  \n         58.2           68.5           58.7",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Intro to Regression</span>"
    ]
  },
  {
    "objectID": "intro_linear_regression.html#some-extensions-optional",
    "href": "intro_linear_regression.html#some-extensions-optional",
    "title": "12  Intro to Regression",
    "section": "12.4 Some extensions (optional)",
    "text": "12.4 Some extensions (optional)\n\n12.4.1 Confidence Intervals\nTo get confidence intervals around each parameter in your model, try this:\n\nconfint(tip.mod)\n\n             2.5 % 97.5 %\n(Intercept) -0.655  0.146\nBill         0.166  0.200\nGuests      -0.236  0.170\nCredity     -0.319  0.403\n\n\nYou can also create them easily using tidy and mutate:\n\ntip.mod |&gt; \n  tidy() |&gt; \n  mutate(upper = estimate + 1.96*std.error,\n         lower = estimate - 1.96*std.error)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic  p.value upper  lower\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)  -0.255    0.203      -1.26  2.11e- 1 0.143 -0.652\n2 Bill          0.183    0.00846    21.6   2.07e-48 0.200  0.166\n3 Guests       -0.0332   0.103      -0.323 7.47e- 1 0.168 -0.235\n4 Credity       0.0422   0.183       0.231 8.18e- 1 0.400 -0.316\n\n\n\n\n12.4.2 Prediction\nSuppose a server at this bistro is about to deliver a $20 bill, and wants to predict their tip. They can get a predicted value and 95% (this is the default level, change with level) prediction interval with\n\nnew.dat = data.frame( Bill = c(20) )\npredict(mod,new.dat,interval = \"prediction\")\n\n   fit  lwr  upr\n1 3.35 1.41 5.29\n\n\nThey should expect a tip somewhere between $1.41 and $5.30.\nIf we know a bit more we can use our more complex model called tip.mod from above:\n\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod,new.dat,interval = \"prediction\")\n\n   fit  lwr  upr\n1 3.37 1.41 5.34\n\n\nThis is the predicted tip for one guest paying with cash for a $20 tip. It is wider than our original interval because our model is a bit more unstable (it turns out guest number and credit card aren’t that relevant or helpful).\nCompare the prediction interval to the confidence interval\n\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod, new.dat, interval = \"confidence\")\n\n   fit  lwr  upr\n1 3.37 3.09 3.65\n\n\nThis predicts the mean tip for all single guests who pay a $20 bill with cash. Our interval is smaller because we are generating a confidence interval for where the mean is, and are ignoring that individuals will vary around that mean. Confidence intervals are different from prediction intervals.\n\n\n12.4.3 Removing Outliers\nIf you can identify which rows the outliers are on, you can do this by hand (say the rows are 5, 10, 12).\n\nnew.data = old.data[ -c(5,10,12), ]\nlm( Y ~ X, data=new.data )\n\nSome technical details: The c(5,10,12) is a list of 3 numbers. The c() is the concatenation function that takes things makes lists out of them. The “-list” notation means give me my old data, but without rows 5, 10, and 12. Note the comma after the list. This is because we identify elements in a dataframe with row, column notation. So old.data[1,3] would be row 1, column 3.\nIf you notice your points all have X bigger than some value, say 20.5, you could use filtering to keep everything less than some value:\n\nnew.data = filter( old.data, X &lt;= 20.5 )\n\n\n\n12.4.4 Missing data\nIf you have missing data, lm will automatically drop those cases because it doesn’t know what else to do. It will tell you this, however, with the summary command.\n\ndata(AllCountries)\ndev.lm = lm( BirthRate ~ Rural + Health + ElderlyPop, data=AllCountries )\nsummary( dev.lm  )\n\n\nCall:\nlm(formula = BirthRate ~ Rural + Health + ElderlyPop, data = AllCountries)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.592  -3.728  -0.791   3.909  16.218 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26.5763     1.6795   15.82  &lt; 2e-16 ***\nRural         0.0985     0.0224    4.40  1.9e-05 ***\nHealth       -0.0995     0.0930   -1.07     0.29    \nElderlyPop   -1.0249     0.0881  -11.64  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.83 on 174 degrees of freedom\n  (39 observations deleted due to missingness)\nMultiple R-squared:  0.663, Adjusted R-squared:  0.657 \nF-statistic:  114 on 3 and 174 DF,  p-value: &lt;2e-16\n\n\n\n\n12.4.5 Residual plots and model fit\nIf we throw out model into the plot function, we get some nice regression diagnostics.\n\nplot(tip.mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo generate classic model fit diagnostics with more control, we need to calculate residuals, make a residual versus fitted values plot, and make a histogram of the residuals. We can make some quick and dirty plots with qplot (standing for “quick plot”) like so:\n\nqplot(tip.mod$fit, tip.mod$residuals )\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nand\n\nqplot(tip.mod$residuals, bins=30)\n\n\n\n\n\n\n\n\nWe see no real pattern other than some extreme outliers. The residual histogram suggests we are not really normally distributed, so we should treat our SEs and \\(p\\)-values with caution. These plots are the canonical “model-checking’’ plots you might use.\nAnother is the “fitted outcomes vs. actual outcomes’’ plot of:\n\npredicted = predict( dev.lm )\nactual = dev.lm$model$BirthRate\nqplot( actual, predicted, main=\"Fit vs. actual Birth Rate\" )\n\n\n\n\n\n\n\n\nNote the dev.lm variable has a model variable inside it. This is a data frame of the used data for the model (i.e., if cases were dropped due to missingness, they will not be in the model). We then grab the birth rates from this, and make a scatterplot. If we tried to skip this, and use the original data, we would get an error because our original data set has some observations that were dropped.\nNote we can’t just add our predictions to AllCountries since we would get an error due to this dropped data issue:\n\nAllCountries$predicted = predict( dev.lm )\n\nError in `$&lt;-.data.frame`(`*tmp*`, predicted, value = c(`1` = 31.630301617421,  : \n  replacement has 179 rows, data has 217\nWe can, however, predict like this:\n\nAllCountries$predicted = predict( dev.lm, newdata=AllCountries )\n\nThe newdata tells predict to generate a prediction for each row in AllCountries rather than each row in the left over data after lm dropped cases with missing values.",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Intro to Regression</span>"
    ]
  },
  {
    "objectID": "doing_things_over_and_over.html",
    "href": "doing_things_over_and_over.html",
    "title": "13  Doing things over and over again",
    "section": "",
    "text": "14 The replicate() command\nThis simple command repeats a line of code a given number of times. If the line of code gives you a number back each time it is run, then you will end up with a list of numbers.\nTo illustrate, say we want to look at rolling dice. Here is some code that provides a function that will roll some number of 6-sided dice:\nroll_dice = function( ndice ) {\n    rolls = sample( 1:6, ndice, replace=TRUE )\n    sum( rolls )\n}\n\n# Roll a single die\nroll_dice( 1 )\n\n[1] 3\n\n# Roll 3 dice and add them up.\nroll_dice( 3 )\n\n[1] 14\nIf we want to get the sum of three dice over and over, we can replicate:\nrolls = replicate( 10, roll_dice( 3 ) )\nrolls\n\n [1] 11  7 12  5  9 13 13 12  9 14\nNote how the rolls variable is a nice numeric vector, easy to work with. It is easy to do calculations with it, like take the average:\nmean( rolls )\n\n[1] 10.5\nHere we use this to see how often we roll above a 15:\nrolls = replicate( 10000, roll_dice( 3 ) )\nmean( rolls &gt; 15 )\n\n[1] 0.0464",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Doing things over and over again</span>"
    ]
  },
  {
    "objectID": "doing_things_over_and_over.html#warning-replicate-doesnt-do-well-with-fancy-functions",
    "href": "doing_things_over_and_over.html#warning-replicate-doesnt-do-well-with-fancy-functions",
    "title": "13  Doing things over and over again",
    "section": "15.1 Warning: replicate() doesn’t do well with fancy functions",
    "text": "15.1 Warning: replicate() doesn’t do well with fancy functions\nThe replicate() command doesn’t act nice in the following:\n\nrolls = replicate( 2, roll_dice_extended(3) )\nrolls\n\n, , 1\n\n  mean     median max\n1 4.333333 5      6  \n\n, , 2\n\n  mean     median max\n1 3.333333 4      5  \n\n\nThat doesn’t look like a fun thing to work with. (It is a 3 dimensional array of output, in case you are wondering.) Use rerun + bind_rows; it is easier to control and understand.",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Doing things over and over again</span>"
    ]
  },
  {
    "objectID": "doing_things_over_and_over.html#take-away",
    "href": "doing_things_over_and_over.html#take-away",
    "title": "13  Doing things over and over again",
    "section": "15.2 Take-away",
    "text": "15.2 Take-away\nIf your function that you want to repeat returns a single number with each call, use replicate(). If it returns more than one thing, use the rerun() + bind_rows() combination.",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Doing things over and over again</span>"
    ]
  },
  {
    "objectID": "doing_things_over_and_over.html#relocate",
    "href": "doing_things_over_and_over.html#relocate",
    "title": "13  Doing things over and over again",
    "section": "18.1 relocate()",
    "text": "18.1 relocate()\nNot really mapping specific, but still a nice way to move a variable to the start of a data frame.\n\nrelocate( resL, freq )\n\n# A tibble: 33 × 3\n    freq ndice roll \n   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;\n 1   162     1 1    \n 2   154     1 2    \n 3   171     1 3    \n 4   178     1 4    \n 5   163     1 5    \n 6   172     1 6    \n 7    23     2 1    \n 8    76     2 2    \n 9   150     2 3    \n10   198     2 4    \n# ℹ 23 more rows",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Doing things over and over again</span>"
    ]
  },
  {
    "objectID": "doing_things_over_and_over.html#pull",
    "href": "doing_things_over_and_over.html#pull",
    "title": "13  Doing things over and over again",
    "section": "18.2 pull()",
    "text": "18.2 pull()\nThis will grab a column from a data frame in a list of pipe commands, which can make it easier to plug into some other tools.\nHere we pull the roll column and then hand it to the table() command to count the number of instances of each roll.\n\nresL %&gt;% pull( roll ) %&gt;%\n  table()\n\n.\n1 2 3 4 5 6 \n4 5 6 6 6 6",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Doing things over and over again</span>"
    ]
  },
  {
    "objectID": "doing_things_over_and_over.html#set_names-list-names",
    "href": "doing_things_over_and_over.html#set_names-list-names",
    "title": "13  Doing things over and over again",
    "section": "18.3 set_names( list, names )",
    "text": "18.3 set_names( list, names )\nOn the fly name a list before handing to map!\n\ndice = 2:6\ndice %&gt;% set_names( paste0( dice, \" dice\" ) ) %&gt;%\n  map_df( roll_dice_extended, .id = \"scenario\" )\n\n  scenario     mean median max\n1   2 dice 2.000000    2.0   2\n2   3 dice 3.666667    4.0   5\n3   4 dice 4.750000    6.0   6\n4   5 dice 3.000000    2.0   5\n5   6 dice 5.333333    5.5   6",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Doing things over and over again</span>"
    ]
  },
  {
    "objectID": "doing_things_over_and_over.html#unpack-and-pack",
    "href": "doing_things_over_and_over.html#unpack-and-pack",
    "title": "13  Doing things over and over again",
    "section": "18.4 unpack() and pack()",
    "text": "18.4 unpack() and pack()\nThese will translate a data frame column into individual columns. You can end up with a data frame column if you use map to make a new column in your data:\n\nscenarios = tibble( n_dice = 1:6 )\nscenarios = scenarios %&gt;%\n  mutate( result = map_df( n_dice, roll_dice_extended ) )\nscenarios\n\n# A tibble: 6 × 2\n  n_dice result$mean $median  $max\n   &lt;int&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1      1        1        1       1\n2      2        3.5      3.5     5\n3      3        2.33     1       5\n4      4        2.5      1.5     6\n5      5        4        4       5\n6      6        3        2       6\n\n\nNotice the weird $ in the printout? This is because the three columns are inside the dataframe of result. You can unpack it like so:\n\nunpack( scenarios, result )\n\n# A tibble: 6 × 4\n  n_dice  mean median   max\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1      1  1       1       1\n2      2  3.5     3.5     5\n3      3  2.33    1       5\n4      4  2.5     1.5     6\n5      5  4       4       5\n6      6  3       2       6\n\n\nto get your nice, normal dataframe.",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Doing things over and over again</span>"
    ]
  },
  {
    "objectID": "demo_cluster_boot.html",
    "href": "demo_cluster_boot.html",
    "title": "14  Demonstration of the Cluster Bootstrap",
    "section": "",
    "text": "15 The Cluster Bootstrap\nThe cluster bootstrap is a resampling technique commonly used in statistics and machine learning for estimating the variability of statistical estimators, such as mean, variance, or regression coefficients. It is especially useful when the underlying data have a complex dependence structure.\nThe main idea of the cluster bootstrap is to resample clusters of data points rather than individual data points. A cluster is a group of data points that are correlated with each other, for example, spatially adjacent data points or data points within the same experimental unit.\nTo perform the cluster bootstrap, we follow these steps:\nThe cluster bootstrap is particularly useful when the underlying data have a complex correlation structure that cannot be easily accounted for using standard resampling methods, such as the simple random sampling bootstrap.\nThe following script is used to explore data from the Tenn Star education randomized controlled trial (RCT). It loads student-level and teacher-level data, merges them by the class ID variable, deletes observations with missing data, and creates an indicator variable for whether the student was in a small class (this is the treatment). The script then fits a linear regression model to estimate the impact of being in a small class on math scores after kindergarten, using small class status, student birth quarter, teacher education level, and teacher experience as predictors.\nHowever, the model is not adjusted for clustering at the class level, so the standard errors may be wrong. To correct for this, we offer two solutions. First, the sandwich package is used to calculate cluster-robust standard errors. This is the classic econometric solution for clustering in a regression. Second, we demonstrate cluster bootstrapping for estimating the standard errors of the model coefficients.",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Demonstration of the Cluster Bootstrap</span>"
    ]
  },
  {
    "objectID": "demo_cluster_boot.html#the-tenn-star-data",
    "href": "demo_cluster_boot.html#the-tenn-star-data",
    "title": "14  Demonstration of the Cluster Bootstrap",
    "section": "15.1 The Tenn Star Data",
    "text": "15.1 The Tenn Star Data\nThe Tennessee Star Experiment was a large-scale randomized controlled trial (RCT) conducted in Tennessee, USA, in the mid-1980s. The experiment aimed to evaluate the effectiveness of class size reduction on student outcome. Students and teachers were randomized to different class sizes, making the treatment assignment effectively assigned at the cluster level when looking at student outcomes.\nWe load and prepare the data as so:\n\nstud &lt;- read.csv('data/tenn_stud_dat.csv') ## student-level data\nteach &lt;- read.csv('data/tenn_teach_dat.csv') ## teacher-level data\ndat &lt;- merge(stud, teach, by=\"clid\")\ndat &lt;- na.omit(dat) ## for simplicity, we'll delete everyone missing any variables\n\n## create an indicator for being in a small class\ndat$small_class &lt;- ifelse( dat$cltypek == 'small class', \"yes\", \"no\" )\n\n# Drop some extra variables\ndat &lt;- dplyr::select( dat, -id, -cltypek, -sesk )\n\nhead( dat )\n\n  clid   ssex srace                  sbirthq sbirthy treadssk tmathssk\n1    1   male black 2nd qtr - april,may,june    1980      445      489\n2    1   male black  3rd qtr - july,aug,sept    1980      393      429\n3    1   male black    4th qtr - oct,nov,dec    1978      395      429\n4    1   male black 2nd qtr - april,may,june    1980      403      405\n5    1 female black 2nd qtr - april,may,june    1980      424      500\n6    1   male black  3rd qtr - july,aug,sept    1980      414      444\n      hdegk      cladk totexpk tracek small_class\n1 bachelors apprentice       1  black          no\n2 bachelors apprentice       1  black          no\n3 bachelors apprentice       1  black          no\n4 bachelors apprentice       1  black          no\n5 bachelors apprentice       1  black          no\n6 bachelors apprentice       1  black          no\n\nlength(unique(dat$clid)) ## 196 classes\n\n[1] 196\n\nnrow(dat) ## 3219 students\n\n[1] 3219\n\n\nOur research question is whether student math achievment was higher for kids in small classrooms vs. large ones.",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Demonstration of the Cluster Bootstrap</span>"
    ]
  },
  {
    "objectID": "demo_cluster_boot.html#the-wrong-method",
    "href": "demo_cluster_boot.html#the-wrong-method",
    "title": "14  Demonstration of the Cluster Bootstrap",
    "section": "15.2 The Wrong Method",
    "text": "15.2 The Wrong Method\nIf we use simple regression we are not taking the correlation of students within a given teacher into account. I.e., say a teacher happened to be effective. Then this teacher being assigned to a small class would have a positive impact on a bunch of treated students due to the single teacher. The student outcomes are correlated with each other by the single teacher. We would need to take this into account when calculating uncertainty: the students are not independent.\n\nmod &lt;- lm(tmathssk ~ small_class + sbirthq + hdegk + totexpk, \n          data = dat)\nres_OLS &lt;- tidy( mod )\nknitr::kable( res_OLS, digits = 2 )\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n486.49\n2.33\n208.38\n0.00\n\n\nsmall_classyes\n6.39\n1.69\n3.79\n0.00\n\n\nsbirthq2nd qtr - april,may,june\n-5.64\n2.33\n-2.42\n0.02\n\n\nsbirthq3rd qtr - july,aug,sept\n-13.85\n2.27\n-6.11\n0.00\n\n\nsbirthq4th qtr - oct,nov,dec\n10.19\n2.49\n4.08\n0.00\n\n\nhdegkmaster +\n-2.71\n5.44\n-0.50\n0.62\n\n\nhdegkmasters\n-5.25\n1.82\n-2.89\n0.00\n\n\nhdegkspecialist\n16.98\n7.62\n2.23\n0.03\n\n\ntotexpk\n0.58\n0.15\n3.88\n0.00\n\n\n\n\n\nTo be clear, the above regression gives a reasonable estimate of the impact of small class size, but the standard errors, and therefore p-values, etc., are wrong.",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Demonstration of the Cluster Bootstrap</span>"
    ]
  },
  {
    "objectID": "demo_cluster_boot.html#cluster-robust-standard-errors",
    "href": "demo_cluster_boot.html#cluster-robust-standard-errors",
    "title": "14  Demonstration of the Cluster Bootstrap",
    "section": "15.3 Cluster robust standard errors",
    "text": "15.3 Cluster robust standard errors\nOne statistical way of handling this is with the lm_robust package that uses the sandwich package that does cluster robust standard errors:\n\nlibrary( estimatr )\nmod_CRVE &lt;- lm_robust(tmathssk ~ small_class + sbirthq + hdegk + totexpk, \n                 clusters = dat$clid,\n                 data = dat) ## another way to get the same result (more or less)\nres_CRVE &lt;- tidy(mod_CRVE) %&gt;%\n  dplyr::select( term, estimate, std.error, p.value )\nknitr::kable( res_CRVE, digits = 2 )\n\n\n\n\nterm\nestimate\nstd.error\np.value\n\n\n\n\n(Intercept)\n486.49\n4.26\n0.00\n\n\nsmall_classyes\n6.39\n3.86\n0.10\n\n\nsbirthq2nd qtr - april,may,june\n-5.64\n2.27\n0.01\n\n\nsbirthq3rd qtr - july,aug,sept\n-13.85\n2.35\n0.00\n\n\nsbirthq4th qtr - oct,nov,dec\n10.19\n2.48\n0.00\n\n\nhdegkmaster +\n-2.71\n11.50\n0.83\n\n\nhdegkmasters\n-5.25\n4.15\n0.21\n\n\nhdegkspecialist\n16.98\n15.10\n0.37\n\n\ntotexpk\n0.58\n0.35\n0.11",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Demonstration of the Cluster Bootstrap</span>"
    ]
  },
  {
    "objectID": "demo_cluster_boot.html#cluster-bootstrap",
    "href": "demo_cluster_boot.html#cluster-bootstrap",
    "title": "14  Demonstration of the Cluster Bootstrap",
    "section": "15.4 Cluster Bootstrap",
    "text": "15.4 Cluster Bootstrap\nAnother way is to use the cluster bootstrap. It is a versitile method for getting standard errors on data that is clustered, as it keeps clusters intact.\nWe write an analysis function as follows:\n\nmy_analysis &lt;- function( the_dat ) {\n    mod &lt;- lm(tmathssk ~  small_class + sbirthq + hdegk + totexpk,\n              data = the_dat)\n    broom::tidy(mod)\n}\n\nmy_analysis( dat )\n\n# A tibble: 9 × 5\n  term                              estimate std.error statistic       p.value\n  &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 \"(Intercept)\"                      486.        2.33    208.    0            \n2 \"small_classyes\"                     6.39      1.69      3.79  0.000156     \n3 \"sbirthq2nd qtr - april,may,june\"   -5.64      2.33     -2.42  0.0155       \n4 \"sbirthq3rd qtr - july,aug,sept\"   -13.9       2.27     -6.11  0.00000000110\n5 \"sbirthq4th qtr - oct,nov,dec\"      10.2       2.49      4.08  0.0000452    \n6 \"hdegkmaster + \"                    -2.71      5.44     -0.497 0.619        \n7 \"hdegkmasters\"                      -5.25      1.82     -2.89  0.00393      \n8 \"hdegkspecialist\"                   17.0       7.62      2.23  0.0259       \n9 \"totexpk\"                            0.579     0.149     3.88  0.000108     \n\n\nWe then nest our data so each row is an entire cluster:\n\ndat_nst &lt;- dat %&gt;%\n    group_by( clid ) %&gt;%\n    nest() %&gt;%\n    ungroup()\ndat_nst\n\n# A tibble: 196 × 2\n    clid data              \n   &lt;int&gt; &lt;list&gt;            \n 1     1 &lt;tibble [20 × 11]&gt;\n 2     2 &lt;tibble [22 × 11]&gt;\n 3     3 &lt;tibble [21 × 11]&gt;\n 4     5 &lt;tibble [23 × 11]&gt;\n 5     6 &lt;tibble [21 × 11]&gt;\n 6     7 &lt;tibble [18 × 11]&gt;\n 7     9 &lt;tibble [23 × 11]&gt;\n 8    11 &lt;tibble [19 × 11]&gt;\n 9    13 &lt;tibble [17 × 11]&gt;\n10    14 &lt;tibble [17 × 11]&gt;\n# ℹ 186 more rows\n\n\nWe can then boostrap our data as so:\n\none_cluster_boot &lt;- function( ) {\n    \n    dat_nst_star = slice_sample( dat_nst, n = nrow(dat_nst), replace=TRUE )\n    dat_nst_star$clid = 1:nrow(dat_nst_star)\n    \n    dat_star &lt;- unnest( dat_nst_star, cols=\"data\" )\n    \n    my_analysis( dat_star )\n}\n\nNote we are regenerating the cluster ID so if we have the same cluster multiple times, each time gets a new ID.\nWe bootstrap and analyze a bunch of times and get standard errors:\n\nboots = map_df( 1:1000, ~ one_cluster_boot(), .id = \"boot\" )\n\nres_boot &lt;- boots %&gt;% group_by( term ) %&gt;%\n    summarise( SE = sd( estimate ) )\nres_boot\n\n# A tibble: 9 × 2\n  term                                  SE\n  &lt;chr&gt;                              &lt;dbl&gt;\n1 \"(Intercept)\"                      4.24 \n2 \"hdegkmaster + \"                  12.2  \n3 \"hdegkmasters\"                     4.12 \n4 \"hdegkspecialist\"                 14.4  \n5 \"sbirthq2nd qtr - april,may,june\"  2.26 \n6 \"sbirthq3rd qtr - july,aug,sept\"   2.31 \n7 \"sbirthq4th qtr - oct,nov,dec\"     2.43 \n8 \"small_classyes\"                   3.89 \n9 \"totexpk\"                          0.347\n\n\nWe can compare to the original (WRONG) OLS estimates, the CRVE standard errors, and the bootstrap standard errors:\n\nCRVE_sub &lt;- res_CRVE %&gt;%\n  dplyr::select( -estimate, -p.value ) %&gt;%\n  rename( SE_CRVE = std.error )\nOLS_sub &lt;- res_OLS %&gt;%\n  dplyr::select( -estimate, -p.value ) %&gt;%\n  rename( SE_OLS = std.error )\n\n# Make the table\ntbl &lt;- left_join( res_boot, OLS_sub, \n                  by = \"term\" ) %&gt;%\n  left_join( CRVE_sub, by = \"term\" ) %&gt;%\n  relocate( term, statistic ) %&gt;% \n  mutate( boot_v_OLS = SE / SE_OLS,\n          boot_v_CRVE = SE / SE_CRVE )\n\nknitr::kable( tbl, digits=2 )\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nstatistic\nSE\nSE_OLS\nSE_CRVE\nboot_v_OLS\nboot_v_CRVE\n\n\n\n\n(Intercept)\n208.38\n4.24\n2.33\n4.26\n1.82\n1.00\n\n\nhdegkmaster +\n-0.50\n12.22\n5.44\n11.50\n2.25\n1.06\n\n\nhdegkmasters\n-2.89\n4.12\n1.82\n4.15\n2.26\n0.99\n\n\nhdegkspecialist\n2.23\n14.43\n7.62\n15.10\n1.90\n0.96\n\n\nsbirthq2nd qtr - april,may,june\n-2.42\n2.26\n2.33\n2.27\n0.97\n0.99\n\n\nsbirthq3rd qtr - july,aug,sept\n-6.11\n2.31\n2.27\n2.35\n1.02\n0.98\n\n\nsbirthq4th qtr - oct,nov,dec\n4.08\n2.43\n2.49\n2.48\n0.97\n0.98\n\n\nsmall_classyes\n3.79\n3.89\n1.69\n3.86\n2.31\n1.01\n\n\ntotexpk\n3.88\n0.35\n0.15\n0.35\n2.32\n0.98\n\n\n\n\n\nA ratio of 1 means the estimated SEs are about the same. More than 1 means the bootstrap is returning larger SEs.\nGenerally, we see that the bootstrap is increasing the SEs for the level-2 coefficients (those that are talking about how clusters are different). This is good: the OLS SEs are way too small since they are not taking clustering into account.\nThe CRVE is basically the same as the bootstrap here, with some mild differences. All these methods are for estimating standard errors; they do not change the estimated coefficents themselves.\nBootstrapping is a simple way of getting uncertainty when you don’t know how to do that with math or a package. When we can do a mathematical approximation, the bootstrapping might not be worth it, due to the extra computation. But bootstrapping, by direct simulation, can also account for things like heteroskedasticity, outliers, or other weirdness that the mathmatical approximations cannot. It is worth using in many cases due to this general applicability, versitility, and robustness.",
    "crumbs": [
      "On Coding",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Demonstration of the Cluster Bootstrap</span>"
    ]
  },
  {
    "objectID": "Predicting_birthweight.html",
    "href": "Predicting_birthweight.html",
    "title": "15  A demonstration of different machine learning methods all fit to the same data",
    "section": "",
    "text": "16 Overview\nThis document showcases a bunch of different machine learning tools, all used on the same data set. At the end we compare the different rmse on the validation set.\nFor context, we will use a classic data set (e.g., Almond et al., 2005) on child birthweight. This data set was originally constructed to estimate the causal effect of maternal smoking on child birthweight; that is not what we are up to now. Our goal is to instead predict child birthweight directly based on observable characteristics prior to birth.\nThe target of interest is the child’s birthweight, stored as child_birthwt. This is a continuous outcome. All other variables are fair game as predictors.\nWhile this is obviously a simplified data set, the prediction problem is very real – many medical insurers and providers target services based on algorithms similar to what you will put together.\nNote: For the purposes of illustration we are going to use a proportionally small training set of 10,000 observations (so we need to regularize and use fancy stuff) and a very large validation set (so we see the real comparison of our different choices). In practice we would try to use as much of our data as possible for training.",
    "crumbs": [
      "On Machine Learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>A demonstration of different machine learning methods all fit to the same data</span>"
    ]
  },
  {
    "objectID": "Predicting_birthweight.html#tuning-the-random-forest",
    "href": "Predicting_birthweight.html#tuning-the-random-forest",
    "title": "15  A demonstration of different machine learning methods all fit to the same data",
    "section": "23.1 Tuning the random forest",
    "text": "23.1 Tuning the random forest\nWe should tune our random forest, to figure out which specification is best. We use caret’s train() to do this (borrowing code from the case-study walk-through from the Data Science in Education textbook):\n\n# setting a seed for reproducibility\nset.seed(2020)\n\n# Create a grid of different values of mtry, splitrule, and min.node.size,\n# the three tuning parameters for our random forest.\ntune_grid &lt;-\n    expand.grid(\n        mtry = c(2, 3, 7, 10),\n        splitrule = c(\"variance\"),        \n        min.node.size = c(10, 20, 50)\n    )\n\n# Fit a new model, using the tuning grid we created above.  This will take\n# awhile to run.\nrf_tuned &lt;-\n    train(child_birthwt ~ ., data = ca_training,\n          method = \"ranger\",\n          tuneGrid = tune_grid)\n\nrf_tuned\n\nRandom Forest \n\n10000 samples\n   25 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 10000, 10000, 10000, 10000, 10000, 10000, ... \nResampling results across tuning parameters:\n\n  mtry  min.node.size  RMSE      Rsquared    MAE     \n   2    10             560.5576  0.08226263  421.6399\n   2    20             560.7701  0.08165209  421.7262\n   2    50             561.0561  0.08155258  421.8141\n   3    10             557.0580  0.08455732  420.1078\n   3    20             557.0095  0.08532631  419.9823\n   3    50             557.4392  0.08517044  419.9752\n   7    10             557.9592  0.08025062  423.9294\n   7    20             556.6819  0.08320896  422.4458\n   7    50             555.3160  0.08691731  420.4274\n  10    10             559.3643  0.07946736  426.1047\n  10    20             557.4722  0.08297020  424.0431\n  10    50             555.1987  0.08782704  421.2257\n\nTuning parameter 'splitrule' was held constant at a value of variance\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 10, splitrule = variance\n and min.node.size = 50.\n\n\nWe then take our final random forest, fit to all of our training data and using the winning tuning parameters, by simply using predict from the result of our train() call:\n\nrmse( rf_tuned, ca_holdout )\n\n[1] 547.0557",
    "crumbs": [
      "On Machine Learning",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>A demonstration of different machine learning methods all fit to the same data</span>"
    ]
  },
  {
    "objectID": "test_train_and_CV.html",
    "href": "test_train_and_CV.html",
    "title": "16  Reflections on using machine learning (for final projects)",
    "section": "",
    "text": "17 Test/train splits, test/train/validate, and cross validation\nIt is important to track what your machine learning methods are doing in terms of spliting your data inside the call. For example, the caret package’s train() method will often be doing cross validation (or something similar) inside of it. You thus do not need to always do a test/train split outside of it!\nAlso, don’t do a test/train split if you don’t have a lot of data. It is too expensive! This is where cross validation is particularly useful, since it lets all of your data be used for testing, and it uses most of your data for fitting your models. It is kind of like a “have your cake and eat it too” situation.\nFinally, the full test/train/validate trifecta is only if you are really, really concerned about predicting your future performance of a model. It turns out that if you use cross-validation to select the best set of tuning parameters, the final estimated accuracy from the cross-validation will generally be pretty close to what a final validation set would tell you.\nLet’s see how the case study in Data Science in Ed, Chapter 14 went about things. They first split the data into a training set and a testing set. They then made a grid of tuning parameters related to tree size and so forth, and then used train() to identify what set of tuning parameters was best for the data. In particular, train() used this grid and a variant of cross validation (bootstrap resampling) to repeatedly divide the training data into a part used to fit one random forest for each tuning parameter combo and a part used to evaluate all of those random forests on out of sample data. The final table produced gives the estimated error rates, and finally train() selects the tuning parameter combo with the best rates.\nThe book then took the final model (via rf_fit2$finalModel), which is a last model fit by train() to all the training data using the best found parameter combo, as their answer. To evaluate the quality of their answer, they used finalModel to predict outcomes for the test data they set aside at the beginning of their case study.\nIn our language, their “test” data was used for the final validation step, and the train() method was doing little test/train splits inside of it to get the estimates of out of sample error as part of the tuning process.\nNote that they also found the predicted error of the finally selected final model was about the same as reported by the out of sample error by train(): this is not unusual. If we are not fitting too many different combos of tuning parameter and so forth, then the estimated error from this process will often be close.",
    "crumbs": [
      "On Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reflections on using machine learning (for final projects)</span>"
    ]
  },
  {
    "objectID": "test_train_and_CV.html#cross-validation-options-for-caret",
    "href": "test_train_and_CV.html#cross-validation-options-for-caret",
    "title": "16  Reflections on using machine learning (for final projects)",
    "section": "17.1 Cross validation options for caret",
    "text": "17.1 Cross validation options for caret\nThe caret package does cross validation internally as part of train(), if you tell it. If you don’t, it does a different kind of internal repeated test/train splitting that it calls bootstrap. This is not bootstrap for inference! What it is doing is resampling the data with replacement to get a training dataset of the same size as it was passed. It will then have about a third of the data not in the training set, and it will use this for out-of-sample testing to estimate the performance of all the models fit. Finally, it repeats this a bunch of times and averages: this means each observation will be in most training sets, but will be used for testing some of the time. This is just like cross validation (except more random)!\nAlso, caret’s cross-validation is, by default, a random cross-validation: repeatedly take 10% of the data as testing, and use the rest for training. Repeat 10 times. This means each observation will be used about once for testing and about nine times for training, but not exactly. Functionally, this will generally be nearly the same as the classic CV where we divide all the data into 10 parts systematically.\nIt basically does not matter which form of cross-validation or splitting you use. The number of iterations will impact running time: 25 iterations will be 2.5 times longer to run than 10!",
    "crumbs": [
      "On Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reflections on using machine learning (for final projects)</span>"
    ]
  },
  {
    "objectID": "test_train_and_CV.html#bootstrap-vs.-cross-validation",
    "href": "test_train_and_CV.html#bootstrap-vs.-cross-validation",
    "title": "16  Reflections on using machine learning (for final projects)",
    "section": "17.2 Bootstrap vs. Cross validation",
    "text": "17.2 Bootstrap vs. Cross validation\nDon’t confuse bootstrapping with cross-validiation. Bootstrapping is a way of doing statistical inference: you use it to ask how much an estimate would change if you happened to get a different data set from the same source. This allows you to decide if, for example, a coefficient is “really” positive–if your bootstrapping doesn’t really give you any negative estimates, then you can be sure that your estimate is probably not positive due to random chance.\nCross validation, by contrast, is a way of doing a lot of test/train splits because you want to know how a fit model would work on new data. It is focusing on estimating future predictive accuracy, not statistical inference.",
    "crumbs": [
      "On Machine Learning",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reflections on using machine learning (for final projects)</span>"
    ]
  },
  {
    "objectID": "interpreting_lasso_models.html",
    "href": "interpreting_lasso_models.html",
    "title": "17  Interpreting LASSO models, and some other stuff",
    "section": "",
    "text": "18 The Story\nYou are newly hired analyst working in the State Ministry of Education. This morning, you open your email to find a request from your supervisor, your first project in the new job. How exciting!\nYour boss writes that the Ministry has received funding to provide a one-on-one tutoring intervention for a modest number of struggling 8th grade math students. It’s important that we select the right students for the intervention because we can’t afford to give it to everyone. Unfortunately, we don’t have previous math scores for students in the state. The first standardized math test they take is at the end of 8th grade.\nYour boss proposes that you build a predictive model, using the other data the ministry has about students, to estimate who will perform poorly in math at the end of the 8th grade.\nThe only other requirement is that the model can be explained to policymakers and other higherups in the ministry. No black-box models.\nHer email includes an attachment with the data on last year’s 8th graders.\nOur goal is to understand what is predictive of math scores. More specifically, we want to predict who the struggling students will be. There are a lot of things we could do. Remember whatever we do must be interpretable/explainable. Trees might work. Of course we’ll settle on LASSO here. Generally speaking, LASSO is more interpretable than Ridge? (LASSO shrinks many predictors to zero, so we only have to explain the non-zero coefficients, which hopefully will be a small number.)",
    "crumbs": [
      "On Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpreting LASSO models, and some other stuff</span>"
    ]
  },
  {
    "objectID": "interpreting_lasso_models.html#what-variables-to-include",
    "href": "interpreting_lasso_models.html#what-variables-to-include",
    "title": "17  Interpreting LASSO models, and some other stuff",
    "section": "19.1 What variables to include?",
    "text": "19.1 What variables to include?\nWe might not want to put all of our available variables into our model. For example, if we are trying to predict math score, we might not want to include other test scores from that same testing occasion, because were looking for contextual factors that lead to a particular outcome. Thus, we might rerun everything we’ve done without including reading and science, which would force the model to use the contextual variables about the students rather than other measures of their academic ability.",
    "crumbs": [
      "On Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpreting LASSO models, and some other stuff</span>"
    ]
  },
  {
    "objectID": "interpreting_lasso_models.html#coefficient-size",
    "href": "interpreting_lasso_models.html#coefficient-size",
    "title": "17  Interpreting LASSO models, and some other stuff",
    "section": "19.2 Coefficient size",
    "text": "19.2 Coefficient size\nThe SIZE of the coefficients is going to be too small, in general, because LASSO shrinks coefficients towards zero to stabilize. We should not, for example, expect that the increase in math for each unit increase in reading is 0.3432561. One trick some folks do is refit the selected coefficients using OLS to get a new, final model:\n\n# Identify nonzero coefficients\ncoef_indices &lt;- which( colnames(x) %in% names(cc) )\ncoef_indices\n\n[1]  9 10 12 13 31\n\n# Subset data to nonzero covariates\nx_sub &lt;- x[, coef_indices]\nhead( x_sub )\n\n  gendermale computeryes    read science    escs\n1          1           1 347.710 273.827  0.7018\n2          0           1 427.958 460.357  0.2091\n3          1           1 271.743 337.150 -1.1074\n4          1           1 336.271 368.131 -0.4944\n5          1           1 402.166 322.163 -0.1840\n6          0           1 350.848 342.939 -0.6022\n\n# Fit OLS model to nonzero covariates\nfit_sub &lt;- lm(y ~ ., data = as.data.frame(x_sub))\n\n# Print summary of refit model\nsummary(fit_sub)\n\n\nCall:\nlm(formula = y ~ ., data = as.data.frame(x_sub))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-189.522  -32.514   -0.457   32.687  239.605 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 51.00114    5.30280   9.618  &lt; 2e-16 ***\ngendermale  19.31204    1.86074  10.379  &lt; 2e-16 ***\ncomputeryes  8.01858    2.71953   2.949  0.00322 ** \nread         0.38790    0.01883  20.602  &lt; 2e-16 ***\nscience      0.47816    0.01944  24.596  &lt; 2e-16 ***\nescs         4.52125    0.97928   4.617 4.04e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.17 on 3309 degrees of freedom\nMultiple R-squared:  0.7529,    Adjusted R-squared:  0.7525 \nF-statistic:  2016 on 5 and 3309 DF,  p-value: &lt; 2.2e-16\n\n\nWe can compare how much shrinkage there was:\n\nols_coef = coef( fit_sub )\nctbl = tibble( coef = names(ols_coef),\n               OLS = ols_coef,\n               Lasso = cc )\nctbl\n\n# A tibble: 6 × 3\n  coef           OLS  Lasso\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 51.0   78.3  \n2 gendermale  19.3    9.08 \n3 computeryes  8.02   0.505\n4 read         0.388  0.343\n5 science      0.478  0.488\n6 escs         4.52   3.01",
    "crumbs": [
      "On Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpreting LASSO models, and some other stuff</span>"
    ]
  },
  {
    "objectID": "interpreting_lasso_models.html#assessing-stability-a-kind-of-inference",
    "href": "interpreting_lasso_models.html#assessing-stability-a-kind-of-inference",
    "title": "17  Interpreting LASSO models, and some other stuff",
    "section": "19.3 Assessing stability (a kind of inference)",
    "text": "19.3 Assessing stability (a kind of inference)\nWe might worry that the Lasso model is picking these variables only due to random chance. We could bootstrap this process to see if the same variables tend to get picked each time. Here is a single bootstrap iteration to illustrate:\n\ndatstar = slice_sample( stu_data, n = nrow(stu_data), replace=TRUE )\nxstar &lt;- model.matrix(math ~ ., datstar)[,-1]\nystar &lt;- datstar$math\n\nmodstar &lt;- glmnet(x = xstar,\n                  y = ystar, \n                  lambda = best_lambda_lasso,\n                  alpha = 1 )\n\ncc &lt;- coef(modstar)\ncc[ cc[,1] != 0, ]\n\n                (Intercept) father_educless than ISCED1 \n                 83.6514190                  -2.6175776 \n                 gendermale                        read \n                  6.5947801                   0.3550822 \n                    science                        escs \n                  0.4687787                   3.3871490 \n\n\nWe generally got the same answers, suggesting stability in what variables are selected. But we should do this a lot, and see how often each variable shows up. To do this, we need a function! And, for our function, life is always easier if we get a data frame back:\n\none_boot &lt;- function( ) {\n  datstar = slice_sample( stu_data, n = nrow(stu_data), replace=TRUE )\n  xstar &lt;- model.matrix(math ~ ., datstar)[,-1]\n  ystar &lt;- datstar$math\n  \n  modstar &lt;- glmnet(x = xstar,\n                    y = ystar, \n                    lambda = best_lambda_lasso,\n                    alpha = 1 )\n  \n  cc &lt;- coef(modstar)\n  cc &lt;- cc[ cc[,1] != 0, ]\n  cc = tibble( coef = names(cc), est = cc)\n}\n\nThen replicate:\n\nrps = map_df( 1:100, ~ one_boot() )\nrps %&gt;% group_by( coef ) %&gt;%\n  summarise( n = n() ) %&gt;%\n  arrange( -n ) %&gt;%\n  knitr::kable()\n\n\n\n\ncoef\nn\n\n\n\n\n(Intercept)\n100\n\n\ngendermale\n100\n\n\nread\n100\n\n\nscience\n100\n\n\nescs\n99\n\n\ncomputeryes\n58\n\n\nwealth\n44\n\n\ncar2\n26\n\n\ninternetyes\n14\n\n\nfather_educless than ISCED1\n13\n\n\ndeskyes\n6\n\n\nmother_educless than ISCED1\n5\n\n\nbook201-500\n2\n\n\ncomputer_n3+\n1\n\n\nmother_educISCED 3A\n1\n\n\n\n\n\nThe things at the top were always selected–they are clearly important. The things near the bottom are less so. Our initial “computer” variable seems somewhat important, but not as reliably selected as things such as reading and science.",
    "crumbs": [
      "On Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpreting LASSO models, and some other stuff</span>"
    ]
  },
  {
    "objectID": "interpreting_lasso_models.html#last-words-on-writing-up-results",
    "href": "interpreting_lasso_models.html#last-words-on-writing-up-results",
    "title": "17  Interpreting LASSO models, and some other stuff",
    "section": "19.4 Last words on writing up results",
    "text": "19.4 Last words on writing up results\nIn writing up your results, consider the following elements:\n\nIntroduction: Begin by introducing the purpose of your analysis and the dataset you used. For example:\n\n“We fit a Lasso regression model to predict the response variable [name of response variable] based on [list of predictor variables]. The dataset used was [name of dataset] and consisted of [number of observations] observations.”\n\nModel Selection: Explain the process you used to select the best model, including the choice of the regularization parameter. For example:\n\n“We used cross-validation to select the value of lambda that minimized the mean squared error. The selected lambda was [value].”\n\nModel Performance: Report the performance of the Lasso model, including the model’s R-squared value, the root mean squared error (RMSE), and any other relevant metrics. For example:\n\n“The Lasso model achieved an R-squared value of [R-squared value], indicating that [percentage of variation in response variable] of the variation in the response variable can be explained by the predictor variables. The RMSE of the model was [RMSE value], indicating an average error of [RMSE value] units in predicting the response variable.”\nYou can calculate R2 by comparing the variance of the predictions to the variance of the original outcome: \\[ R^2 = 1 - \\frac{ Var( predictions ) }{ Var( Y ) } \\]\n\nInterpretation of Coefficients: Report the estimated coefficients. For example:\n\n“Table XX shows the estimated coefficients for the predictor variables.”\nYou might include OLS values as well, on this table.\n\nConclusion: Summarize the key findings of your analysis and any insights gained from the Lasso model. For example:\n\n“In conclusion, we found that [some variables] predicted our outcome, suggesting [something]. On the other hand, we expected [something else] to be selected, but it never was across the bootstrap samples, indicating that, given the other variables, there is no strong connection between [this thing] and the outcome.”\nRemember to provide enough detail for the reader to understand your analysis, but keep your report concise and focused on the key points.",
    "crumbs": [
      "On Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Interpreting LASSO models, and some other stuff</span>"
    ]
  },
  {
    "objectID": "interactions_discussion.html",
    "href": "interactions_discussion.html",
    "title": "18  Main effects, interactions in linear models, and prediction",
    "section": "",
    "text": "18.1 Getting the data ready\nWe use a data set of a bunch of behavioral outcomes measured for how a mother interacts with a child. One question of interest is whether mothers systematically interact with girls differently than they do with boys.\nWe first load tidyverse and load the data.\nqm = read.table( \"data/KONG.txt\" )\nnames( qm ) = c( \"wealth\", \"age\", \"fed\", \"med\", \"bookread\", \"hwhelp\", \"talkchld\", \"female\", \"sibling\" )\nhead( qm )\n\n  wealth       age fed med bookread hwhelp talkchld female sibling\n1  11830 11.333330   9   9        2      2        2      0       1\n2   8500 12.333330   2   5        2      2        2      1       1\n3  20047 13.416670   9   8        2      2        2      1       1\n4  18650 11.166670   0  12        2      2        2      1       1\n5   6760 11.666670   5   0        1      2        1      0       1\n6   4580  9.416667   8   8        2      2        2      1       1\n\nnrow( qm )\n\n[1] 1976\nWe make an aggregate measure of interaction by adding our three interview questions together. There are better ways of doing this, but this will work for now.\nqm = mutate( qm, lwealth = log(wealth),\n             interact = bookread + hwhelp + talkchld,\n             sex = factor( female, levels=c(0,1), labels=c(\"boy\",\"girl\" ) ) )\nTo simplify things, we are going to take the middle 80% of the data based on wealth. (There is odd tail behavior that clouds the trends that I want to avoid for pedagogical purposes.) qm = filter( qm, wealth &gt;= quantile( wealth, 0.1 ), wealth &lt;= quantile( wealth, 0.9 ) ) nrow( qm )\nLets see what we have:\nggplot( qm, aes( x=lwealth, y=interact, col=sex ) ) +\n    geom_point( alpha=0.5 ) +\n    stat_smooth( aes( group=sex ), method=\"loess\", se=FALSE )\nLets look at a simple main effect of sex\nM0 = lm( interact ~ sex + lwealth + sibling, data=qm )\nsummary( M0 )\n\n\nCall:\nlm(formula = interact ~ sex + lwealth + sibling, data = qm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4280 -0.9432  0.0462  0.9583  3.7106 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.365853   0.311989  13.994  &lt; 2e-16 ***\nsexgirl      0.005984   0.062663   0.095    0.924    \nlwealth      0.197600   0.032674   6.048 1.75e-09 ***\nsibling     -0.219954   0.043481  -5.059 4.62e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.367 on 1972 degrees of freedom\nMultiple R-squared:  0.03351,   Adjusted R-squared:  0.03203 \nF-statistic: 22.79 on 3 and 1972 DF,  p-value: 1.679e-14",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Main effects, interactions in linear models, and prediction</span>"
    ]
  },
  {
    "objectID": "interactions_discussion.html#fitting-an-interacted-model",
    "href": "interactions_discussion.html#fitting-an-interacted-model",
    "title": "18  Main effects, interactions in linear models, and prediction",
    "section": "18.2 Fitting an interacted model",
    "text": "18.2 Fitting an interacted model\nNow let’s fit a model where we interact sex and wealth. The plot does not look particularly linear, but we will proceed for illustrative purposes.\n\nM1 = lm( interact ~ sex * lwealth + sibling, data=qm )\nsummary( M1 )\n\n\nCall:\nlm(formula = interact ~ sex * lwealth + sibling, data = qm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3704 -0.9534  0.0387  0.9547  3.7785 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      4.80629    0.41588  11.557  &lt; 2e-16 ***\nsexgirl         -0.95389    0.60286  -1.582 0.113750    \nlwealth          0.14987    0.04422   3.389 0.000715 ***\nsibling         -0.22096    0.04347  -5.083 4.06e-07 ***\nsexgirl:lwealth  0.10468    0.06539   1.601 0.109569    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.366 on 1971 degrees of freedom\nMultiple R-squared:  0.03476,   Adjusted R-squared:  0.0328 \nF-statistic: 17.74 on 4 and 1971 DF,  p-value: 2.543e-14\n\n\nNotice the coefficient for sex changes a lot. This is because this is the estimated difference of girls and boys for those with a lwealth of 0.\nTo recover our “main effect” estimate we need to see what the average predicted difference would be for all the individuals in our dataset. We do this by doing the following steps:\n\nCopy our dataset to make two new ones\n\n\nqm.g = qm.b = qm\n\n\nNow make all the folks in the qm.g dataset girls, and all folks in the other boys.\n\n\nqm.g$sex = \"girl\"\nqm.b$sex = \"boy\"\n\n\nPredict interaction assuming everyone is a girl and everyone is a boy:\n\n\npred.g = predict( M1, qm.g )\npred.b = predict( M1, qm.b )\n\n\nThen the difference of the predictions is our predicted difference in interaction for two kids with the same covariates except sex:\n\n\ndeltas = pred.g - pred.b\nsex.diff = mean( deltas )\nsex.diff\n\n[1] 0.006737327\n\n\nSee? Now it matches more closely with the main effect estimate of M0. The core idea here is that the real interpretation of the main effect is a difference in average outcomes (when looking at a dummy varaible) between two groups, when holding all else equal. When you have interactions, the main effect is the predicted difference for those with zeros for the interaction terms, which might not be very sensible. We can get back to the difference between the two groups by predicting for everyone and comparing the averages.",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Main effects, interactions in linear models, and prediction</span>"
    ]
  },
  {
    "objectID": "interactions_discussion.html#plotting",
    "href": "interactions_discussion.html#plotting",
    "title": "18  Main effects, interactions in linear models, and prediction",
    "section": "18.3 Plotting",
    "text": "18.3 Plotting\nAs a reminder, we can plot with our predictions in a nice way. The cleanest would be to put all our predictions into a dataframe, convert to long form, and plot.\n(Note we use ‘gender’ as our key, in the following, to avoid colliding with the ‘sex’ variable name.)\n\nqm$girl = pred.g\nqm$boy = pred.b\nqml = qm %&gt;%\n  pivot_longer( cols = c(girl, boy),\n                names_to = \"gender\",\n                values_to = \"pred\" )\n\n\nggplot( qml, aes( x=lwealth, y= pred, col=gender ) ) +\n    geom_point()\n\n\n\n\n\n\n\n\nWe see different stripes for the different numbers of siblings.\nAs a better way, we can get fancier with the data grid stuff.\n\nlibrary( modelr )\ngrid = data_grid( qm, lwealth = seq_range( lwealth, 30 ), sex, .model = M1 )\nhead( grid )\n\n# A tibble: 6 × 3\n  lwealth sex   sibling\n    &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;\n1    4.74 boy         1\n2    4.74 girl        1\n3    5.00 boy         1\n4    5.00 girl        1\n5    5.26 boy         1\n6    5.26 girl        1\n\ngrid = add_predictions( grid, M1, \"pred\" )\n\n\nggplot( grid, aes( x=lwealth, y= pred, col=sex ) ) +\n    geom_point()\n\n\n\n\n\n\n\n\nI used geom_point() to show the individual predictions. Normally we would use geom_line()",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Main effects, interactions in linear models, and prediction</span>"
    ]
  },
  {
    "objectID": "interactions_discussion.html#centering-an-alternative-approach",
    "href": "interactions_discussion.html#centering-an-alternative-approach",
    "title": "18  Main effects, interactions in linear models, and prediction",
    "section": "18.4 Centering, an alternative approach",
    "text": "18.4 Centering, an alternative approach\nYou can also center your continuous variable, which means your coefficient for your main effect on your dummy variable will correspond to the average value of the continuous. Much more interpretable!\n\nqm = mutate( qm, lwealth.cent = lwealth - mean(lwealth) )\nM1b = lm( interact ~ sex * lwealth.cent + sibling, data=qm )\n\ncoef( M0 )\n\n (Intercept)      sexgirl      lwealth      sibling \n 4.365852906  0.005984015  0.197600022 -0.219953588 \n\ncoef( M1b )\n\n         (Intercept)              sexgirl         lwealth.cent \n         6.181712978          0.006737327          0.149873199 \n             sibling sexgirl:lwealth.cent \n        -0.220960207          0.104675040 \n\n\nSee? Much more interpretable!",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Main effects, interactions in linear models, and prediction</span>"
    ]
  },
  {
    "objectID": "interactions_discussion.html#confidence-intervals-for-the-gap",
    "href": "interactions_discussion.html#confidence-intervals-for-the-gap",
    "title": "18  Main effects, interactions in linear models, and prediction",
    "section": "18.5 Confidence intervals for the gap?",
    "text": "18.5 Confidence intervals for the gap?\nA bootstrap is the easiest here.\n\nreps = replicate( 1000, {\n    qm.star = mosaic::sample( qm, replace=TRUE )\n\n    # refit the original model with the bootstrap data\n    M1.star = update( M1, data=qm.star )\n\n    pred.g = predict( M1.star, qm.g )\n    pred.b = predict( M1.star, qm.b )\n\n    deltas = pred.g - pred.b\n    mean( deltas )\n} )\n\nOur bootstrap confidence interval: quantile( reps, c( 0.025, 0.975 ) )\n\nhist( reps, breaks=30, col=\"grey\" )\nabline( v=sex.diff, col=\"red\", lwd=3 )",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Main effects, interactions in linear models, and prediction</span>"
    ]
  },
  {
    "objectID": "interactions_discussion.html#a-final-investigation",
    "href": "interactions_discussion.html#a-final-investigation",
    "title": "18  Main effects, interactions in linear models, and prediction",
    "section": "18.6 A final investigation",
    "text": "18.6 A final investigation\nI don’t like the linear relationship. Better to have, perhaps a cubic for both girl and boy?\n\nM3 = lm( interact ~ ( lwealth + I((lwealth-mean(lwealth))^2) +\n                          I((lwealth-mean(lwealth))^3) ) * sex + sibling,\n         data=qm )\ncoef( M3 )\n\n                           (Intercept)                                lwealth \n                          3.9677413630                           0.2376064190 \n        I((lwealth - mean(lwealth))^2)         I((lwealth - mean(lwealth))^3) \n                          0.0226463368                          -0.0264912833 \n                               sexgirl                                sibling \n                         -1.0082023177                          -0.2159902502 \n                       lwealth:sexgirl I((lwealth - mean(lwealth))^2):sexgirl \n                          0.1117568699                          -0.0122453285 \nI((lwealth - mean(lwealth))^3):sexgirl \n                         -0.0007258967 \n\ngrid = add_predictions( grid, M3, \"pred\" )\nggplot( grid, aes( x=lwealth, y= pred, col=sex ) ) +\n    geom_line()\n\n\n\n\n\n\n\n\nIn truth, splines would be the best. A topic for another day (or prior handout).",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Main effects, interactions in linear models, and prediction</span>"
    ]
  },
  {
    "objectID": "interactions_discussion.html#disclaimer",
    "href": "interactions_discussion.html#disclaimer",
    "title": "18  Main effects, interactions in linear models, and prediction",
    "section": "18.7 Disclaimer",
    "text": "18.7 Disclaimer\nThe coefficients of the above are not significant, and we are not finding a real difference between girls and boys in this case. But the code is designed to illustrate the core concept of using predict to get access to a real estimate of a main effect when you are fitting a model with an interaction term.\nTo check if a complex model is an improvement, use anova()\n\nanova( M0, M1, M3 )\n\nAnalysis of Variance Table\n\nModel 1: interact ~ sex + lwealth + sibling\nModel 2: interact ~ sex * lwealth + sibling\nModel 3: interact ~ (lwealth + I((lwealth - mean(lwealth))^2) + I((lwealth - \n    mean(lwealth))^3)) * sex + sibling\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1   1972 3684.0                              \n2   1971 3679.2  1    4.7838 2.5691 0.10913  \n3   1967 3662.7  4   16.5032 2.2157 0.06503 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNope, the interactions (or the more flexible cubic) are not helping us (other than helping our understanding of how to model interactions).",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Main effects, interactions in linear models, and prediction</span>"
    ]
  },
  {
    "objectID": "ipeds_data.html",
    "href": "ipeds_data.html",
    "title": "19  Finding and Merging Data Online",
    "section": "",
    "text": "19.1 Datasets Posted on the Web\nThis handout provides a walk-through of downloading publicly available datasets from the web and merging them together. No guide can cover all cases, but this example should give you a sense of what to look for/think about when wrangling data posted on websites. Note that this is not about scraping. What we’re considering here are datasets posted for download, often - but not always - found on government sites.",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Finding and Merging Data Online</span>"
    ]
  },
  {
    "objectID": "ipeds_data.html#ipeds",
    "href": "ipeds_data.html#ipeds",
    "title": "19  Finding and Merging Data Online",
    "section": "19.2 IPEDS",
    "text": "19.2 IPEDS\nThis example uses data from IPEDS, the Integrated Postsecondary Education Data System. These data are made available by NCES, the National Center for Education Statistics, a great place to start looking for education-related data in the US. The data represent a set of surveys, conducted every year, of all colleges, universities, and other post-secondary institutions that take federal student aide money (which is like, many of them). Data are available going back to 1986 (some scattered earlier years also have data). The surveys cover enrollments, graduation rates, prices, and many other things.\nThere are 12 total survey components that cover 9 major topics:\n\nAcademic Libraries\nAdmissions\nCompletions\nEnrollment (Fall and 12-Month)\nFinance\nGraduation Rates and Outcome Measures\nHuman Resources\nInstitutional Characteristics\nStudent Financial Aid\n\nIt’s worth noting that some info is only collected every other year. In addition, the data are not consistent over time. New survey items get added. Old items get dropped. Definitions change. Identifying codes, like the Classification of Instructional Program (CIP) codes or the Standard Occupational Classification (SOC), are periodically reviewed and updated. Being aware of (and dealing with) these issues is part of our job as data scientists. When you’re working with new data, take some time to identify these kinds of challenges before you dig too deeply into analysis.",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Finding and Merging Data Online</span>"
    ]
  },
  {
    "objectID": "ipeds_data.html#finding-ipeds-data",
    "href": "ipeds_data.html#finding-ipeds-data",
    "title": "19  Finding and Merging Data Online",
    "section": "19.3 Finding IPEDS Data",
    "text": "19.3 Finding IPEDS Data\nThe landing page for IPEDS is here. When you open the page, you’ll see many links. NCES provides a lot of information about their data (and different ways to access it). Sorting through surfeits of info like this is often the biggest challenge of working with public data sources. My description of IPEDS (in the above section) came from digging around these links.1\nOn the IPEDS page, there’s a link to their data explorer. When you hit this page, you’ll see links to the most recently released surveys. You can filter the datasets by year or by which of the 12 survey components you’re interested in. These data are aggregated, but it’s a good way to get a sense of what information is available in the individual survey components.\nTo get the survey data, look for this section of the landing page:\n\nIn the dropdown menu, select “Complete Data Files”. This will take you to a page with download links for each survey component. The files we want are found in the “Data File” column. Let’s download this admissions and test scores file,\n\nWhen we download the file, we should put it in a folder on our computer that’s dedicated to this project. That’ll make things easier once we start writing code. On my laptop, I’ve made a folder called “ipeds_handout”. Inside that folder, I made another folder called “data”. I put the downloaded file into that “data” folder. When I start writing code, I’ll store it in the parent folder (“ipeds_handout”).\nThe downloaded file is a zipped csv. We know how to work with a csv in R, but before we can do that, we have to unzip the file. If you haven’t done this before, here’s a page that walks through how to unzip files on PC or Mac. Once you have the unzipped csv, you can delete the original zip file to tidy things up.\nWhile we’re at it, let’s grab one more file, data on instructional staff salaries,\n\nLike before, unzip the file and put the csv in a project-specific folder.2 You should also download and unzip the dictionary files for both datasets.\n\nThe dictionaries contain codebooks that explain what each variable and value in the dataset means. These usually aren’t big files, so we can open and browse them directly in Excel. I’m going to use them soon to figure out how to find the information I’m interested in.\nNow we’re ready to get started.",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Finding and Merging Data Online</span>"
    ]
  },
  {
    "objectID": "ipeds_data.html#our-example-case",
    "href": "ipeds_data.html#our-example-case",
    "title": "19  Finding and Merging Data Online",
    "section": "19.4 Our Example Case",
    "text": "19.4 Our Example Case\nLet’s start by loading our libraries and data.\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\nadmit &lt;- read_csv('data/adm2021.csv')\n\nRows: 1981 Columns: 68\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (29): XAPPLCN, XAPPLCNM, XAPPLCNW, XADMSSN, XADMSSNM, XADMSSNW, XENRLT, ...\ndbl (39): UNITID, ADMCON1, ADMCON2, ADMCON3, ADMCON4, ADMCON5, ADMCON6, ADMC...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsal &lt;- read_csv('data/sal2021_is.csv')\n\nRows: 15382 Columns: 110\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (54): XSAINSTT, XSAINSTM, XSAINSTW, XSA_9MCT, XSA_9MCM, XSA_9MCW, XSATOT...\ndbl (56): UNITID, ARANK, SAINSTT, SAINSTM, SAINSTW, SA_9MCT, SA_9MCM, SA_9MC...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWith the datasets we’ve downloaded, we can explore the relationship between assistant professor salaries and the math ability of incoming Freshmen. To begin, we narrow down each dataset to the variables of interest. To identify these, I looked at the dictionaries we downloaded from IPEDS.\n\n# Keep only school IDs and average salary for 9-month\n# Contract instructors\nsal &lt;- sal %&gt;% filter(ARANK==3) %&gt;% \n  # The dictionary tells us that these are assistant profs\n  select(UNITID, SA09MAT)\n\n\n# Keep school Ids, # of applicants, % that submit SATs,\n# and 75th pctile of SAT scores\nadmit &lt;- admit %&gt;% select(UNITID, APPLCN, SATPCT, SATMT75)\n\nIn other cases, you might have to do much more cleaning before you merge these datasets together. In general, the process will be to clean each dataset so they can be easily merged. (Of course, more data cleaning may be necessary, post-merge.)\n\n19.4.1 Merging the Admissions and Salary Data\nIt’s a good idea to do a full join. This will keep all rows from both datasets and allow us to explore which rows did and did not match, post-merge. That said, I often start with a left or right join just as a quick check of how many records match.\n\ndf &lt;- left_join(sal, admit, by='UNITID')\nsum(is.na(df$SATPCT)) \n\n[1] 1298\n\n\nAbout half (around 1,300 institutions) don’t have a match. This should be explored. Maybe it’s from particular types of institutions (e.g. Vocational/Technical Schools). We might have to merge in additional data to explore the matching behavior. We might figure it out by reading carefully through the dictionaries or other documentation on IPEDS. You can take these steps by following the framework laid out above.\nFor now, we can run a preliminary regression and call it a day.\n\n# First let's mean-center both variables\ndf &lt;- df %&gt;% mutate(across(c(SA09MAT, SATMT75), ~.x-mean(.x, na.rm=TRUE)))\n\nsummary(lm(SA09MAT ~ SATMT75, df))\n\n\nCall:\nlm(formula = SA09MAT ~ SATMT75, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-49118  -8410   -889   8346  52056 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2829.703    459.431   6.159 1.18e-09 ***\nSATMT75      164.646      6.186  26.616  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12740 on 771 degrees of freedom\n  (1529 observations deleted due to missingness)\nMultiple R-squared:  0.4788,    Adjusted R-squared:  0.4782 \nF-statistic: 708.4 on 1 and 771 DF,  p-value: &lt; 2.2e-16\n\n\nIt looks like assistant professors make more at schools with higher SAT math scores (as measured by the 75th percentile of the distribution). Assistant profs at schools with 100 points higher scores earn about 10k more on average. There are lots of reasons we shouldn’t put much weight on this result. Lots of rows didn’t merge. We haven’t investigated missingness in the data. But the above steps walk through the process of a first, rough-cut analysis with IPEDS data.",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Finding and Merging Data Online</span>"
    ]
  },
  {
    "objectID": "ipeds_data.html#footnotes",
    "href": "ipeds_data.html#footnotes",
    "title": "19  Finding and Merging Data Online",
    "section": "",
    "text": "Along the way, I encountered some broken pages, like this link to the IPEDS data release calendar. Maybe they’ll fix it, but you can often find the information you need on some other page. When that doesn’t work, ask someone who’s familiar with the data. When that doesn’t work, you can email someone who works for the agency responsible for the data. In my experience, people get back to you reasonably quickly and are happy to be helpful, as long as you write politely to them.↩︎\nTo be clear, this should be the same folder as the first dataset. Also, if you’re working with many datasets for a project, it’s a good idea to rename them at this stage. Give the datasets clear, descriptive names so you know what they contain. This will make it easier to write and read code later.↩︎",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Finding and Merging Data Online</span>"
    ]
  },
  {
    "objectID": "predictive_papers.html",
    "href": "predictive_papers.html",
    "title": "20  Applied Prediction Papers",
    "section": "",
    "text": "Below is a list of papers related to predictive data science and machine learning in public policy (including education). These are organized into: (1) surveys and reviews and (2) original applications. You may find many of these papers helpful as you are working on your final project and interpreting results - please peruse them with this use case in mind as well as for your general understanding of how these methods are used in the real world.\n\n21 Surveys and reviews\nThe following list includes review articles, textbooks, and textbook chapters that provide an overview of the application of machine learning in various fields related to public policy. These fields include education, public health, urban planning, economics, and urban planning, among many others. Many of these works discuss or cite specific applications of these methods within these fields as well (including some of the references that follow after this section).\n\n21.0.0.1 Athey, Susan. “Beyond Prediction.” Science (American Association for the Advancement of Science), vol. 355, no. 6324, 2017, pp. 483–85, https://doi.org/10.1126/science.aal4321.\nDiscussion of the challenges of using machine learning predictions to improve public policies. Includes many references to real-world uses of predictive data science.\n\n\n21.0.0.2 Athey, Susan. The Impact of Machine Learning on Economics. In: The Economics of Artificial Intelligence: An Agenda [Internet]. University of Chicago Press; 2018 [cited 2023 Mar 31]. p. 507–47. Available from: https://www.nber.org/books-and-chapters/economics-artificial-intelligence-agenda/impact-machine-learning-economics\nThis chapter discusses use of machine learning in economics and includes a discussion of “prediction policy” applications for informing economic decision-making.\n\n\n21.0.0.3 Casali, Ylenia, et al. Machine learning for spatial analyses in urban areas: a scoping review. Sustainable Cities and Society. 2022 Oct 1;85:104050.\nAuthors review the use of machine learning in city/urban planning. Specific attention is given to machine learning applications involving spatial data.\n\n\n21.0.0.4 Hu, Xindi C., et al. “The Utility of Machine Learning Models for Predicting Chemical Contaminants in Drinking Water: Promise, Challenges, and Opportunities.” Current Environmental Health Reports. 2023 Mar;10(1):45–60.\nReview of the use and challenges of machine learning models for predicting chemical contaminants in drinking water. Also discusses their frequent use to guide sampling efforts by prioritizing at-risk areas.\n\n\n21.0.0.5 Payedimarri, Anil Babu, et al. Prediction Models for Public Health Containment Measures on COVID-19 Using Artificial Intelligence and Machine Learning: A Systematic Review. International Journal of Environmental Research and Public Health. 2021 Jan;18(9):4499.\nA brief review of the use of machine learning (and artificial intelligence) methods to evaluate public health interventions to contain the spread of SARS-CoV-2.\n\n\n21.0.0.6 Perry, Walt L. 2013. Predictive Policing: The Role of Crime Forecasting in Law Enforcement Operations. Rand Corporation.\nOpen access book gives an overview of common predictive policing practices.\n\n\n21.0.0.7 Williamson, Ben. 2016. “Digital education governance: data visualization, predictive analytics, and ‘real-time’ policy instruments.” Journal of Education Policy 31(2):123-141.\nBroad overview of uses of modern education data. Includes a section on predictive analytics. See first full paragraph on p. 136.\n\n\n21.0.0.8 Baker, R. S., Martin, T., & Rossi, L. M. (2016). Educational Data Mining and Learning Analytics. In The Wiley Handbook of Cognition and Assessment (pp. 379–396). John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118956588.ch16\nThis chapter written by experts in the field provides a nice overview of educational data mining and learning analytics. Data obtained through naturally occurring log data (e.g., from learning management systems) or specifically procured sources (e.g., eyetracking) can be used to both make inferences and predictions on learning behavior and outcomes.\n\n\n\n22 Original applications\nThe following list includes papers that apply machine learning methods to specific research questions. All are related to prediction and public policy, but span a similarly wide range of disciplines. While reading these, focus on the descriptions of the methods and results. This may be useful for your final project (and, of course, more generally!).\n\n22.0.0.1 Bansak, Kirk, et al. “Improving Refugee Integration through Data-Driven Algorithmic Assignment.” Science (American Association for the Advancement of Science), vol. 359, no. 6373, 2018, pp. 325–29.\nIn this paper, the authors propose a model that aims to predict where refugees will integrate best. They suggest governments take up their approach to make their refugee programs more efficient.\n\n\n22.0.0.2 Goel S, Rao JM, Shroff R. Precinct or prejudice? Understanding racial disparities in New York City’s stop-and-frisk policy. The Annals of Applied Statistics. 2016 Mar;10(1):365–94.\nThe authors of this paper used machine learning to estimate the probability that a detained individual truly has a weapon for stops related to suspicion of criminal weapon possession in New York City. Disproportionate stops by racial/ethnic groups, and the factors resulting in these disparities, are also discussed.\n\n\n22.0.0.3 Hino, M, et al. Machine learning for environmental monitoring. Nature Sustainability. 2018 Oct;1(10):583–8.\nDemonstration of how machine learning methods can help allocate resources to more efficiently conduct inspections for violations of the Clean Water Act.\n\n\n22.0.0.4 Kelly, Sean, et al. “Automatically Measuring Question Authenticity in Real-World Classrooms.” Educational Researcher, vol. 47, no. 7, 2018, pp. 451–64, Available from: https://doi.org/10.3102/0013189X18785613.\nThis team uses regression trees to predict which teacher questions are “authentic”.\n\n\n22.0.0.5 Lee Kwang-Sig, et al. Association of Preterm Birth with Depression and Particulate Matter: Machine Learning Analysis Using National Health Insurance Data. Diagnostics. 2021 Mar;11(3):555.\nDemonstrates a use of machine learning to predict and identify the majors determinants of preterm birth in South Korea. Authors discuss that strategies to reduce air pollution could be an effective intervention based on these findings.\n\n\n22.0.0.6 Yoo, Sanglim. Investigating important urban characteristics in the formation of urban heat islands: a machine learning approach. Journal of Big Data. 2018 Jan 24;5(1):2.\nThe author presents and discusses use of random forest to predict the formation of urban heat islands in Indianapolis, Indiana.\n\n\n22.0.0.7 https://chicago.github.io/food-inspections-evaluation/\nThe city of Chicago uses a predictive model to decide which food establishments are inspected first.\n\n\n22.0.0.8 Romero, C., López, M. I., Luna, J. M., & Ventura, S. (2013). Predicting students’ final performance from participation in on-line discussion forums. Computers & Education, 68, 458-472.\nThe researchers used classification and clustering to predict student performance based on a collection of features from an online discussion forum. This paper is a good example of the complicated data cleaning and feature pre-processing usually required for learning analytics (LA) work. It also demonstrates the various models that can be used in the process; plus, the paper actually explains what the models are and how they work before going into the results. Overall, it is a good entry paper into the field.\n\n\n22.0.0.9 Bozick, Robert and Dalton, Benjamin. Balancing Career and Technical Education With Academic Coursework: The Consequences for Mathematics Achievement in High School. Educational Evaluation and Policy Analysis. 2013 Jun 1;35(2):123-38.\nThese researchers assessed the ability of career and technical education courses to improve student learning. Their work provides an example of the use of bootstrapping methods to calculate standard errors of regression coefficients.",
    "crumbs": [
      "Extra Stuff",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Applied Prediction Papers</span>"
    ]
  }
]