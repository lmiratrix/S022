[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resources for S043/Stat151: Multilevel and Longitudinal Models",
    "section": "",
    "text": "Preface\nThis online book has a bunch of resources for S-043: Multilevel and Longitudinal Models. The book is written in Quarto, and is basically a bunch of handouts stapled together. It is very much a work in progress. If you notice errors, please notify luke_miratrix@gse.harvard.edu or joshua_gilbert@g.harvard.edu.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Resources for S043/Stat151: Multilevel and Longitudinal Models",
    "section": "Overview",
    "text": "Overview\nThere are several parts to the book, loosely arranged by type of handout. We give an overview of all the parts next:\n\nR & R Markdown\nThe book starts with material on just using R and making tables and whatnot.\n\n\nUsing ggPlot\nThe ggPlot section’s handouts are on using ggPlot, with an emphasis on using small multiples and other tricks to plot clustered or longitudinal data and results from multilevel data analysis. This section also includes how to use prediction to visualize a model’s fit, which is especially important for longitudinal data, and plotting growth curves.\n\n\nModel Fitting and Interpretation\nThis section has information on how to deal with the results from a lmer() call, and also has material connecting the code to the mathematical model. There are handouts on how to interpret parameters as well. This has help for much of the core content of the course.\n\n\nWorked Examples\nThe section has several case studies that illustrate things such as three-level models. One central chapter is the one with all the code to replicate the High School and Beyond example from Chapter 4 of Raudenbush and Bryk,\n\n\nVisualizations\nThe visualization section has some interactive visualiations made by Josh Gilbert that can bring some of the ideas of this course to life.\n\n\nMath Derivations\nThe math derivations at the end show how some of the variance decomposition stuff works, or error correlation matrices are built.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Resources for S043/Stat151: Multilevel and Longitudinal Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nSome of these handouts, or early drafts of these handouts, were written by the many prior TFs of this course. We have attributed authorship where we had it, but input from TFs has improved pretty much everything you see here. Thanks also to the many prior students who have asked for these handouts, given feedback, and overall have helped this course be what it is today (which, as you can tell from the number of handouts, is a lot).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "self_assess.html",
    "href": "self_assess.html",
    "title": "Do I take S-043? A self-assessment",
    "section": "",
    "text": "A self-assessment quiz\nTrying to figure out how S43 will be for you? ​Add up the points across the following categories:",
    "crumbs": [
      "Do I take S-043? A self-assessment"
    ]
  },
  {
    "objectID": "self_assess.html#a-self-assessment-quiz",
    "href": "self_assess.html#a-self-assessment-quiz",
    "title": "Do I take S-043? A self-assessment",
    "section": "",
    "text": "Work Experience\n+0        I have no work experience with quantitative data, or\n+1        I have some work experience with quantitative data, or\n+2        I have substantial work experience with quantitative data\n\n\nStat Courses\n-2        I have no prior stat experience under my belt, or\n+1        I have taken a very intro stat course (e.g., S12) or have at least a little stat knowledge, or\n+3        I have taken a linear regression course (e.g., S30, S40), or\n+5        I have taken an intermediate or advanced stats course (e.g., S52 or beyond)\n+1 bonus if concurrently enrolling in S052\n+1 bonus if classes were comfortable for you\n\n\nProgramming\n+0        I have basically no experience doing computer programming, or\n+1        I have some experience doing computer programming, or\n+2        I have a lot of experience doing computer programming\n\n\nR Skills\n+0        I have no real experience with R, or\n+1        I have a small bit of experience with R (e.g., ran scripts of it in S040), or\n+2        I have some experience with R (e.g., played around with scripts a bit in S040), or\n+3        I have substantial experience with R (e.g., write my own R code for my work)\n+1 bonus if learning R has been comfortable for you\n+1 bonus if you are comfortable with something like STATA\n\n\nMath\n+0        I don’t recall much math from my past education, or\n+1        I have taken (and somewhat remember) Calculus, or\n+2        I have taken classes beyond Calculus\n+1 bonus if classes were comfortable for you\n\n\nOther\n+2        I am content with getting a B or taking the class SAT/UNSAT",
    "crumbs": [
      "Do I take S-043? A self-assessment"
    ]
  },
  {
    "objectID": "self_assess.html#total-the-above",
    "href": "self_assess.html#total-the-above",
    "title": "Do I take S-043? A self-assessment",
    "section": "Total the above",
    "text": "Total the above\nA VERY ROUGH recommendation is:\n&lt; 4:      Danger! Please email the instructor to talk about how this might go for you.\n4-6:      It will be really hard! You could take this course, but will likely find it will take much more time than a typical course. We would support you through this, but be warned that this could feel like a lot to take on.\n7-9:      It will be hard! There are lots of folks like you who are taking this course. The course will likely be a fair bit of work and could feel confusing/overwhelming at times. We would support you through this. At the end you will have learned a lot if you stick with it.\n10+:     It will probably feel like a normal course. No reservations. You can either work a reasonable amount and learn a lot about data science, or dig deeper to really go far with the skills we cover.\n14+:     It will probably be a cake-walk. You will learn some concepts but not have to work particularly hard in this course. We would still love to have you!\nStudents have historically said this course teaches you a lot; the question is just whether you have the time to allocate for the course. This quiz helps assess the time.",
    "crumbs": [
      "Do I take S-043? A self-assessment"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting Started with R",
    "section": "",
    "text": "Some Resources and Directions\nThe following are ways of getting help with R.",
    "crumbs": [
      "Getting Started with R"
    ]
  },
  {
    "objectID": "getting_started.html#some-resources-and-directions",
    "href": "getting_started.html#some-resources-and-directions",
    "title": "Getting Started with R",
    "section": "",
    "text": "Handouts\nThis textbook has many handouts written by the teaching team that illustrates how to use R for a variety of tasks. Also see the Resources page on Canvas for further commentary and organization.\n\n\nClass Code\nEach class has a R script that shows how to do the stuff from that class. See the Packets on Canvas for this code.\n\n\nA Very Important Online Textbook\nSee R for Data Science.. This textbook provides important information on wrangling data, making plots, and doing statistical programming. It is full of examples and code snippets you can steal.\n\n\nClass Sections\nSections will typically have a hands-on component which will give you a chance to try things out yourself. These sections will also publish the final R code for future reference. See the section pages on Canvas to get this information.\n\n\nOffice Hours\nOffice hours are fine time to get help troubleshooting a specific or script you are working on.\n\n\nGSE Stat Help Desk\nEducation students can write to stathelp@gse.harvard.edu to get help getting started with R. If groups of 3 or more want some tutorials, they can ask for them as well.\nThe Help Desk sometimes has Intro to R workshops. For example, grab some old workshop materials here: http://its.gse.harvard.edu/gentle-introduction-r",
    "crumbs": [
      "Getting Started with R"
    ]
  },
  {
    "objectID": "assignment_guidelines.html",
    "href": "assignment_guidelines.html",
    "title": "Assignment Formatting Guidelines",
    "section": "",
    "text": "Formatting",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "assignment_guidelines.html#formatting",
    "href": "assignment_guidelines.html#formatting",
    "title": "Assignment Formatting Guidelines",
    "section": "",
    "text": "Start each question on a new page.\nProvide at least a brief title for the question or sub-question along with the question number. E.g. “(a). Association between graduation rates and school type.” You can copy the entire prompt for your future reference if you want, but abbreviated prompts are fine with us!\nUse headings or other means to highlight problem titles (e.g., bold, italic, etc.).\nMake sure you answer all parts of each question, and do so under the proper labeled subpart. For each question or sub-question, include any R code, R output and answer.\nUse different fonts/formatting for your R code, R output and answers and use font formatting consistently throughout each assignment.\nUse single line spacing and normal 1-inch margins.\nInclude page numbers.\nLet people say of your work: “There is no bombast, no similes, flowers, digressions, or unnecessary descriptions. Everything tends directly to the catastrophe.” – Horace Walpole, The Castle of Otranto",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "assignment_guidelines.html#numbers",
    "href": "assignment_guidelines.html#numbers",
    "title": "Assignment Formatting Guidelines",
    "section": "Numbers",
    "text": "Numbers\n\nRound your final answers, not intermediary steps.\nPut a zero in front of a decimal place (e.g. 0.2 instead of .2). This is optional for bounded numbers (e.g., p-values or proportions).\nRound to the nearest meaningful digit. What this means is a little hard to say, and different people can have different standards. However, if you have a statistic with a standard error of 1, it’s not meaningful to report decimals because the estimate simply isn’t precise enough to estimate numbers that small. Similarly, if you’re reporting average salaries, it’s usually not meaningful to report past the hundreds place regardless of your precision, because tens of dollars are too small to matter. Because this is so imprecise, we’ll give a lot of lee-way, but spurious precision will annoy. If you’re not sure what this means in practice, feel free to ask a TF.\nFormat your p-values! Round p-values to 3 places, and never report a p-value of 0. Instead say, for example, “p &lt; 0.001” or “p &lt; 10^-r” for some r.\n“Numbers have life; they’re not just symbols on paper.” – Shakuntala Devi",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "assignment_guidelines.html#plots",
    "href": "assignment_guidelines.html#plots",
    "title": "Assignment Formatting Guidelines",
    "section": "Plots",
    "text": "Plots\n\nMake sure your plots are well labelled, including title, axis, legends and any other elements you choose to include.\nYour plots should be self-explanatory.\nInclude notes and captions as necessary.\nTry to make plots easier to compare when you have multiple plots. For example, it is nice to have the same \\(X\\)-axis bounds if giving two histograms.\nDo not include best fit lines unless you have some reason, e.g. a significant \\(p\\)-value or a scientific basis for understanding an association.\n“Above all else show the data.” –Edward Tufte",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "assignment_guidelines.html#avoiding-bias",
    "href": "assignment_guidelines.html#avoiding-bias",
    "title": "Assignment Formatting Guidelines",
    "section": "Avoiding bias",
    "text": "Avoiding bias\n\nUse plural phrases, nouns or pronouns, e.g. “children and their toys” for “a child and his toy.” You may use the singular “they” pronoun (“a child and their toy”), but be warned that broader academic communities are still in flux regarding this usage.\nTry to avoid biased forms of language concerning race, gender, disability and sexuality. A reasonable list of case studies to consider on this topic is https://academicguides.waldenu.edu/writingcenter/scholarlyvoice/avoidingbias. The APA also has a guide for race at https://apastyle.apa.org/style-grammar-guidelines/bias-free-language/racial-ethnic-minorities.\n“I have yet to see a piece of writing, political or non-political, that does not have a slant. All writing slants the way a writer leans, and no man is born perpendicular.” ― E.B. White",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "assignment_guidelines.html#conciseness",
    "href": "assignment_guidelines.html#conciseness",
    "title": "Assignment Formatting Guidelines",
    "section": "Conciseness",
    "text": "Conciseness\n\n“Brevity is the soul of wit.” –Hamlet",
    "crumbs": [
      "Assignment Formatting Guidelines"
    ]
  },
  {
    "objectID": "self_grading.html",
    "href": "self_grading.html",
    "title": "Self Grading Instructions",
    "section": "",
    "text": "Acknowledgement:\nThis idea of self-grading was inspired by Cora Wigger. She used to have some stuff on twitter, but she appears to have removed her account.",
    "crumbs": [
      "Self Grading Instructions"
    ]
  },
  {
    "objectID": "style_guide.html",
    "href": "style_guide.html",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "",
    "text": "1.1 Why have coding style?\nMuch of this is from the Tidyverse style guide at http://style.tidyverse.org; we are primarily focusing on Chapters 2 and 4. Cartoon is xkcd; read those if you want to be an awesome nerd.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "style_guide.html#why-have-coding-style",
    "href": "style_guide.html#why-have-coding-style",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "",
    "text": "Many style decisions are arbitrary.\nWhy bother?\n\nit makes your code readable\nit means you can focus on writing good code\nyou will be looked down on if you use bad style",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "style_guide.html#names-style-guide-rule-2.1",
    "href": "style_guide.html#names-style-guide-rule-2.1",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "1.2 names (style guide rule 2.1)",
    "text": "1.2 names (style guide rule 2.1)\n\n1.2.1 Rule 2.1: Naming\n\n“There are only two hard things in Computer Science: cache invalidation and naming things.” —Phil Karlton\n\n\nVariable and function names should be lowercase.\nUse an underscore to separate words within a name.\nGenerally, variable names should be nouns and function names should be verbs.\n\n\n# Good\nday_one\nfirst_day\n\n# Bad\nfirst_day_of_the_month\nDayOne\ndayone\ndjm1\n\n\n\n1.2.2 Rule 2.2: Don’t use common names\n\n# Bad\nTRUE &lt;- FALSE\npi &lt;- 10\nmean &lt;- function(x) sum(x)\n\n\n\n1.2.3 Example: winsorization\n\n# Good\nwinsor_upper &lt;- 0.99\nwinsor_lower &lt;- 0.01\ndiamonds &lt;-\n  diamonds %&gt;%\n  mutate(y_winsor = winsorize(y, probs = c(winsor_lower, winsor_upper)))\n\n# Mediocre\ndiamonds_clean &lt;-\n  diamonds %&gt;%\n  mutate(y = winsorize(y, probs = c(0.01, 0.99)))\n\n\n\n1.2.4 Naming summary\n\nPrinciple: Ideally, your names should be self-explanatory and your code should be “self-documenting.”\nA few specific tips:\n\nNever use numbers to store versions of a data frame\nBy default, names for variables, functions, files, etc. should consist of complete words. (dest_short is an exception since it explicitly builds on source var dest) Source: Code and Data for the Social Sciences: A Practitioner’s Guide, Gentzkow and Shapiro\n\nThis is hard. more art than science.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "style_guide.html#syntax",
    "href": "style_guide.html#syntax",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "1.3 Syntax",
    "text": "1.3 Syntax\n\n1.3.1 syntax: roadmap\n\nnaming\nspaces\nargument names\nline length\nassignment\nquotes\ncomments\n\n\n\n1.3.2 Rule 2.2: Spaces (I)\n\nPut a space before and after = when naming arguments in function calls.\nAlways put a space after a comma, and never before (just like in regular English).\n\n\n# Good\naverage &lt;- mean(x, na.rm = TRUE)\n\n# Also good\naverage &lt;- mean( x, na.rm = TRUE )\n\n# Bad\naverage&lt;-mean(x, na.rm = TRUE)\naverage &lt;- mean(x ,na.rm = TRUE)\n\n\n\n1.3.3 Rule 2.2: Spaces (II)\n\nMost infix operators (==, +, -, &lt;-, etc.) should be surrounded by spaces.\nThe exception are those with relatively high precedence: ^, :, ::, and :::. (“High precedence” means that these operators are evaluated first, like multiplication goes before addition.)\n\n\n# Good\nheight &lt;- (feet * 12) + inches\nsqrt(x^2 + y^2)\nx &lt;- 1:10\nbase::get\n\n# Bad\nheight&lt;-feet*12 + inches\nsqrt(x ^ 2 + y ^ 2)\nx &lt;- 1 : 10\nbase :: get\n\n\n\n1.3.4 Rule 2.2: Spaces (III)\nExtra spacing (i.e., more than one space in a row) is ok if it improves alignment of equal signs or assignments (&lt;-).\n\n# Good\nlist(\n  total = a + b + c,\n  mean  = (a + b + c) / n\n)\n\n# Less good, but livable\nlist(\n  total = a + b + c,\n  mean = (a + b + c) / n\n)\n\n\n\n1.3.5 Rule 2.3: Argument names\nFunction arguments: data to compute on and details of computation.\nOmit names of common arguments (e.g. data, aes)\nIf you override the default value of an argument, use the full name:\n\n# Good\nmean(1:10, na.rm = TRUE)\n\n# Bad\nmean(x = 1:10, , FALSE)\nmean(, TRUE, x = c(1:10, NA))\n\n\n\n1.3.6 Rule 2.5: Line length: 80 characters\n\nuse one line each for the function name, each argument, and the closing )\n\n\n# Good\ndo_something_very_complicated(\n  something = \"that\",\n  requires = many,\n  arguments = \"some of which may be long\"\n)\n\n# Very bad\ndo_something_very_complicated(\"that\", requires, many, arguments, \"some of which may be long\")\n\n# Still bad\ndo_something_very_complicated(\n  \"that\", requires, many,\n  arguments,\n  \"some of which may be long\"\n)\n\n# Yup, still bad\ndo_something_very_complicated(\n  \"that\", requires, many, arguments,\n  \"some of which may be long\"\n)\n\n\n\n1.3.7 Rule 2.5: Line length\nException: short unnamed arguments can also go on the same line as the function name, even if the whole function call spans multiple lines.\n\nmap(x, f,\n  extra_argument_a = 10,\n  extra_argument_b = c(1, 43, 390, 210209)\n)\n\n\n\n1.3.8 Rule 2.6: Assignment (if you are prissy)\nUse &lt;-, not =, for assignment.\n\n# Good\nx &lt;- 5\n\n# Bad\nx = 5\n\n\n\n1.3.9 Rule 2.8: Quotes\nUse \", not ', for quoting text. The only exception is when the text already contains double quotes and no single quotes.\n\n# Good\n\"Text\"\n'Text with \"quotes\"'\n'&lt;a href=\"http://style.tidyverse.org\"&gt;A link&lt;/a&gt;'\n\n# Bad\n\"Text\"\n'Text with \"double\" and \\'single\\' quotes'\n\n\n\n1.3.10 Rule 2.9: Comments\nIf you need comments to explain what your code is doing, rewrite your code.\nRemarks\n\nThis is counter-intuitive! The problem with comments is that you can change your code without changing the comments. So when you go back and make a change to the code (as is very often necessary), then your comment becomes a source of confusion rather than clarity.\n30535: You can use text in the markdown document to explain what your code is doing in plain English. Use complete sentences. But it is better if you just write the code well.\nLife post 30535: There are times when comments are useful, but I try to use them sparingly.\n\n\n\n1.3.11 Syntax summary\n\nuse whitespace\narguments: data before details\nline length: 80 characters\nassignment: &lt;-\nuse double quotes\navoid comments\nI skipped 2.4 and 2.7 because they relate to material we haven’t learned yet",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "style_guide.html#pipes-with-magrittr",
    "href": "style_guide.html#pipes-with-magrittr",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "1.4 Pipes with magrittr",
    "text": "1.4 Pipes with magrittr\n\n1.4.1 pipes %&gt;%: roadmap\n\nintro\nwhitespace\nlong lines\nshort pipes\nno arguments\nassignment\n\n\n\n1.4.2 Rule 4.1: intro\nUse %&gt;% (or |&gt;, if you are modern) to emphasise a sequence of actions, rather than the object that the actions are being performed on.\nAvoid using the pipe when:\n\nYou need to manipulate more than one object at a time. Reserve pipes for a sequence of steps applied to one primary object.\nThere are meaningful intermediate objects that could be given informative names (cf rule 2.9).\n\n\n\n1.4.3 Rule 4.2: whitespace\n%&gt;% should always have a space before it, and should usually be followed by a new line. After the first step, each line should be indented by two spaces. This structure makes it easier to add new steps (or rearrange existing steps) and harder to overlook a step.\n\n# Good\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize_if(is.numeric, mean) %&gt;%\n  ungroup() %&gt;%\n  gather(measure, value, -Species) %&gt;%\n  arrange(value)\n\n# Bad\niris %&gt;% group_by(Species) %&gt;% summarize_all(mean) %&gt;% \nungroup() %&gt;% gather(measure, value, -Species) %&gt;%\narrange(value)\n\n\n\n1.4.4 Rule 4.4: short pipes I\nIt is ok to keep a one-step pipe in one line:\n\n# Good\niris %&gt;% arrange(Species)\n\n# Mediocre\niris %&gt;%\n  arrange(Species)\n\narrange(iris, Species)\n\n\n\n1.4.5 Rule 4.4: short pipes II\n\n# Bad\nx %&gt;%\n  select(a, b, w) %&gt;%\n  left_join(\n    y %&gt;% filter(!u) %&gt;% gather(a, v, -b) %&gt;% select(a, b, v),\n    by = c(\"a\", \"b\")\n  )\n\n\n\n1.4.6 Rule 4.4: short pipes III\n\n# Good\nx %&gt;%\n  select(a, b, w) %&gt;%\n  left_join(y %&gt;% select(a, b, v), by = c(\"a\", \"b\"))\n\nx_join &lt;-\n  x %&gt;%\n  select(a, b, w)\ny_join &lt;-\n  y %&gt;%\n  filter(!u) %&gt;%\n  gather(a, v, -b) %&gt;%\n  select(a, b, v)\nleft_join(x_join, y_join, by = c(\"a\", \"b\"))\n\n\n\n1.4.7 Rule 4.5: No arguments\nmagrittr allows you to omit () on functions that don’t have arguments. Avoid this. This way data objects never have parentheses and functions always do.\n\n# Good\nx %&gt;%\n  unique() %&gt;%\n  sort()\n\n# Bad\nx %&gt;%\n  unique %&gt;%\n  sort\n\n\n\n1.4.8 Rule 4.6: Assignment\nUse a separate line for the target of the assignment followed by &lt;-.\n\n# Good\niris_long &lt;-\n  iris %&gt;%\n  gather(measure, value, -Species) %&gt;%\n  arrange(-value)\n\n# Bad\niris_long &lt;- iris %&gt;%\n  gather(measure, value, -Species) %&gt;%\n  arrange(-value)\n\n\n\n1.4.9 Pipes %&gt;% summary\n\npipes are awesome\nuse whitespace\nshort pipes can be on one line\nuse parentheses even if there are no arguments\nassignment on a separate line\n\nSkipped rule 4.3 since redundant to prior chapter",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "style_guide.html#code-style-summary",
    "href": "style_guide.html#code-style-summary",
    "title": "1  An R Code Style Guide (Miratrix version)",
    "section": "1.5 Code style summary",
    "text": "1.5 Code style summary\n\nStyle is awesome. Save a future researcher from spending two months trying to disentangle your spaghetti!\nYou don’t need to memorize these rules! Just as you have spell check and grammarly on your computer for prose, there is a package styler to help you follow the code style guide.\nJust as you still need to learn to spell (since spell checker doesn’t capture everything), you need to learn these rules as well.\n\nIn closing:\n\n“Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.” –Hadley Wickham",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Code Style Guide (Miratrix version)</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html",
    "href": "intro_markdown.html",
    "title": "2  Intro to R Markdown",
    "section": "",
    "text": "2.1 Overview\nR Markdown (and its newer cousin Quarto) is a simple but powerful markdown language which you can use to create documents with inline R code and results. This makes it much easier for you to complete homework assignments and reports; makes it much less likely that your work will include errors; and makes your work much easier to reproduce. For example, if you find you have to drop cases from your dataset, you can simply add that line of code to your document, and recompile your document. Any text that’s drawn directly from your analyses will be automatically updated.\nOther R packages, such as Sweave and knitr, allow you to do the same things, but R Markdown has the added advantage of being relatively simple to use. This document will show you how to use R Markdown to create documents which draw directly on your data to produce reports.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#getting-started",
    "href": "intro_markdown.html#getting-started",
    "title": "2  Intro to R Markdown",
    "section": "2.2 Getting started",
    "text": "2.2 Getting started\nEvery R Markdown document starts with a header. Headers look like this:\n---\ntitle: \"My perfect homework\"\nauthor: \"R master\"\noutput: pdf_document\n---\nA header can contain more or less information, as you see fit. Your computer needs to have a copy of LaTex installed in order to output .pdf documents. If you don’t, you should change output: pdf_document to output: html_document or output: word_document.\nYou identify sections of the document using hashtags; more hashtags indicate less important sections.\nFor example, this:\n# A big section\nproduces a big header (large font, etc.)\nwhile this\n## A small section\nproduces a smaller header (still a large font, but less large).\nAlso, if your document includes a table of contents, the sections get used to automatically generate the table of contents.\nYou can italicize words by writing *italicize* or _italicize_. You can bold words with **bold** or __bold__.\nYou can add superscripts (E=mc2) by writing E=mc^2^.\nYou can create unordered lists:\n- Item 1\n- Item 2\n- Item 3\nto get\n\nItem 1\nItem 2\nItem 3\n\nOr ordered lists:\n1. Item 1\n2. Item 2\n3. Item 3\nto get\n\nItem 1\nItem 2\nItem 3\n\nTo start a new page, just type \\newpage (not relevant for HTML output).\nAs you may have noticed, one of the driving ideas behind R Markdown is that the text should be interpretable even if it’s not compiled. A person should be able to read this text file and understand the basic organization and what all of the symbols denote.\nYou can also add links and images, and do many other things beyond what we’ll show you in this class. There are many resources out there, but here’s one place you can start.\nInstead of writing markdown using this, we note that newer versions of Markdown and Quarto have a visual editor that allows you to format things in the usual way, e.g., control-B for bold. Some people prefer to take that approach.\nRegardless, to compile or knit the document, click on the button that says Knit or Render, or Shift + Ctrl/Cmd + K.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#embedding-r-code",
    "href": "intro_markdown.html#embedding-r-code",
    "title": "2  Intro to R Markdown",
    "section": "2.3 Embedding R code",
    "text": "2.3 Embedding R code\nThere are two main ways to embed R code in R Markdown, code chunks or inline.\n\n2.3.1 Code chunks\nTo insert a code chunk click on Insert on the top right corner of your R Markdown file and select R. Or use keyboard shortcuts: Ctrl + Alt + I for PC and Cmd + Option + I for Mac:\nCode chunks have a number of different options. The most important ones for us are:\n\neval = TRUE, which means every time you knit the file, the code inside the R code chunk will get evaluated. This is the default.\necho = TRUE, which means every time you knit the file, the code inside the R code chunk witll be rendered, and you can see both the code itself and the results from evaluating the code.\n\nFor class, you should keep echo = TRUE, so that we can see your code and be able to tell what went wrong, if something did. You can set echo = FALSE for code chunks that load and manipulate data.\nOther code chunks options you may see in class are:\n\nwarning = FALSE, which means warning messages generated by the code will not be displayed.\nresults = 'asis', which means results will not be reformatted when the file is compiled (useful if results return raw HTLM).\nfig.height and fig.width, which specify the height and width (in inches) of plots created by the chunk.\n\nLet’s try loading some data:\n\nlibrary(haven)\ndat &lt;- read_dta(\"data/neighborhood.dta\")\n\nYou can see the code is displayed, and the command is carried out. The file dat is loaded in the R environment.\nInstead of specifying code chunks options every time, you can specify them globally in the setup chunk by using knitr::opts_chunk$set(echo = TRUE, eval = TRUE). You can then add additional options only to relevant chunks. If you want to exclude specific chunks, you can re-set echo = FALSE and eval = FALSE for those specific chunks.\nRunning code chunks: A good practice is to run individual code chunks to make sure they are doing what you want them to do. You can do this by executing individual lines of code, or whole chunks. Go to Run in the upper right corner and select what chunks to execute, e.g. Run Current Chunk, Run Next Chunk, etc.\n\n\n2.3.2 Inline code\nCode results can also be inserted directly in the text of your R Markdown file. This is particularly useful when you are extracting and interpreting model parameters. You can extract the coefficient from the model and use inline code to report it. If the data or model change, the text will change too when you knit the document.\nTo add inline code, enclose it in `r `. For example, to report the mean reading score, you can use\n\n`r mean(dat$p7read)`\n\nWhich will produce -0.0443549. That’s a few too many decimals, let’s round it off, using\n\n`r round(mean(dat$p7read),2)`\n\nwhich produces “-0.04.”\nHere we used two commands: round and mean. You can use more commands and write more complex inline code, depending on what you want to report.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#embedding-plots",
    "href": "intro_markdown.html#embedding-plots",
    "title": "2  Intro to R Markdown",
    "section": "2.4 Embedding plots",
    "text": "2.4 Embedding plots\nPlots are easy to embed. For example,\n\nlibrary(ggplot2)\n\ndat$male &lt;- factor(dat$male, levels = c(0, 1), labels = c(\"Female\", \"Male\"))\n  \nggplot(data=dat, aes(p7vrq, attain, colour=male)) + \n  geom_point() + \n  labs(title=\"Attainment as a function of verbal reasoning\",\n       x = \"Verbal reasoning quotient\", \n       y = \"Educational attainment\", colour=\"Gender\") +\n  geom_smooth(method=\"lm\", formula = y ~ x, se=FALSE, colour=\"darkorchid3\")\n\n\n\n\n\n\n\n\nGirls are rendered as coral, boys are rendered in turquoise, and the line of best fit is drawn in darkorchid3 (because why not). Just because you have a lot of colors and plotting characters to work with doesn’t mean you need to use them all. In the options, I specified fig.width = 7 and fig.height = 7. Notice that this command draws on dat, which we loaded in a previous chunk. When knitting the document, code chunks get executed in order and the results persist throughout the R Markdown document.\nFor the purposes of class, we want to see both your plot code and the plot itself. It’s not uncommon to use wrong code to create a plot that looks correct (at least visually).",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#embedding-tables",
    "href": "intro_markdown.html#embedding-tables",
    "title": "2  Intro to R Markdown",
    "section": "2.5 Embedding tables",
    "text": "2.5 Embedding tables\nYou can directly render tables in R Markdown. The idea is, inside an R chunk, you call a command that prints out a table. The report then takes this printout and integrates it into your overall report. There are many different packages to make tables, but in class we’ll mostly use knitr, texreg, stargazer, and the tab_model() function in sjPlot.\nYou can use these packages to create a descriptive table. For example:\n\nhead( dat ) %&gt;%\n  knitr::kable( digits = 2 )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nneighid\nschid\nattain\np7vrq\np7read\ndadocc\ndadunemp\ndaded\nmomed\nmale\ndeprive\n\n\n\n\n675\n0\n0.74\n21.97\n12.13\n2.32\n0\n0\n0\nMale\n-0.18\n\n\n647\n0\n0.26\n-7.03\n-12.87\n16.20\n0\n0\n1\nFemale\n0.21\n\n\n650\n0\n-1.33\n-11.03\n-31.87\n-23.45\n1\n0\n0\nMale\n0.53\n\n\n650\n0\n0.74\n3.97\n3.13\n2.32\n0\n0\n0\nMale\n0.53\n\n\n648\n0\n-0.13\n-2.03\n0.13\n-3.45\n0\n0\n0\nFemale\n0.19\n\n\n648\n0\n0.56\n-5.03\n-0.87\n-3.45\n0\n0\n0\nFemale\n0.19\n\n\n\n\nSee Chapter 6 for more on making various tables.\nWe can also use texreg or stargazer to create a taxonomy of regression models. We recommend texreg, which automatically outputs the variances of random effects (more on this soon).\nFor example:\n\nlibrary(texreg)\n\n# fit some models \nm1 &lt;- lm(attain ~ male, data=dat)\nm2 &lt;- lm(attain ~ male + momed, data=dat)\nm3 &lt;- lm(attain ~ male + momed + daded, data=dat)\n\nscreenreg(list(m1,m2,m3), \n          custom.coef.names=c(\"Intercept\", \"Male\", \n                              \"Maternal education\", \"Paternal education\"))\n\n\n=========================================================\n                    Model 1      Model 2      Model 3    \n---------------------------------------------------------\nIntercept              0.15 ***     0.03        -0.02    \n                      (0.03)       (0.03)       (0.03)   \nMale                  -0.12 **     -0.12 **     -0.12 ** \n                      (0.04)       (0.04)       (0.04)   \nMaternal education                  0.49 ***     0.24 ***\n                                   (0.05)       (0.05)   \nPaternal education                               0.54 ***\n                                                (0.06)   \n---------------------------------------------------------\nR^2                    0.00         0.05         0.09    \nAdj. R^2               0.00         0.05         0.08    \nNum. obs.           2310         2310         2310       \n=========================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nBoth packages include a lot of options and make it easy to produce publication-quality tables with little effort. See later chapters of this book (Chapter 7, in particular) for more detail.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#embedding-math",
    "href": "intro_markdown.html#embedding-math",
    "title": "2  Intro to R Markdown",
    "section": "2.6 Embedding math",
    "text": "2.6 Embedding math\nWe’ll be writing a lot of mathematical models in class. R Markdown can use LaTeX style math-writing to display mathematical script. Another chapter in the book has more resources with LaTeXsyntax for the mostly commonly used models in the class. Similar to code chunks and inline code, you can use LaTeX for single or multiple equations, or for individual parameters embedded in the text.\nFor example, the following statement\n$$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$$\ncompiles to\n\\[Y_i = \\beta_0 + \\beta_1 Y_i + \\epsilon_i\\]\nAnd the following statement $\\mu$ compiles to \\(\\mu\\). This will be very helpful when we ask you to match R output to model parameters in homework.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown</span>"
    ]
  },
  {
    "objectID": "intro_markdown.html#help-r-markdown-report-generation-doesnt-work",
    "href": "intro_markdown.html#help-r-markdown-report-generation-doesnt-work",
    "title": "2  Intro to R Markdown",
    "section": "2.7 Help! R Markdown report generation doesn’t work",
    "text": "2.7 Help! R Markdown report generation doesn’t work\nDon’t put “View()” in your Markdown file when loading your csv file. Just put in the read_csv line. Otherwise you will not be able to knit.\nAlso watch for the skim() command–it can crash report generation as well.\nIf you can’t knit PDFs you need to install latex (tex). Once you do, reboot your computer. If things don’t work, then knit to Microsoft word (or, failing that, html as a last resort), print to pdf, and turn that in. But then ask a teaching fellow to help get things set up, since PDFs make for much more readable reports.\n\n2.7.1",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to R Markdown</span>"
    ]
  },
  {
    "objectID": "configuring_code_chunks.html",
    "href": "configuring_code_chunks.html",
    "title": "3  Configuring Rmarkdown chunks",
    "section": "",
    "text": "3.1 Options for including/suppressing code and output\ninclude: Should chunk be included in knit file? Defaults to TRUE. If FALSE, code chunk is run, but chunk and any output is not included in the knit file.\neval: Should chunk be evaluated by R? Defaults to TRUE. If FALSE, code chunk is included in the knit file, but not run.\necho: Should the code from this chunk be included in knit file along with output? Defaults to TRUE. If FALSE, the output from the chunk is included, but the code that created it is not. Most useful for plots.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Configuring Rmarkdown chunks</span>"
    ]
  },
  {
    "objectID": "configuring_code_chunks.html#options-for-includingsuppressing-r-messages",
    "href": "configuring_code_chunks.html#options-for-includingsuppressing-r-messages",
    "title": "3  Configuring Rmarkdown chunks",
    "section": "3.2 Options for including/suppressing R messages",
    "text": "3.2 Options for including/suppressing R messages\nR has “errors” meaning it could not run your code, “warnings” meaning that the code was wrong, but there are some potential issues with it, and “messages” which are simply information about what your code ran. You can include or suppress each of these types of message.\nerror: Should R continue knitting if code produces an error? Defaults to FALSE. Generally don’t want to change this because it means you can miss serious issues with your code.\nwarning: Should R include warnings in knit file? Defaults to TRUE.\nmessage: Should R include informational messages in knit file? Defaults to TRUE. Easy way to clean up your markdowns.\n\n#This code produces an error\ndat %&gt;%\n  filter(dest = 1)\n\nError in `filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `dest == 1`?\n\n#Example warning\nparse_number(c(\"1\", \"$3432\", \"tomato\"))\n\n[1]    1 3432   NA\nattr(,\"problems\")\n# A tibble: 1 × 4\n    row   col expected actual\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt; \n1     3    NA a number tomato\n\n#Example message\nlibrary(gridExtra)",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Configuring Rmarkdown chunks</span>"
    ]
  },
  {
    "objectID": "configuring_code_chunks.html#options-for-modifying-figure-outputs",
    "href": "configuring_code_chunks.html#options-for-modifying-figure-outputs",
    "title": "3  Configuring Rmarkdown chunks",
    "section": "3.3 Options for modifying figure outputs",
    "text": "3.3 Options for modifying figure outputs\nYou can control figure size and shape (see more in ?sec-plot-tips).\nIn particular, consider these:\nout.width: What percentage of the page width should output take?\nfig.height: What should be the height of figures?\nfig.width: What should be the width of figures?\nfig.asp: What should be the aspect ratio of figures?\nfig.align: How should figures be aligned?\nWe might want a bigger plot for this:\n\n\n\n\n\n\n\n\n\nAnd a smaller plot for this:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n$x\n[1] \"Minutes of delay\"\n\n$y\n[1] \"Destimations\"\n\nattr(,\"class\")\n[1] \"labels\"",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Configuring Rmarkdown chunks</span>"
    ]
  },
  {
    "objectID": "configuring_code_chunks.html#changing-your-defaults",
    "href": "configuring_code_chunks.html#changing-your-defaults",
    "title": "3  Configuring Rmarkdown chunks",
    "section": "3.4 Changing your defaults",
    "text": "3.4 Changing your defaults\nAt the beginning of your code, you can set custom defaults so all your chunks will render the same way (unless you override by specifically adding arguments to a chunk itself). This is handy in that you will then not need to repeat the custom arguments in each code chunk. For example, you can set a default figure size.\nHere is an example:\n\n knitr::opts_chunk$set(echo = TRUE, \n                       fig.width = 5,\n                       fig.height = 3,\n                       out.width = \"5in\", \n                       out.height = \"3in\", fig.align = \"center\")",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Configuring Rmarkdown chunks</span>"
    ]
  },
  {
    "objectID": "intro_linear_regression.html",
    "href": "intro_linear_regression.html",
    "title": "4  Intro to Regression",
    "section": "",
    "text": "4.1 Simple Regression\nWe are going to use an example dataset, RestaurantTips, that records tip amounts for a series of bills. Let’s first regress Tip on Bill. Before doing regression, we should plot the data to make sure using simple linear regression is reasonable. For kicks, we add in an automatic regression line as well by taking advantage of ggplot’s geom_smooth() method:\n# load the data into memory\ndata(RestaurantTips)\n\n# plot Tip on Bill\nggplot( RestaurantTips, aes(x = Bill, y = Tip) ) +\n    geom_point() +\n    geom_smooth( method=\"lm\", se=FALSE ) +\n    geom_smooth( method=\"loess\", se=FALSE, col=\"orange\" ) +\n    labs(title = \"Tip given Bill\")\nThat looks pretty darn linear! There are a few unusually large tips, but no extreme outliers, and variability appears to be constant at all levels of Bill , so we proceed:\n# fit the linear model\nmod &lt;- lm(Tip ~ Bill, data = RestaurantTips)\nsummary(mod)\n\n\nCall:\nlm(formula = Tip ~ Bill, data = RestaurantTips)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.391 -0.489 -0.111  0.284  5.974 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.29227    0.16616   -1.76    0.081 .  \nBill         0.18221    0.00645   28.25   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.98 on 155 degrees of freedom\nMultiple R-squared:  0.837, Adjusted R-squared:  0.836 \nF-statistic:  798 on 1 and 155 DF,  p-value: &lt;2e-16\nThe first line tells R to fit the regression. The thing on the left of the ~ is our outcome, the things on the right are our covariates or predictors. R then saves the results of all that work under the name mod (short for model - you can call it anything you want). Once we fit the model, we used summary() command to print the output to the screen.\nResults relevant to the intercept are in the (Intercept) row and results relevant to the slope are in the Bill row (Bill is the explanatory variable). The Estimate column gives the estimated coefficients, the Std. Error column gives the standard error for these estimates, the t value is simply estimate/SE, and the p-value is the result of a hypothesis test testing whether that coefficient is significantly different from 0.\nWe also see the RMSE as Residual standard error and \\(R^2\\) as Multiple R-squared. The last line of the regression output gives details relevant to an ANOVA table for testing our model against no model. It has the F-statistic, degrees of freedom, and p-value.\nYou can pull the coefficients of your model out with the coef() command:\ncoef(mod)\n\n(Intercept)        Bill \n     -0.292       0.182 \n\ncoef(mod)[1] # intercept\n\n(Intercept) \n     -0.292 \n\ncoef(mod)[2] # slope\n\n Bill \n0.182 \n\ncoef(mod)[\"Bill\"] # alternate way.\n\n Bill \n0.182\nAlternatively, you can use the tidy() function from broom to turn the regression results into a tidy data frame, which makes it easier to work with:\ntidy(mod)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -0.292   0.166       -1.76 8.06e- 2\n2 Bill           0.182   0.00645     28.2  5.24e-63\n\ntidy(mod)[[2,2]] # slope\n\n[1] 0.182\nWe can plot our regression line on top of the scatterplot manually using the geom_abline() layer in ggplot:\nggplot( RestaurantTips, aes( Bill, Tip ) ) +\n  geom_point() +\n  geom_abline( intercept = -0.292, slope =  0.182, col=\"red\" )",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Regression</span>"
    ]
  },
  {
    "objectID": "intro_linear_regression.html#multiple-regression",
    "href": "intro_linear_regression.html#multiple-regression",
    "title": "4  Intro to Regression",
    "section": "4.2 Multiple Regression",
    "text": "4.2 Multiple Regression\nWe now include the additional explanatory variables of number in party (Guests) and whether or not they pay with a credit card (Credit):\n\ntip.mod &lt;- lm(Tip ~ Bill + Guests + Credit, data=RestaurantTips )\nsummary(tip.mod)\n\n\nCall:\nlm(formula = Tip ~ Bill + Guests + Credit, data = RestaurantTips)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.384 -0.478 -0.108  0.272  5.984 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.25468    0.20273   -1.26     0.21    \nBill         0.18302    0.00846   21.64   &lt;2e-16 ***\nGuests      -0.03319    0.10282   -0.32     0.75    \nCredity      0.04217    0.18282    0.23     0.82    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.985 on 153 degrees of freedom\nMultiple R-squared:  0.838, Adjusted R-squared:  0.834 \nF-statistic:  263 on 3 and 153 DF,  p-value: &lt;2e-16\n\n\nThis output should look very similar to the output for one variable, except now there is a row corresponding to each explanatory variable. Our two-category (y, n) Credit variable was automatically converted to a 0-1 dummy variable (with “y” being 1 and “n” our baseline).\nYou can make plots and tables of your fit models. For one easy kind of regression graph, try ggeffects:\n\n# graph model 2, with Bill on X, Credit as color, and Guests held constant at the mean\nggeffect(tip.mod, terms = c(\"Bill\", \"Credit\")) |&gt; \n  plot(add.data = TRUE, ci = FALSE)\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n\n\n\n\n\nFor making tables, Chapter 7.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Regression</span>"
    ]
  },
  {
    "objectID": "intro_linear_regression.html#categorical-variables-and-factors",
    "href": "intro_linear_regression.html#categorical-variables-and-factors",
    "title": "4  Intro to Regression",
    "section": "4.3 Categorical Variables (and Factors)",
    "text": "4.3 Categorical Variables (and Factors)\nYou can include any explanatory categorical variable in a multiple regression model, and R will automatically create corresponding 0/1 variables. For example, if you were to include gender coded as male/female, R would create a variable GenderMale that is 1 for males and 0 for females.\n\n4.3.1 Numbers Coding Categories.\nIf you have multiple levels of a category, but your levels are coded with numbers you have to be a bit careful because R can treat this as a quantitative (continuous) variable by mistake in some cases. You will know it did this if you only see the single variable on one line of your output. For categorical variables with \\(k\\) categories, you should see \\(k-1\\) lines.\nTo make a variable categorical, even if the levels are numbers, convert the variable to a factor with as.factor or factor:\n\n# load the US states data\ndata( USStates )\n\n# convert Region to a factor\nUSStates &lt;- USStates |&gt; \n  mutate(Region = factor(Region))\n\n\n\n4.3.2 Setting new baselines.\nWe can reorder the levels if desired (the first is our baseline).\n\nlevels( USStates$Region )\n\n[1] \"MW\" \"NE\" \"S\"  \"W\" \n\nUSStates$Region = relevel(USStates$Region, \"S\" )\nlevels( USStates$Region )\n\n[1] \"S\"  \"MW\" \"NE\" \"W\" \n\n\nNow any regression will use the south as baseline.\n\n\n4.3.3 Testing for significance of a categorical variable.\nWhen deciding whether to keep a categorical variable, we need to test how important all the dummy variables for that category are to the model all at once. We do this with ANOVA. Here we examine whether region is useful for predicting the percent vote for Clinton in 2016:\n\nmlm = lm( ClintonVote ~ Region, data=USStates)\nanova( mlm )\n\nAnalysis of Variance Table\n\nResponse: ClintonVote\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nRegion     3   1643     548    6.99 0.00057 ***\nResiduals 46   3603      78                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt is quite important.\nWe can also compare for region beyond some other variable:\n\nmlm2 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath, data=USStates)\n\nmlm3 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath + Region, data=USStates)\nanova( mlm2, mlm3 )\n\nAnalysis of Variance Table\n\nModel 1: ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n    EighthGradeMath\nModel 2: ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n    EighthGradeMath + Region\n  Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)  \n1     46 3287                           \n2     43 2649  3       638 3.45  0.025 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRegion is still important, beyond including some further controls. Interpreting this mess of a regression is not part of this document; this document shows you how to run regressions but it doesn’t discuss whether you should or not.\n\n\n4.3.4 Missing levels in a factor\nR often treats categorical variables as factors. This is often useful, but sometimes annoying. A factor has different levels which are the different values it can be. For example:\n\ndata(FishGills3)\nlevels(FishGills3$Calcium)\n\n[1] \"\"       \"High\"   \"Low\"    \"Medium\"\n\ntable(FishGills3$Calcium)\n\n\n         High    Low Medium \n     0     30     30     30 \n\n\nNote the weird nameless level; it also has no actual observations in it. Nevertheless, if you make a boxplot, you will get an empty plot in addition to the other three. This error was likely due to some past data entry issue. You can drop the unused level:\n\nFishGills3$Calcium = droplevels(FishGills3$Calcium)\n\nYou can also turn a categorical variable into a numeric one like so:\n\nsummary( FishGills3$Calcium )\n\n  High    Low Medium \n    30     30     30 \n\nasnum = as.numeric( FishGills3$Calcium )\nasnum\n\n [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3\n[39] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[77] 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nRegression on only a categorical variable is fine:\n\nmylm = lm( GillRate ~ Calcium, data=FishGills3 )\nmylm\n\n\nCall:\nlm(formula = GillRate ~ Calcium, data = FishGills3)\n\nCoefficients:\n  (Intercept)     CalciumLow  CalciumMedium  \n         58.2           10.3            0.5  \n\n\nR has made you a bunch of dummy variables automatically. Here “high” is the baseline, selected automatically. We can also force it so there is no baseline by removing the intercept, in which case the coefficients are the means of each group.\n\nmymm = lm( GillRate ~ 0 + Calcium, data=FishGills3 )\nmymm\n\n\nCall:\nlm(formula = GillRate ~ 0 + Calcium, data = FishGills3)\n\nCoefficients:\n  CalciumHigh     CalciumLow  CalciumMedium  \n         58.2           68.5           58.7",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Regression</span>"
    ]
  },
  {
    "objectID": "intro_linear_regression.html#some-extensions-optional",
    "href": "intro_linear_regression.html#some-extensions-optional",
    "title": "4  Intro to Regression",
    "section": "4.4 Some extensions (optional)",
    "text": "4.4 Some extensions (optional)\n\n4.4.1 Confidence Intervals\nTo get confidence intervals around each parameter in your model, try this:\n\nconfint(tip.mod)\n\n             2.5 % 97.5 %\n(Intercept) -0.655  0.146\nBill         0.166  0.200\nGuests      -0.236  0.170\nCredity     -0.319  0.403\n\n\nYou can also create them easily using tidy and mutate:\n\ntip.mod |&gt; \n  tidy() |&gt; \n  mutate(upper = estimate + 1.96*std.error,\n         lower = estimate - 1.96*std.error)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic  p.value upper  lower\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)  -0.255    0.203      -1.26  2.11e- 1 0.143 -0.652\n2 Bill          0.183    0.00846    21.6   2.07e-48 0.200  0.166\n3 Guests       -0.0332   0.103      -0.323 7.47e- 1 0.168 -0.235\n4 Credity       0.0422   0.183       0.231 8.18e- 1 0.400 -0.316\n\n\n\n\n4.4.2 Prediction\nSuppose a server at this bistro is about to deliver a $20 bill, and wants to predict their tip. They can get a predicted value and 95% (this is the default level, change with level) prediction interval with\n\nnew.dat = data.frame( Bill = c(20) )\npredict(mod,new.dat,interval = \"prediction\")\n\n   fit  lwr  upr\n1 3.35 1.41 5.29\n\n\nThey should expect a tip somewhere between $1.41 and $5.30.\nIf we know a bit more we can use our more complex model called tip.mod from above:\n\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod,new.dat,interval = \"prediction\")\n\n   fit  lwr  upr\n1 3.37 1.41 5.34\n\n\nThis is the predicted tip for one guest paying with cash for a $20 tip. It is wider than our original interval because our model is a bit more unstable (it turns out guest number and credit card aren’t that relevant or helpful).\nCompare the prediction interval to the confidence interval\n\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod, new.dat, interval = \"confidence\")\n\n   fit  lwr  upr\n1 3.37 3.09 3.65\n\n\nThis predicts the mean tip for all single guests who pay a $20 bill with cash. Our interval is smaller because we are generating a confidence interval for where the mean is, and are ignoring that individuals will vary around that mean. Confidence intervals are different from prediction intervals.\n\n\n4.4.3 Removing Outliers\nIf you can identify which rows the outliers are on, you can do this by hand (say the rows are 5, 10, 12).\n\nnew.data = old.data[ -c(5,10,12), ]\nlm( Y ~ X, data=new.data )\n\nSome technical details: The c(5,10,12) is a list of 3 numbers. The c() is the concatenation function that takes things makes lists out of them. The “-list” notation means give me my old data, but without rows 5, 10, and 12. Note the comma after the list. This is because we identify elements in a dataframe with row, column notation. So old.data[1,3] would be row 1, column 3.\nIf you notice your points all have X bigger than some value, say 20.5, you could use filtering to keep everything less than some value:\n\nnew.data = filter( old.data, X &lt;= 20.5 )\n\n\n\n4.4.4 Missing data\nIf you have missing data, lm will automatically drop those cases because it doesn’t know what else to do. It will tell you this, however, with the summary command.\n\ndata(AllCountries)\ndev.lm = lm( BirthRate ~ Rural + Health + ElderlyPop, data=AllCountries )\nsummary( dev.lm  )\n\n\nCall:\nlm(formula = BirthRate ~ Rural + Health + ElderlyPop, data = AllCountries)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.592  -3.728  -0.791   3.909  16.218 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26.5763     1.6795   15.82  &lt; 2e-16 ***\nRural         0.0985     0.0224    4.40  1.9e-05 ***\nHealth       -0.0995     0.0930   -1.07     0.29    \nElderlyPop   -1.0249     0.0881  -11.64  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.83 on 174 degrees of freedom\n  (39 observations deleted due to missingness)\nMultiple R-squared:  0.663, Adjusted R-squared:  0.657 \nF-statistic:  114 on 3 and 174 DF,  p-value: &lt;2e-16\n\n\n\n\n4.4.5 Residual plots and model fit\nIf we throw out model into the plot function, we get some nice regression diagnostics.\n\nplot(tip.mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo generate classic model fit diagnostics with more control, we need to calculate residuals, make a residual versus fitted values plot, and make a histogram of the residuals. We can make some quick and dirty plots with qplot (standing for “quick plot”) like so:\n\nqplot(tip.mod$fit, tip.mod$residuals )\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nand\n\nqplot(tip.mod$residuals, bins=30)\n\n\n\n\n\n\n\n\nWe see no real pattern other than some extreme outliers. The residual histogram suggests we are not really normally distributed, so we should treat our SEs and \\(p\\)-values with caution. These plots are the canonical “model-checking’’ plots you might use.\nAnother is the “fitted outcomes vs. actual outcomes’’ plot of:\n\npredicted = predict( dev.lm )\nactual = dev.lm$model$BirthRate\nqplot( actual, predicted, main=\"Fit vs. actual Birth Rate\" )\n\n\n\n\n\n\n\n\nNote the dev.lm variable has a model variable inside it. This is a data frame of the used data for the model (i.e., if cases were dropped due to missingness, they will not be in the model). We then grab the birth rates from this, and make a scatterplot. If we tried to skip this, and use the original data, we would get an error because our original data set has some observations that were dropped.\nNote we can’t just add our predictions to AllCountries since we would get an error due to this dropped data issue:\n\nAllCountries$predicted = predict( dev.lm )\n\nError in `$&lt;-.data.frame`(`*tmp*`, predicted, value = c(`1` = 31.630301617421,  : \n  replacement has 179 rows, data has 217\nWe can, however, predict like this:\n\nAllCountries$predicted = predict( dev.lm, newdata=AllCountries )\n\nThe newdata tells predict to generate a prediction for each row in AllCountries rather than each row in the left over data after lm dropped cases with missing values.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Regression</span>"
    ]
  },
  {
    "objectID": "summarizing_data.html",
    "href": "summarizing_data.html",
    "title": "5  Summarizing and exploring data",
    "section": "",
    "text": "5.1 National Youth Survey Example\nOur running example is the National Youth Survey (NYS) data as described in Raudenbush and Bryk, page 190. This data comes from a survey in which the same students were asked yearly about their acceptance of 9 “deviant” behaviors (such as smoking marijuana, stealing, etc.). The study began in 1976, and followed two cohorts of children, starting at ages 11 and 14 respectively. We will analyze the first 5 years of data.\nAt each time point, we have measures of:\nBoth of these variables have been transformed to a logarithmic scale to reduce skew.\nFor each student, we have:\nWe’ll focus on the first cohort, from ages 11-15. First, let’s read the data. Note that this data frame is in “wide format”. That is, there is only one row for each student, with all the different observations for that student in different columns of that one row.\nnyswide &lt;- read_csv(\"data/nyswide.csv\")\nhead(nyswide)\n\n# A tibble: 6 × 14\n     ID ATTIT.11 EXPO.11 ATTIT.12 EXPO.12 ATTIT.13 EXPO.13 ATTIT.14 EXPO.14\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     3     0.11   -0.37     0.2    -0.27     0      -0.37     0      -0.27\n2     8     0.29    0.42     0.29    0.2      0.11    0.42     0.51    0.2 \n3     9     0.8     0.47     0.58    0.52     0.64    0.2      0.75    0.47\n4    15     0.44    0.07     0.44    0.32     0.89    0.47     0.75    0.26\n5    33     0.2    -0.27     0.64   -0.27     0.69   -0.27    NA      NA   \n6    45     0.11    0.26     0.37   -0.17     0.37    0.14     0.37    0.14\n# ℹ 5 more variables: ATTIT.15 &lt;dbl&gt;, EXPO.15 &lt;dbl&gt;, FEMALE &lt;dbl&gt;,\n#   MINORITY &lt;dbl&gt;, INCOME &lt;dbl&gt;\nGenerally, we would want such data in “long format”, i.e. each student has multiple rows for the different observations. The pivot_longer() command does this for us.\nnys1 &lt;- nyswide |&gt; \n  pivot_longer(ATTIT.11:EXPO.15, names_to = \"score\") |&gt; \n  mutate(outcome = word(score, 1, 1, sep = \"\\\\.\"),\n         age = as.numeric(word(score, 2, 2, sep = \"\\\\.\")),\n         age_fac = factor(age)) |&gt; \n  select(-score) |&gt; \n  pivot_wider(names_from = outcome) |&gt; \n  # drop missing ATTIT values\n  drop_na(ATTIT)\n\nhead( nys1 )\n\n# A tibble: 6 × 8\n     ID FEMALE MINORITY INCOME   age age_fac ATTIT  EXPO\n  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     3      1        0      3    11 11       0.11 -0.37\n2     3      1        0      3    12 12       0.2  -0.27\n3     3      1        0      3    13 13       0    -0.37\n4     3      1        0      3    14 14       0    -0.27\n5     3      1        0      3    15 15       0.11 -0.17\n6     8      0        0      4    11 11       0.29  0.42\nJust to get a sense of the data, let’s plot each age as a boxplot\nggplot(nys1, aes(age_fac, ATTIT)) +\n    geom_boxplot() + \n    labs(title = \"Boxplot of attitude towards deviance by age\", \n         x = \"Age\", y = \"Attitude towards deviance\")\nNote: The boxplot’s “x” variable is the group. You get one box per group. The “y” variable is the data we are making boxplots of.\nNote some features of the data:\nIf we plot individual lines, grouped by gender and minority status, we have:\nnys1 |&gt; \n  drop_na() |&gt; \n  ggplot(aes(age, ATTIT, group=ID)) +\n  facet_grid( FEMALE ~ MINORITY ) +\n    geom_line(alpha=0.2, position = \"jitter\") + \n    labs(title = \"Individual trajectories of attitude towards deviance over time\",\n         x = \"Age\",\n         y =\"Attitude towards deviance\")\nIf we squint, we can kind of see correlation of residuals: some students have systematically lower trajectories and some students have systematically higher trajectories (although there is a lot of bouncing around).",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarizing and exploring data</span>"
    ]
  },
  {
    "objectID": "summarizing_data.html#national-youth-survey-example",
    "href": "summarizing_data.html#national-youth-survey-example",
    "title": "5  Summarizing and exploring data",
    "section": "",
    "text": "ATTIT, the attitude towards deviance, with higher numbers implying higher tolerance for deviant behaviors.\nEXPO, the “exposure”, based on asking the children how many friends they had who had engaged in each of the “deviant” behaviors.\n\n\n\n\nGender (binary)\nMinority status (binary)\nFamily income, in units of $10K (this can be either categorical or continuous).\n\n\n\n\n\n\n\n\n\n\nFirst, we see that ATTIT goes up over time.\nSecond, we see the variation of points also goes up over time. This is evidence of heteroskedasticity.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarizing and exploring data</span>"
    ]
  },
  {
    "objectID": "summarizing_data.html#tabulating-data-categorical-variables",
    "href": "summarizing_data.html#tabulating-data-categorical-variables",
    "title": "5  Summarizing and exploring data",
    "section": "5.2 Tabulating data (Categorical variables)",
    "text": "5.2 Tabulating data (Categorical variables)\nWe can tabulate data as so:\n\ntable(nys1$age)\n\n\n 11  12  13  14  15 \n202 209 230 220 218 \n\n\nor\n\ntable(nys1$MINORITY, nys1$age)\n\n   \n     11  12  13  14  15\n  0 159 165 182 175 175\n  1  43  44  48  45  43\n\n\nInterestingly, we have more observations for later ages.\nWe can make “proportion tables” as well:\n\nprop.table( table( nys1$MINORITY, nys1$INCOME  ), margin=1 )\n\n   \n          1       2       3       4       5       6       7       8       9\n  0 0.06075 0.13551 0.18341 0.18107 0.14369 0.10981 0.06893 0.05257 0.00935\n  1 0.28251 0.41704 0.12556 0.05830 0.05830 0.02242 0.01345 0.00000 0.00000\n   \n         10\n  0 0.05491\n  1 0.02242\n\n\nThe margin determines what adds up to 100%.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarizing and exploring data</span>"
    ]
  },
  {
    "objectID": "summarizing_data.html#summary-statistics-for-continuous-variables",
    "href": "summarizing_data.html#summary-statistics-for-continuous-variables",
    "title": "5  Summarizing and exploring data",
    "section": "5.3 Summary statistics for continuous variables",
    "text": "5.3 Summary statistics for continuous variables\nThe tableone package is useful:\n\n  library(tableone)\n  \n# sample mean  \n  CreateTableOne(data = nys1,\n                 vars = c(\"ATTIT\"))\n\n                   \n                    Overall    \n  n                 1079       \n  ATTIT (mean (SD)) 0.33 (0.27)\n\n# you can also stratify by a variables of interest\n  CreateTableOne(data = nys1,\n                 vars = c(\"ATTIT\"), \n                 strata = c(\"FEMALE\"))\n\n                   Stratified by FEMALE\n                    0           1           p      test\n  n                  559         520                   \n  ATTIT (mean (SD)) 0.37 (0.27) 0.29 (0.27) &lt;0.001     \n\n# you can also include binary variables\n  CreateTableOne(data = nys1, \n                 vars = c(\"ATTIT\", \"age_fac\"),  # include both binary and continuous variables here\n                 factorVars = c(\"age_fac\"), # include only binary variables here\n                 strata = c(\"FEMALE\"))\n\n                   Stratified by FEMALE\n                    0            1            p      test\n  n                  559          520                    \n  ATTIT (mean (SD)) 0.37 (0.27)  0.29 (0.27)  &lt;0.001     \n  age_fac (%)                                  0.991     \n     11              106 (19.0)    96 (18.5)             \n     12              105 (18.8)   104 (20.0)             \n     13              119 (21.3)   111 (21.3)             \n     14              115 (20.6)   105 (20.2)             \n     15              114 (20.4)   104 (20.0)",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarizing and exploring data</span>"
    ]
  },
  {
    "objectID": "summarizing_data.html#descriptive-statistics-with-the-psych-package",
    "href": "summarizing_data.html#descriptive-statistics-with-the-psych-package",
    "title": "5  Summarizing and exploring data",
    "section": "5.4 Descriptive Statistics with the psych Package",
    "text": "5.4 Descriptive Statistics with the psych Package\nAnother package for obtaining detailed descriptive statistics for your data is the psych package in R, which has describe(), a function that generates a comprehensive summary of each variable in your dataset.\nIf you haven’t already installed the psych package, you can do so using install.packages(). You then load the library as so:\n\n# install.packages(\"psych\")\nlibrary(psych)\n\nThe describe() function provides descriptive statistics such as mean, standard deviation, skewness, and kurtosis for each variable in your dataset.\nHere’s an example using a built-in dataset, iris:\n\nsummary_stats &lt;- describe(nys1)\nprint(summary_stats)\n\n         vars    n   mean     sd median trimmed    mad   min     max   range\nID          1 1079 841.47 483.55 851.00  839.79 597.49  3.00 1720.00 1717.00\nFEMALE      2 1079   0.48   0.50   0.00    0.48   0.00  0.00    1.00    1.00\nMINORITY    3 1079   0.21   0.41   0.00    0.13   0.00  0.00    1.00    1.00\nINCOME      4 1079   4.10   2.35   4.00    3.87   2.97  1.00   10.00    9.00\nage         5 1079  13.04   1.40  13.00   13.05   1.48 11.00   15.00    4.00\nage_fac*    6 1079   3.04   1.40   3.00    3.05   1.48  1.00    5.00    4.00\nATTIT       7 1079   0.33   0.27   0.29    0.31   0.27  0.00    1.24    1.24\nEXPO        8 1079   0.00   0.30  -0.09   -0.03   0.27 -0.37    1.04    1.41\n          skew kurtosis    se\nID        0.01    -1.18 14.72\nFEMALE    0.07    -2.00  0.02\nMINORITY  1.45     0.09  0.01\nINCOME    0.79     0.03  0.07\nage      -0.04    -1.27  0.04\nage_fac* -0.04    -1.27  0.04\nATTIT     0.63    -0.36  0.01\nEXPO      0.88     0.31  0.01\n\n\nThe describe() function generates a table with the following columns:\n\nvars: The variable number.\nn: Number of valid cases.\nmean: The mean of the variable.\nsd: The standard deviation.\nmedian: The median of the variable.\ntrimmed: The mean after trimming 10% of the data from both ends.\nmad: The median absolute deviation (a robust estimate of the variability).\nmin: The minimum value.\nmax: The maximum value.\nrange: The range (max - min).\nskew: The skewness (measure of asymmetry).\nkurtosis: The kurtosis (measure of peakedness).\nse: The standard error.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarizing and exploring data</span>"
    ]
  },
  {
    "objectID": "summarizing_data.html#the-skimr-package",
    "href": "summarizing_data.html#the-skimr-package",
    "title": "5  Summarizing and exploring data",
    "section": "5.5 The skimr Package",
    "text": "5.5 The skimr Package\nYet another package that provides a comprehensive summary of your data is the skimr package. This package is more about exploring data in the moment, and less about report generation, however.\nOne warning is skimr can generate special characters that can crash a R markdown report in some cases–so if you are using it, and getting weird errors when trying to render your reports, try commenting out the skim() call. Using it is simple:\n\nskimr::skim( nys1 )\n\n\nData summary\n\n\nName\nnys1\n\n\nNumber of rows\n1079\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nage_fac\n0\n1\nFALSE\n5\n13: 230, 14: 220, 15: 218, 12: 209\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nID\n0\n1\n841.47\n483.55\n3.00\n422.00\n851.00\n1242.00\n1720.00\n▇▇▆▇▆\n\n\nFEMALE\n0\n1\n0.48\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nMINORITY\n0\n1\n0.21\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nINCOME\n0\n1\n4.10\n2.35\n1.00\n2.00\n4.00\n5.00\n10.00\n▇▇▅▂▂\n\n\nage\n0\n1\n13.04\n1.40\n11.00\n12.00\n13.00\n14.00\n15.00\n▇▇▇▇▇\n\n\nATTIT\n0\n1\n0.33\n0.27\n0.00\n0.11\n0.29\n0.51\n1.24\n▇▅▃▂▁\n\n\nEXPO\n0\n1\n0.00\n0.30\n-0.37\n-0.27\n-0.09\n0.20\n1.04\n▇▃▃▁▁",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarizing and exploring data</span>"
    ]
  },
  {
    "objectID": "summarizing_data.html#summarizing-by-group",
    "href": "summarizing_data.html#summarizing-by-group",
    "title": "5  Summarizing and exploring data",
    "section": "5.6 Summarizing by group",
    "text": "5.6 Summarizing by group\nTo plot summaries by group, first aggregate your data, and plot the results. Do like so:\n\naggdat = nys1 %&gt;% \n  group_by( ID, FEMALE, MINORITY) %&gt;%\n  summarize( avg.ATTIT = mean( ATTIT, na.rm=TRUE ),\n             n_obs = n(), .groups=\"drop\" )\n\nhead( aggdat )\n\n# A tibble: 6 × 5\n     ID FEMALE MINORITY avg.ATTIT n_obs\n  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1     3      1        0     0.084     5\n2     8      0        0     0.378     5\n3     9      0        0     0.75      5\n4    15      0        0     0.664     5\n5    33      1        0     0.41      4\n6    45      1        0     0.382     5\n\n\nAs shown above, you can include level 2 variables in your group_by() command to ensure they get carried through to the aggregated results. Neat trick.\nAnyway, we then plot:\n\nggplot( aggdat, aes(avg.ATTIT) ) +\n  geom_histogram( binwidth = 0.05 ) +\n  labs(main = \"Average ATTIT across students\", \n       xlab = \"\" )\n\n\n\n\n\n\n\n\nWe can facet to see multiple groups:\n\nggplot( aggdat, aes(avg.ATTIT) ) +\n  facet_grid( FEMALE ~ MINORITY, labeller = label_both ) +\n  geom_histogram( binwidth = 0.05 ) +\n  labs(main = \"Average ATTIT across students\", \n       xlab = \"\" )",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summarizing and exploring data</span>"
    ]
  },
  {
    "objectID": "making_tables.html",
    "href": "making_tables.html",
    "title": "6  Making tables in Markdown",
    "section": "",
    "text": "6.1 Making a “table one”\nThe “table one” is the first table in a lot of papers that show general means of different variables for different groups. Perhaps not surprisingly, the tableone package is useful for making such tables:\nlibrary(tableone)\n\n# sample mean  \nCreateTableOne(data = dat,\n               vars = c(\"G\", \"Z\", \"X\"))\n\n               \n                Overall     \n  n              100        \n  G (%)                     \n     A            24 (24.0) \n     B            16 (16.0) \n     C            14 (14.0) \n     D            23 (23.0) \n     E            23 (23.0) \n  Z = tx (%)      55 (55.0) \n  X (mean (SD)) 0.02 (0.95) \n\n# you can also stratify by a variables of interest\ntb &lt;- CreateTableOne(data = dat,\n                     vars = c(\"X\", \"G\", \"Y\"), \n                     strata = c(\"Z\"))\ntb\n\n               Stratified by Z\n                co           tx            p      test\n  n               45            55                    \n  X (mean (SD)) 0.21 (0.94)  -0.14 (0.93)   0.065     \n  G (%)                                     0.995     \n     A            11 (24.4)     13 (23.6)             \n     B             7 (15.6)      9 (16.4)             \n     C             7 (15.6)      7 (12.7)             \n     D            10 (22.2)     13 (23.6)             \n     E            10 (22.2)     13 (23.6)             \n  Y (mean (SD)) 0.00 (1.02)   0.19 (1.04)   0.365\nYou can then use kable on your table as so:\nprint(tb$ContTable, printToggle = FALSE) %&gt;%\n    knitr::kable()\n\n\n\n\n\nco\ntx\np\ntest\n\n\n\n\nn\n45\n55\n\n\n\n\nX (mean (SD))\n0.21 (0.94)\n-0.14 (0.93)\n0.065\n\n\n\nY (mean (SD))\n0.00 (1.02)\n0.19 (1.04)\n0.365",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Making tables in Markdown</span>"
    ]
  },
  {
    "objectID": "making_tables.html#the-stargazer-package",
    "href": "making_tables.html#the-stargazer-package",
    "title": "6  Making tables in Markdown",
    "section": "6.2 The stargazer package",
    "text": "6.2 The stargazer package\nYou can easily make pretty tables using the stargazer package. You need to ensure the data is a data.frame, not tibble, because stargazer is old school. It appears to only do continuous variables. Stargazer is probably best known for making regression tables (see next chapter), but it can make other kinds of tables as well, such as data summaries.\nWhen using stargazer to summarize a dataset, you can specify that it should include only some of the variables and you can omit stats that are not of interest:\n\n# to include only variables of interest\nstargazer(as.data.frame(dat), header=FALSE, \n          omit.summary.stat = c(\"p25\", \"p75\", \"min\", \"max\"), \n          # to omit percentiles\n          title = \"Table 1: Descriptive statistics\",\n          type = \"text\")\n\n\nTable 1: Descriptive statistics\n============================\nStatistic  N  Mean  St. Dev.\n----------------------------\nX         100 0.018  0.948  \nY         100 0.101  1.027  \n----------------------------\n\n\nSee the stargazer help file for how to set/change more of the options: https://cran.r-project.org/web/packages/stargazer/stargazer.pdf\nWarning: stargazer does not work well with tibbles (the data frames you get from tidyverse commands), so you need to convert your data to a data.frame before using it. In particular, you have to “cast” your data to a data.frame to make it work:\n\n  library(stargazer)\n  \n  # to include all variables\n  stargazer( as.data.frame(dat), header = FALSE, type=\"text\")\n\nTo use stargazer in a PDF or HTML report, you will want the report to format the table so it doesn’t look like raw output. To do so, you would not set type=\"text\" but rather type=\"latex\" or type=\"html\", and then in the markdown chunk header (the thing that encloses all your R code) you would say “results=‘asis’” in your code chunk header like so:\nThis will ensure the output of stargazer gets formatted properly in your R Markdown.\nUnfortunately, it is hard to dynamically make a report that can render to either html or a pdf, so you will have to choose one or the other. If you are making a PDF, you will want to use type=\"latex\" and if you are making an HTML report, you will want to use type=\"html\".",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Making tables in Markdown</span>"
    ]
  },
  {
    "objectID": "making_tables.html#the-xtable-package",
    "href": "making_tables.html#the-xtable-package",
    "title": "6  Making tables in Markdown",
    "section": "6.3 The xtable package",
    "text": "6.3 The xtable package\nThe xtable package is another great package for making tables. It is particularly good for LaTeX documents. It is a bit more complicated to use than stargazer, but it is very powerful. Here is an example of how to use it:\n\nlibrary(xtable)\nxtable(sdat, caption = \"A table of fake data\" )\n\nHere you would again use the “results=‘asis’” in the chunk header to get the table to render properly in your R Markdown document.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Making tables in Markdown</span>"
    ]
  },
  {
    "objectID": "reg_table_demo.html",
    "href": "reg_table_demo.html",
    "title": "7  Making Regression and ANOVA Tables",
    "section": "",
    "text": "7.1 The basics of regression tables\nFor the basics we quickly illustrate regression tables using a subset of the Making Caring Common dataset, which we will eventually discuss in class. This dataset has a measure of emotional safety (our outcome) and we want to see, in a specific school, if this is predicted by gender and/or grade.\nOur data look like this:\nsample_n( sch1, 6 )\n\n  ID esafe grade gender     disc race_white\n1  1     4     8   Male 1.111111          1\n2  1     4     6 Female 1.000000          1\n3  1     4     6 Female 1.000000          1\n4  1     4     8 Female 1.000000          1\n5  1     4     5 Female 1.000000          1\n6  1     4     8   Male 1.888889          0\nWe fit some models:\nM_A = lm( esafe ~ grade, data = sch1 )\nM_B = lm( esafe ~ grade + gender, data = sch1 )\nM_C = lm( esafe ~ grade * gender, data = sch1 )\nOk, we have fit our regression models. We can look at big complex printout of a single model like so:\nsummary( M_C )\n\n\nCall:\nlm(formula = esafe ~ grade * gender, data = sch1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7894 -0.1570  0.1550  0.2662  0.4938 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.12733    0.37946  10.877   &lt;2e-16 ***\ngrade            -0.07764    0.05859  -1.325   0.1879    \ngenderMale       -0.72735    0.49762  -1.462   0.1467    \ngrade:genderMale  0.13327    0.07627   1.747   0.0834 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4333 on 108 degrees of freedom\nMultiple R-squared:  0.04914,   Adjusted R-squared:  0.02273 \nF-statistic: 1.861 on 3 and 108 DF,  p-value: 0.1406\nOr we can make regression tables. Consider these two packages, the first being texreg\nlibrary( texreg )\nscreenreg(list(M_A, M_B, M_C))\n\n\n====================================================\n                  Model 1     Model 2     Model 3   \n----------------------------------------------------\n(Intercept)         3.68 ***    3.62 ***    4.13 ***\n                   (0.25)      (0.25)      (0.38)   \ngrade               0.00        0.00       -0.08    \n                   (0.04)      (0.04)      (0.06)   \ngenderMale                      0.13       -0.73    \n                               (0.08)      (0.50)   \ngrade:genderMale                            0.13    \n                                           (0.08)   \n----------------------------------------------------\nR^2                 0.00        0.02        0.05    \nAdj. R^2           -0.01        0.00        0.02    \nNum. obs.         112         112         112       \n====================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\nAnother is stargazer.\nlibrary( stargazer )\nstargazer( M_A, M_B, M_C, header=FALSE, type='text')\n\n\n===============================================================================\n                                        Dependent variable:                    \n                    -----------------------------------------------------------\n                                               esafe                           \n                            (1)                 (2)                 (3)        \n-------------------------------------------------------------------------------\ngrade                      0.004               0.001              -0.078       \n                          (0.038)             (0.038)             (0.059)      \n                                                                               \ngenderMale                                     0.130              -0.727       \n                                              (0.083)             (0.498)      \n                                                                               \ngrade:genderMale                                                  0.133*       \n                                                                  (0.076)      \n                                                                               \nConstant                 3.676***            3.624***            4.127***      \n                          (0.249)             (0.250)             (0.379)      \n                                                                               \n-------------------------------------------------------------------------------\nObservations                112                 112                 112        \nR2                        0.0001               0.022               0.049       \nAdjusted R2               -0.009               0.004               0.023       \nResidual Std. Error  0.440 (df = 110)    0.437 (df = 109)    0.433 (df = 108)  \nF Statistic         0.009 (df = 1; 110) 1.241 (df = 2; 109) 1.861 (df = 3; 108)\n===============================================================================\nNote:                                               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Regression and ANOVA Tables</span>"
    ]
  },
  {
    "objectID": "reg_table_demo.html#extending-to-the-multilevel-model",
    "href": "reg_table_demo.html#extending-to-the-multilevel-model",
    "title": "7  Making Regression and ANOVA Tables",
    "section": "7.2 Extending to the multilevel model",
    "text": "7.2 Extending to the multilevel model\nFor our multilevel examples, we use the Making Caring Common data from Project A, and fit data to the 8th grade students only, but do it for all schools. We have made a High School dummy variable.\nOur two models we use for demo purposes have a HS term and no HS term:\n\nmodA &lt;- lmer( esafe ~ 1 + (1 | ID), data=dat.g8)\nmodB &lt;- lmer( esafe ~ 1 + HS + (1 | ID), data=dat.g8)\n\nIn the next sections we first show how to get better summary output (according to some folks) and then we walk through making regression tables in a bit more detail than above.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Regression and ANOVA Tables</span>"
    ]
  },
  {
    "objectID": "reg_table_demo.html#getting-p-values-for-lmer-output",
    "href": "reg_table_demo.html#getting-p-values-for-lmer-output",
    "title": "7  Making Regression and ANOVA Tables",
    "section": "7.3 Getting p-values for lmer output",
    "text": "7.3 Getting p-values for lmer output\nThe lmerTest package is a way of making R give you more complete output. We are going to load it, and then put the new lmer models into new variables so we can see how the different model fitting packages work with the regression table packages below.\n\nlibrary( lmerTest )\nmodB.T &lt;- lmer( esafe ~ 1 + HS + (1 | ID), data=dat.g8)\nmodA.T &lt;- lmer( esafe ~ 1 + (1 | ID), data=dat.g8)\n\nsummary( modB.T )\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: esafe ~ 1 + HS + (1 | ID)\n   Data: dat.g8\n\nREML criterion at convergence: 2746.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3883 -0.6156  0.2021  0.7628  1.7331 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.04809  0.2193  \n Residual             0.46459  0.6816  \nNumber of obs: 1305, groups:  ID, 26\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)  3.52798    0.08637 29.91033  40.846   &lt;2e-16 ***\nHSTRUE      -0.29480    0.10787 25.77814  -2.733   0.0112 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nHSTRUE -0.801",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Regression and ANOVA Tables</span>"
    ]
  },
  {
    "objectID": "reg_table_demo.html#the-texreg-package",
    "href": "reg_table_demo.html#the-texreg-package",
    "title": "7  Making Regression and ANOVA Tables",
    "section": "7.4 The texreg package",
    "text": "7.4 The texreg package\nIn texreg there are two primary functions for table making, one is screenreg() and the other is texreg().\n\n7.4.1 Using screenreg()\nScreenreg is fine for MLMs. It looks a bit like raw output, but it is clear and clean. It will take models fit using lmer or lmerTest, no problem.\n\nscreenreg(list(modA,modB))\n\n\n===============================================\n                     Model 1       Model 2     \n-----------------------------------------------\n(Intercept)              3.35 ***      3.53 ***\n                        (0.06)        (0.09)   \nHSTRUE                                -0.29 ** \n                                      (0.11)   \n-----------------------------------------------\nAIC                   2756.78       2754.79    \nBIC                   2772.30       2775.49    \nLog Likelihood       -1375.39      -1373.40    \nNum. obs.             1305          1305       \nNum. groups: ID         26            26       \nVar: ID (Intercept)      0.07          0.05    \nVar: Residual            0.46          0.46    \n===============================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nComment: Note that the number of stars are different for the display vs the summary output! (Look at the HS coefficient for example.) Not good, it would seem.\nThis is because the \\(p\\)-values are calculated using the normal approximation by the screenreg command, and using the \\(t\\)-test with approximate degrees of freedom by lmerTest. This makes a difference. Consider the following, using the \\(t\\) statistics for the HS variable:\n\n2 * pt( -2.733, df=25.77814 )\n\n[1] 0.0111831\n\n2 * pnorm( -2.733 )\n\n[1] 0.006276033\n\n\nOne is below 0.01, and one is not. An extra star!\n\n\n7.4.2 Using texreg() and TeX\nThe texreg command is part of the texreg package and can be integrated with latex (which you would need to install). Once you do this, when you compile to a pdf, all is well. In the R code chunk you need to include results=\"asis\" to get the latex to compile right. E.g., “r, results=\"asis\"” when you declare a code chunk.\ntexreg(list(modA,modB), table=FALSE)\nNote that the table=FALSE puts the table right where you want it, not at some random spot latex things is nice. Latex likes to have “floating tables,” where it puts the table where there is space; this makes it easier to make the entire formatted page look nice.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Regression and ANOVA Tables</span>"
    ]
  },
  {
    "objectID": "reg_table_demo.html#the-stargazer-package",
    "href": "reg_table_demo.html#the-stargazer-package",
    "title": "7  Making Regression and ANOVA Tables",
    "section": "7.5 The stargazer package",
    "text": "7.5 The stargazer package\nlibrary( stargazer )\nstargazer(modA, modB, header=FALSE, type='latex')\nOne issue is stargazer does not include the random effect variances, so the output is quite limited for multilevel modeling. It also has less stringent conditions for when to put down stars. One star is below 0.10, two is below 0.05, and three is below 0.01. This is quite generous. Also it is using the normal approximation.\n\n7.5.1 Stargazer with lmerTest\nStargazer with lmerTest is a bit fussy. This shows how to make it work if you have loaded the lmerTest package. Recall the lmerTest package makes your lmer commands have p-values and whatnot. But this means your new lmer() command is not quite the same as the old—and stargazer is expecting the old. You gix this by lying to R, telling it the new thing is the old thing. This basically works.\nNow for stargazer, we need to tell it that our models are the right type. First note:\n\nclass( modB )\n\n[1] \"lmerMod\"\nattr(,\"package\")\n[1] \"lme4\"\n\nclass( modB.T)\n\n[1] \"lmerModLmerTest\"\nattr(,\"package\")\n[1] \"lmerTest\"\n\n\nSo we fix as follows:\nlibrary( stargazer )\nclass( modB.T ) = \"lmerMod\" \nclass( modA.T ) = \"lmerMod\" \nstargazer(modA.T, modB.T, header=FALSE, type='latex' )\n\n\n7.5.2 The sjPlot package\nOne function, tab_model from sjPlot, makes nice regression tables:\n\n# tabulate the results of our two tip models\nlibrary( sjPlot )\ntab_model(modA.T, modB.T)\n\n\n\n \nesafe\nesafe\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n3.35\n3.23 – 3.46\n&lt;0.001\n3.53\n3.36 – 3.70\n&lt;0.001\n\n\nHSTRUE\n\n\n\n-0.29\n-0.51 – -0.08\n0.006\n\n\nRandom Effects\n\n\n\nσ2\n0.46\n0.46\n\n\n\nτ00\n0.07 ID\n0.05 ID\n\n\nICC\n0.12\n0.09\n\n\nN\n26 ID\n26 ID\n\nObservations\n1305\n1305\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.125\n0.028 / 0.119",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Regression and ANOVA Tables</span>"
    ]
  },
  {
    "objectID": "reg_table_demo.html#pretty-anova-liklihood-ratio-test-tables",
    "href": "reg_table_demo.html#pretty-anova-liklihood-ratio-test-tables",
    "title": "7  Making Regression and ANOVA Tables",
    "section": "7.6 Pretty ANOVA (liklihood ratio test) Tables",
    "text": "7.6 Pretty ANOVA (liklihood ratio test) Tables\nWe now turn to how to give a nice display of likelihood ratio test results using the kable function. To show how to make such tables, we first create a fake illustrative data set called a that has 100 observations and specifies our outcome Y as a funciton of two uncorrelated variables A and B\n\na &lt;- tibble( A = rnorm( 100 ),\n            B = rnorm( 100 ),\n            Y = A * 0.2 + B * 0.5 + rnorm( 100, 0, 1 ) )\n\n\n7.6.1 Run the Models\nWe fit two models, one with A and B, the other with just A.\n\nM1 &lt;- lm( Y~ A + B, data = a )\nM2 &lt;- lm( Y ~ A, data = a )\n\n\n\n7.6.2 Comparing the Models\nWe use the anova function to compare the two models (see also the chapter on Likelihood Ratio tests). We see that B improves the model fit significantly.\n\naa = anova( M2, M1 )\naa\n\nAnalysis of Variance Table\n\nModel 1: Y ~ A\nModel 2: Y ~ A + B\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     98 130.54                                 \n2     97 109.03  1    21.503 19.13 3.074e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naa |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\np.value\n\n\n\n\nY ~ A\n98\n130.5386\nNA\nNA\nNA\nNA\n\n\nY ~ A + B\n97\n109.0352\n1\n21.50338\n19.12986\n3.07e-05\n\n\n\n\n\n\n\n7.6.3 Compare to the Significance test on B\nNote that the p value for B is identical to the ANOVA results above. Why bother with ANOVA? It can test more complex hypotheses as well (multiple coefficients, random effects, etc.)\n\nM1 |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.1331894\n0.1063276\n1.252633\n0.2133507\n\n\nA\n0.2320947\n0.1037421\n2.237229\n0.0275605\n\n\nB\n0.4375799\n0.1000464\n4.373769\n0.0000307\n\n\n\n\n\n\n\n7.6.4 Acknowledgements\nThe ANOVA portion of this chapter was initially drafted by Josh Gilbert",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Regression and ANOVA Tables</span>"
    ]
  },
  {
    "objectID": "diagnostic_plots.html",
    "href": "diagnostic_plots.html",
    "title": "8  Regression diagnostic plots for MLMs",
    "section": "",
    "text": "In this chapter we outline a few simple checks you might conduct on a fitted random effects model to check for extreme outliers and whatnot.\nfirst, let’s fit a random intercept model to our High School & Beyond data:\n\nm1 &lt;- lmer(mathach ~ 1 + ses + (1|schoolid), data=dat)\narm::display(m1)\n\nlmer(formula = mathach ~ 1 + ses + (1 | schoolid), data = dat)\n            coef.est coef.se\n(Intercept) 12.66     0.19  \nses          2.39     0.11  \n\nError terms:\n Groups   Name        Std.Dev.\n schoolid (Intercept) 2.18    \n Residual             6.09    \n---\nnumber of obs: 7185, groups: schoolid, 160\nAIC = 46653.2, DIC = 46637\ndeviance = 46641.0 \n\n\nWe can check if some of our assumptions are being grossly violated, i.e. residuals at all levels are normally distributed.\n\n  qplot(ranef(m1)$schoolid[,1],\n       main = \"Histogram of random intercepts\", xlab=\"\")\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n  qplot(resid(m1), \n       main = \"Hisogram of residuals\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can check for heteroskedasticity by plotting residuals against predicted values\n\n  dat$yhat  = predict(m1)\n  dat$resid = resid(m1)\n  \n  ggplot(dat, aes(yhat, resid)) + \n      geom_point(alpha=0.3) + \n      geom_smooth() + \n      labs(title = \"Residuals against predicted values\",\n           x = \"Predicted values\", y =\"Residuals\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nIt looks reasonable (up to the discrete and bounded nature of our data). No major weird curves in the loess line through the residuals means linearity is a reasonable assumption. That being said, our nominal SEs around our loess line are tight, so the mild curve is probably evidence of some model misfit.\nWe can also look at the distribution of random effects using the lattice package\n\n  library(lattice)\n  qqmath(ranef(m1, condVar=TRUE), strip=FALSE)\n\n$schoolid",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression diagnostic plots for MLMs</span>"
    ]
  },
  {
    "objectID": "math_reference.html",
    "href": "math_reference.html",
    "title": "9  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "",
    "text": "9.1 Introduction\nThis document has a bunch of mathematical equations we use in the class. It is a good reference for how to write your own math equations in your life moving forward. Generally, people write math equations using something called Latex. Latex (or Tex) is a way of writing documents where mixed in with the writing of what you want to say are commands (editorial markup, if you will) describing how you want your document to look. This is a very powerful thing: there are Tex editors that allow you to write entire articles, books, reports, poetry, or whatever with extreme control over the typesetting used. It creates beautifully typeset documents that are easy to distinguish from those written in, say, MS Word due to how they adjust whitespace on the page. That being said, it can be a lot to jump in to.\nEnter R Markdown. R Markdown is a useful and easy way to take advantage of this syntax without the overhead of writing entire documents in Latex, even if you don’t have any R code in your document. Inside R Markdown you can write math equations, and then when you render the report, it not only runs all the R code, but it formats all the math for you as well! You can even have R Markdown render to MS Word to give you a word doc with all your math equations ready to go.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Math Reference: Sample Modeling Equations to Borrow</span>"
    ]
  },
  {
    "objectID": "math_reference.html#introduction",
    "href": "math_reference.html#introduction",
    "title": "9  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "",
    "text": "9.1.1 Using this document\nYou are probably reading the PDF version of this document. But really you should open the .Rmd file that generated this document, so you can cut and paste the relevant equations into your own work, and then modify as necessary. The link to this file here.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Math Reference: Sample Modeling Equations to Borrow</span>"
    ]
  },
  {
    "objectID": "math_reference.html#overview-of-using-latex",
    "href": "math_reference.html#overview-of-using-latex",
    "title": "9  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "9.2 Overview of Using Latex",
    "text": "9.2 Overview of Using Latex\nFor math in your writing, you denote the beginning and the end of a math equation in your text using “$”s—one at the start and one at the stop. E.g., “$ math stuff $”. Most greek letters are written as their names with a backslash “\\” just before it. E.g., “\\alpha”.\nSo if I want to write an alpha, I write “$\\alpha$” and get \\(\\alpha\\).\nI can do subscripts by using an underscore. E.g., “$\\alpha_j$” gives \\(\\alpha_j\\). I can also do superscripts by using a hat. E.g., “$\\alpha^2$” gives \\(\\alpha^2\\). To put more than one character in a subscript (or superscript), put the stuff to be subscripted in curly braces, e.g., “$\\alpha_{ij}$” gives \\(\\alpha_{ij}\\).\n\n\n9.2.1 Some useful greek letters\nHere are some useful greek letters and symbols\n\n\n\nLetter\nName\n\n\n\n\n\\(\\alpha\\)\n\\alpha\n\n\n\\(\\beta\\)\n\\beta\n\n\n\\(\\delta, \\Delta\\)\n\\delta, \\Delta\n\n\n\\(\\epsilon\\)\n\\epsilon\n\n\n\\(\\sigma, \\Sigma\\)\n\\sigma, \\Sigma\n\n\n\\(\\rho\\)\n\\rho\n\n\n\\(\\mu\\)\n\\mu (Meew!)\n\n\n\\(\\tau\\)\n\\tau\n\n\n\\(\\times\\)\n\\times\n\n\n\\(\\sim\\)\n\\sim\n\n\n\nSee many more symbols at, e.g., https://www.caam.rice.edu/~heinken/latex/symbols.pdf. This was found by searching “tex symbols” on Google.\n\n\n9.2.2 Equations on lines by themselves\nTo write an equation on a line by itself, put the math stuff in between a pair of double “$”. E.g., if we write:\n$$ Y = a X + b $$\nWe get \\[ Y = a X + b .\\]\nIf we want multiple lines, we have to put our equation between a \\begin{aligned} and \\end{aligned} command and use a double backslash (“\\\\”) to denote each line break (even if we have a line break we have to do this—we have to explicitly tell the program converting our raw text to nice formatted text where the line breaks are). Finally, inside the begin-end block of math, line things up with & symbols on each row of our equation. The & symbols will be lined up vertically.\nSo if we write\n$$\n\\begin{aligned}\nY &= 10 X + 2 \\\\\nY - 5 &= 3 X^2 + 5\n\\end{aligned}\n$$\nwe get \\[\n\\begin{aligned}\nY &= 10 X + 2 \\\\\nY - 5 &= 3 X^2 + 5\n\\end{aligned}\n\\] Also consider:\n$$\n\\begin{aligned}\na + b + c + d &= c \\\\\n d &= e + f + g + h \n\\end{aligned}\n$$\ngiving \\[\n\\begin{aligned}\na + b + c + d &= c \\\\\nd &= e + f + g + h\n\\end{aligned}\n\\]\n\n\n9.2.3 Normal text in equations\nIf you put words in your equations, they get all italliced and weird, without their spaces:\n$$\n5 + my dog = 10\n$$\n\\[\n5 + my dog = 10\n\\]\nYou can fix using the “\\mbox{}” command as so:\n$$\n5 + \\mbox{my dog} = 10\n$$\n\\[\n5 + \\mbox{my dog} = 10\n\\]\nWe next walk through some latex code for the models you will most see.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Math Reference: Sample Modeling Equations to Borrow</span>"
    ]
  },
  {
    "objectID": "math_reference.html#sample-code-random-intercept-model",
    "href": "math_reference.html#sample-code-random-intercept-model",
    "title": "9  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "9.3 Sample code: Random Intercept Model",
    "text": "9.3 Sample code: Random Intercept Model\nOur canonical Random Intercept model is as follows. First, our Level 1 model:\n$$\n\\begin{aligned}\ny_{ij} &= \\alpha_{j} + \\beta_{1} ses_{ij} +  \\epsilon_{ij} \\\\\n\\epsilon_{ij} &\\sim N( 0, \\sigma^2_y ) \\\\\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_{ij} &= \\alpha_{j} + \\beta_{1} ses_{ij} +  \\epsilon_{ij} \\\\\n\\epsilon_{ij} &\\sim N( 0, \\sigma^2_y ) \\\\\n\\end{aligned}\n\\]\nOur Level 2 model:\n$$\n\\begin{aligned}\n\\alpha_{j} &= \\gamma_{0} + \\gamma_{1} sector_j + u_{j} \\\\\nu_{j} &\\sim N( 0, \\sigma^2_\\alpha ) \\\\\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\n\\alpha_{j} &= \\gamma_{0} + \\gamma_{1} sector_j + u_{j} \\\\\nu_{j} &\\sim N( 0, \\sigma^2_\\alpha ) \\\\\n\\end{aligned}\n\\]\nThe Gelman and Hill bracket notation looks like this:\n$$\n\\begin{aligned}\ny_{i} &= \\alpha_{j[i]} + \\beta_{1} ses_{i} +  \\epsilon_{i} \\\\\n\\epsilon_i &\\sim N( 0, \\sigma^2_y ) \\\\\n\\alpha_{j} &= \\gamma_{0} + \\gamma_{1} sector_j + u_{j} \\\\\nu_{j} &\\sim N( 0, \\sigma^2_\\alpha ) \\\\\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_{i} &= \\alpha_{j[i]} + \\beta_{1} ses_{i} +  \\epsilon_{i} \\\\\n\\epsilon_i &\\sim N( 0, \\sigma^2_y ) \\\\\n\\alpha_{j} &= \\gamma_{0} + \\gamma_{1} sector_j + u_{j} \\\\\nu_{j} &\\sim N( 0, \\sigma^2_\\alpha ) \\\\\n\\end{aligned}\n\\]\nThe reduced form would look like this:\n$$\ny_{i} = \\gamma_{0} + \\gamma_{1} sector_{j[i]} + \\beta_{1} ses_{i} + u_{j[i]} + \\epsilon_{i}\n$$\n\\[\ny_{i} = \\gamma_{0} + \\gamma_{1} sector_{j[i]} + \\beta_{1} ses_{i} + u_{j[i]} + \\epsilon_{i}\n\\]\nwith\n$$\n\\epsilon_i \\sim N( 0, \\sigma^2_y ), \\mbox{ and } u_{j} \\sim N( 0, \\sigma^2_\\alpha )\n$$\n\\[\n\\epsilon_i \\sim N( 0, \\sigma^2_y ), \\mbox{ and } u_{j} \\sim N( 0, \\sigma^2_\\alpha )\n\\]\nIf we want to be really prissy, we can write down the i.i.d. aspect of our random effects like this\n$$\n\\epsilon_i \\stackrel{i.i.d}{\\sim} N( 0, \\sigma^2_y ), \n\\mbox{ and } u_{j} \\stackrel{i.i.d}{\\sim} N( 0, \\sigma^2_\\alpha )\n$$\n\\[\n\\epsilon_i \\stackrel{i.i.d}{\\sim} N( 0, \\sigma^2_y ), \\\\\n\\mbox{ and } u_{j} \\stackrel{i.i.d}{\\sim} N( 0, \\sigma^2_\\alpha )\n\\] The \\stackrel{}{} command takes two bits of latex, each in the curly braces, and stacks them on top of each other.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Math Reference: Sample Modeling Equations to Borrow</span>"
    ]
  },
  {
    "objectID": "math_reference.html#sample-code-random-slope-model",
    "href": "math_reference.html#sample-code-random-slope-model",
    "title": "9  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "9.4 Sample code: Random Slope Model",
    "text": "9.4 Sample code: Random Slope Model\nThe canonical random slope model for HS&B with ses at level 1 and sector at level 2 involves a matrix for the pair of random effects. We have to get a bit fancier with our TeX, therefore!\nLevel 1 models:\n$$\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} +  \\epsilon_{ij} \\\\\n\\epsilon_{ij} &\\sim N( 0, \\sigma^2_y ) \\\\\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} +  \\epsilon_{ij} \\\\\n\\epsilon_{ij} &\\sim N( 0, \\sigma^2_y ) \\\\\n\\end{aligned}\n\\]\nLevel 2 models:\n$$\n\\begin{aligned}\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} sector_j + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} sector_j + u_{1j} \n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} sector_j + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} sector_j + u_{1j}\n\\end{aligned}\n\\]\nwith\n$$\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{01}\\\\\n & \\tau_{11} \\\\\n\\end{pmatrix}\n\\end{bmatrix}\n$$\n\\[\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{01}\\\\\n& \\tau_{11} \\\\\n\\end{pmatrix}\n\\end{bmatrix}\n\\]\nThe TeX for the derivation of the reduced form is:\n$$\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} + \\epsilon_{ij}\\\\\n&= \\left( \\gamma_{00} + \\gamma_{01} sector_j + u_{0j} \\right)+ \\\\\\\n  (\\gamma_{10} + \\gamma_{11} sector_j + u_{1j}) ses_{ij} +  \\epsilon_{ij} \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j + u_{0j}  + \\gamma_{10}ses_{ij} + \\\\\n  \\gamma_{11} sector_j ses_{ij} + u_{1j} ses_{ij} +  \\epsilon_{ij}  \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j +  \\gamma_{10}ses_{ij} + \\\\\n  \\gamma_{11} sector_j ses_{ij} + \\left(u_{0j} + u_{1j} ses_{ij} + \\epsilon_{ij} \\right) \n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} + \\epsilon_{ij}\\\\\n&= \\left( \\gamma_{00} + \\gamma_{01} sector_j + u_{0j} \\right)+ (\\gamma_{10} + \\gamma_{11} sector_j + u_{1j}) ses_{ij} +  \\epsilon_{ij} \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j + u_{0j}  + \\gamma_{10}ses_{ij} + \\gamma_{11} sector_j ses_{ij} + u_{1j} ses_{ij} +  \\epsilon_{ij}  \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j +  \\gamma_{10}ses_{ij} + \\gamma_{11} sector_j ses_{ij} + \\left(u_{0j} + u_{1j} ses_{ij} + \\epsilon_{ij} \\right)\n\\end{aligned}\n\\]\nCommentary: There are various and competing ways of writing the covariance matrix for the random effects. The \\(\\tau_{**}\\) notation is easy and expands to any sized matrix (if we, for example, have more than one random slope). But all the \\(\\tau_{**}\\) are variances, not standard deviations, and we often like to talk about random effect variation in terms of standard deviations. We can thus use something like this instead:\n$$\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\sigma^2_{0} & \\rho \\sigma_0 \\sigma_1 \\\\\n & \\sigma^2_{1} \\\\\n\\end{pmatrix}\n\\end{bmatrix}\n$$\n\\[\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\sigma^2_{0} & \\rho \\sigma_0 \\sigma_1 \\\\\n& \\sigma^2_{1} \\\\\n\\end{pmatrix}\n\\end{bmatrix}\n\\]\nHere we have a correlation of random effects, \\(\\rho\\), instead of a covariance, \\(\\tau_{01}\\). And we can talk about the standard deviation of, e.g., the random intercepts, as \\(\\sigma_0\\) rather than \\(\\sqrt{ \\tau_{00} }\\). Different ways of writing the same mathematical thing are called different parameterizations; they are equivalent, but are more or less clear for different contexts.\nUnfortunately, this means there is no one right answer for how to write down a mathematical equation!",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Math Reference: Sample Modeling Equations to Borrow</span>"
    ]
  },
  {
    "objectID": "math_reference.html#summations-and-fancy-stuff",
    "href": "math_reference.html#summations-and-fancy-stuff",
    "title": "9  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "9.5 Summations and fancy stuff",
    "text": "9.5 Summations and fancy stuff\nFractions are as follows:\n$$\ncor( A, B ) = \\frac{ cov( A, B ) }{ \\sigma_A \\sigma_B }\n$$\n\\[\ncor( A, B ) = \\frac{ cov( A, B ) }{ \\sigma_A \\sigma_B }\n\\]\nFor reference, you can do summations and whatnot as follows:\n$$\nVar( Y_{i} ) = \\frac{1}{n-1} \\sum_{i=1}^n \\left( Y_{i} - \\bar{Y} \\right)^2 \n$$\n\\[\nVar( Y_{i} ) = \\frac{1}{n-1} \\sum_{i=1}^n \\left( Y_{i} - \\bar{Y} \\right)^2\n\\]\nAnd if you have fractions you can have big brackets with “\\left(” and “\\right)” as follows:\n$$\nX = \\left( \\frac{1}{2} + y \\right)\n$$\n\\[\nX = \\left( \\frac{1}{2} + y \\right)\n\\]\nAnnoyingly, you always need a pair of these big brackets. If you really don’t want one, you use a backslash and a dot, like so:\n$$\nX = \\left( \\frac{1}{2} + y \\right.\n$$\n\\[\nX = \\left( \\frac{1}{2} + y \\right.\n\\]\nThe rest you can find on StackOverflow or similar. Or perhaps have ChatGPT help you write your code!",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>A Math Reference: Sample Modeling Equations to Borrow</span>"
    ]
  },
  {
    "objectID": "pivot_wider.html",
    "href": "pivot_wider.html",
    "title": "10  pivot_longer and pivot_wider",
    "section": "",
    "text": "10.1 Converting wide data to long data\nWe use pivot_longer to take our Y values and nest them within each ID for longitudinal MLM analysis. (NB you can use SEM to fit longitudinal models with wide data; we do not explore that application here.)\ndatL &lt;- pivot_longer(dat, Y1:Y3, \n                     names_to = \"time\", \n                     values_to = \"front\" )\n\ndatL\n\n# A tibble: 9 × 4\n     ID     X time  front\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1    10 Y1        1\n2     1    10 Y2       11\n3     1    10 Y3       21\n4     2    20 Y1        2\n5     2    20 Y2       12\n6     2    20 Y3       22\n7     3    30 Y1        3\n8     3    30 Y2       13\n9     3    30 Y3       23",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>`pivot_longer` and `pivot_wider`</span>"
    ]
  },
  {
    "objectID": "pivot_wider.html#converting-long-data-to-wide-data",
    "href": "pivot_wider.html#converting-long-data-to-wide-data",
    "title": "10  pivot_longer and pivot_wider",
    "section": "10.2 Converting long data to wide data",
    "text": "10.2 Converting long data to wide data\npivot_wider takes us back in the other direction.\n\nnewdat &lt;- pivot_wider( datL, c(ID, X), \n                       names_from=time, \n                       values_from=front  )\n\nnewdat\n\n# A tibble: 3 × 5\n     ID     X    Y1    Y2    Y3\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    10     1    11    21\n2     2    20     2    12    22\n3     3    30     3    13    23\n\n\nWe then verify our work with a few checks.\n\nstopifnot( length( unique( newdat$ID ) ) == nrow( newdat ) )\n\nstudents = datL %&gt;% dplyr::select( ID, X ) %&gt;%\n    unique()\nstudents\n\n# A tibble: 3 × 2\n     ID     X\n  &lt;int&gt; &lt;dbl&gt;\n1     1    10\n2     2    20\n3     3    30\n\nstudents = merge( students, newdat, by=\"ID\" )",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>`pivot_longer` and `pivot_wider`</span>"
    ]
  },
  {
    "objectID": "pivot_wider.html#optional-wrangling-data-with-reshape",
    "href": "pivot_wider.html#optional-wrangling-data-with-reshape",
    "title": "10  pivot_longer and pivot_wider",
    "section": "10.3 Optional: wrangling data with reshape",
    "text": "10.3 Optional: wrangling data with reshape\nThe reshape() command is the old-school way of doing things, and it is harder to use but also can be more powerful in some ways (alternatively, there is a long literature on doing fancy stuff with the pivot methods as well). This section is entirely optional and possibly no longer useful.\nAnyway, say you have data in a form where a row has a value for a variable for several different points in time. The following code turns it into a data.frame where each row (case) is a value for the variable at that point in time. You also have an ID variable for which Country the GDP came from.\n\ndtw = read.csv( \"data/fake_country_block.csv\", as.is=TRUE )\ndtw\n\n  Country X1997 X1998 X1999 X2000 X2001 X2002 X2003 X2004\n1   China   0.5     1     2   3.4     4   5.3   6.0     7\n2 Morocco  31.9    32    33  34.0    NA  36.0  37.0    NA\n3 England  51.3    52    53  54.3    55  56.0  57.3    58\n\n\nHere we have three rows, but actually a lot of cases if we consider each time point a case. For trying it on your own, get the sample csv file ()[here]\nSee the website to get the sample csv file \\verb|fake_country_block.csv|.\nThe following our original data by making a case for each time point:\n\ndt = reshape( dtw, idvar=\"Country\", timevar=\"Year\", varying=2:9, sep=\"\", direction=\"long\" )\nhead(dt)\n\n             Country Year    X\nChina.1997     China 1997  0.5\nMorocco.1997 Morocco 1997 31.9\nEngland.1997 England 1997 51.3\nChina.1998     China 1998  1.0\nMorocco.1998 Morocco 1998 32.0\nEngland.1998 England 1998 52.0\n\n\nThings to notice: each case has a “row name” made out of the country and the Year. The “2:9” indicates a range of columns for the variable that is actually the same variable.\nR picked up that, for each of these columns, “X” is the name of the variable and the number is the time, and seperated them. You can set the name of your time variable, \\verb|timevar|, to whatever you want.\nThe above output is called “long format” and the prior is called “wide format.”\nYou can go in either direction. Here:\n\ndtn = reshape( dt, idvar=\"Country\", timevar=\"Year\" )\ndtn\n\n             Country X.1997 X.1998 X.1999 X.2000 X.2001 X.2002 X.2003 X.2004\nChina.1997     China    0.5      1      2    3.4      4    5.3    6.0      7\nMorocco.1997 Morocco   31.9     32     33   34.0     NA   36.0   37.0     NA\nEngland.1997 England   51.3     52     53   54.3     55   56.0   57.3     58\n\n\nYou can reshape on multiple variables. For example:\n\nexp.dat = data.frame( ID=c(\"a\",\"b\",\"c\",\"d\"), \n      cond = c(\"AI\",\"DI\",\"DI\",\"AI\"),\n            trial1 = c(\"E\",\"U\",\"U\",\"E\"),\n            dec1 = c(1,1,0,1),\n            trial2 = c(\"U\",\"E\",\"U\",\"E\"),\n            dec2 = c(0,0,0,1),\n                trial3 = c(\"U\",\"E\",\"E\",\"U\"),\n            dec3 = c(0,1,0,1),\n                trial4 = c(\"E\",\"U\",\"E\",\"U\"),\n            dec4 = c(0,1,0,0) )\nexp.dat\n\n  ID cond trial1 dec1 trial2 dec2 trial3 dec3 trial4 dec4\n1  a   AI      E    1      U    0      U    0      E    0\n2  b   DI      U    1      E    0      E    1      U    1\n3  c   DI      U    0      U    0      E    0      E    0\n4  d   AI      E    1      E    1      U    1      U    0\n\nrs = reshape( exp.dat,  idvar=\"ID\", \n        varying=c( 3:10 ), sep=\"\", direction=\"long\")            \nhead(rs)\n\n    ID cond time trial dec\na.1  a   AI    1     E   1\nb.1  b   DI    1     U   1\nc.1  c   DI    1     U   0\nd.1  d   AI    1     E   1\na.2  a   AI    2     U   0\nb.2  b   DI    2     E   0\n\n\nIt sorts out which variables are which. Note the names have to be exactly the same for any group of variables.\nOnce you have reshaped, you can look at things more easily (I use mosaic’s tally instead of the base table):\n\nmosaic::tally( trial ~ dec, data=rs )\n\n     dec\ntrial 0 1\n    E 4 4\n    U 5 3\n\n\nor\n\nmosaic::tally( trial~dec+cond, data=rs )\n\n, , cond = AI\n\n     dec\ntrial 0 1\n    E 1 3\n    U 3 1\n\n, , cond = DI\n\n     dec\ntrial 0 1\n    E 3 1\n    U 2 2",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>`pivot_longer` and `pivot_wider`</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html",
    "href": "intro_missing_data.html",
    "title": "11  An Introduction to Missing Data",
    "section": "",
    "text": "11.1 Introduction\nHandling missing data is the icky, unglamorous part of any statistical analysis. It is where the skeletons lie. There’s a range of options available, which are, broadly speaking:\nThe first two general approaches are imperfect, while the third is often more work than the original analysis that we were hoping to perform. For this course, doing a 2a, 2b, or 2c are all reasonable choices. If you have very little missing data you can often get away with 1. We have no expectations that people will take the plunge into #3 (multiple imputation). In real life, people will often analyze their data with a complete case analysis and some other strategy, and then compare the results. In Education, if missingness is below 10% people usually just do mean imputation, but regression imputation would probably be superior.\nThis handout provides an introduction to missing data, and includes a few commands to explore and deal with missing data. In this document we first talk about exploring missing data (in particular getting plots that show you if you have any notable patterns in how things are missing) and then we give a brief walk-through of the 3 methods listed above.\nWe will the mice and VIM packages, which you can install using install.packages() if you have not yet done so. These are simple and powerful packages for visualizing and imputing missing data. At the end of this document we also describe the Amelia package.\nlibrary(tidyverse)\nlibrary(mice)\nlibrary(VIM)\nThroughout we use a small built-in R dataset on air quality as a working example.\ndata(airquality)\n  nrow(airquality)\n\n[1] 153\n\n  head(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n  summary(airquality[1:4])\n\n     Ozone          Solar.R         Wind            Temp     \n Min.   :  1.0   Min.   :  7   Min.   : 1.70   Min.   :56.0  \n 1st Qu.: 18.0   1st Qu.:116   1st Qu.: 7.40   1st Qu.:72.0  \n Median : 31.5   Median :205   Median : 9.70   Median :79.0  \n Mean   : 42.1   Mean   :186   Mean   : 9.96   Mean   :77.9  \n 3rd Qu.: 63.2   3rd Qu.:259   3rd Qu.:11.50   3rd Qu.:85.0  \n Max.   :168.0   Max.   :334   Max.   :20.70   Max.   :97.0  \n NA's   :37      NA's   :7",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html#introduction",
    "href": "intro_missing_data.html#introduction",
    "title": "11  An Introduction to Missing Data",
    "section": "",
    "text": "Delete the observations with missing covariates (this is a “complete case analysis”)\nPlug in some kind of reasonable value for the missing covariate. This is called “imputation.” We discuss three ways of doing this that are increasingly sophisticated and layered on each other:\n\n\n\nMean imputation. Simply take the mean of all the observations where you know the value, and then use that for anything that is missing.\nRegression imputation. You generate regression equations describing how all the variables are connected, and use those to predict any missing value.\nStochastic regression imputation. Here we use regression imputation, but we also add some residual noise to all our imputed values so that our imputed values have as much variation as our actual values (otherwise our imputed values will tend to be all clumped together).\n\n\n\nMultiply impute the missing data, by fully modeling the covariate and the missingness, and generating a range of complete datasets under this model. Here you end up with a bunch of complete datasets that are all “reasonable guesses” as to what the full dataset might have been. You then analyze each one, and aggregate your findings across them to get a final answer.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html#visualizing-missing-data",
    "href": "intro_missing_data.html#visualizing-missing-data",
    "title": "11  An Introduction to Missing Data",
    "section": "11.2 Visualizing missing data",
    "text": "11.2 Visualizing missing data\nJust like with anything in statistics, the first thing to do is to look at our data. We want to know which variables are often missing, and if some variables are often missing together. We also want to know how much data is missing. The mice package has a variety of plots to show us patterns of missingness:\n\n  md.pattern(airquality)\n\n\n\n\n\n\n\n\n    Wind Temp Month Day Solar.R Ozone   \n111    1    1     1   1       1     1  0\n35     1    1     1   1       1     0  1\n5      1    1     1   1       0     1  1\n2      1    1     1   1       0     0  2\n       0    0     0   0       7    37 44\n\n\nThis plot gives us the different missing data patterns and the number of observations that have each missing data pattern. For example, the second row in the plot says there are 35 observations that have a missing data pattern where only Ozone is missing.\nEasier to understand patterns!\nWe can also just look at 10 observations to see everything that is going on. Here we take the first 10 rows of our dataset, but could also take a random 10 row with the tidyverse’s sample_n method.\n\n  airqualitysub = airquality[1:10, ]\n  airqualitysub\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    NA     194  8.6   69     5  10\n\n\nWe see that we have one observation missing two covariates and one each of missing Ozone only and Solar.R only.\n\n11.2.1 The VIM Package\nThe VIM package gives some alternate plots to explore missing data patterns. For example, aggr():\n\n aggr(airquality, col=c('navyblue','red'),\n      numbers=TRUE, sortVars=TRUE, labels=names(data),\n      cex.axis=.7, gap=3, prop=c(TRUE, FALSE), \n      ylab=c(\"Histogram of missing data\",\"Pattern\"))\n\n\n\n\n\n\n\n\n\n Variables sorted by number of missings: \n Variable  Count\n    Ozone 0.2418\n  Solar.R 0.0458\n     Wind 0.0000\n     Temp 0.0000\n    Month 0.0000\n      Day 0.0000\n\n\nOn the left, we have the proportion of missing data for each variable in our dataset. We can see that Ozone and Solar.R have missing values. On the right, we have the joint distribution of missingness. We can see that 111 observations have no missing values. From those with missing values, the majority have missing values for Ozone, some have missing values for Solar.R and only 2 observations have missing values for both Ozone and Solar.R.\n\n  marginplot(airquality[1:2])\n\n\n\n\n\n\n\n\nHere we have a scatterplot for the first two variables in our dataset: Ozone and Solar.R. These are the variables that have missing data. In addition to the standard scatterplot we are familiar with, information about missingness is shown in the margins. The red dots indicate observations with one or both values missing (so there can be a bunch of dots stacked up in the bottom-left corner). The numbers (37, 7, and 2 tells us how many observations are missing either or both of these variables).",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html#complete-case-analysis",
    "href": "intro_missing_data.html#complete-case-analysis",
    "title": "11  An Introduction to Missing Data",
    "section": "11.3 Complete case analysis",
    "text": "11.3 Complete case analysis\nWorking with complete cases (dropping observations with any missing data on our outcome and predictors) is always an option. We have been doing this in class and section. However, this can lead to substantial data loss, if we have a lot of missingness and it can heavily bias our results depending on why observations are missing.\nComplete case analysis is the R default.\n\n  fit &lt;- lm(Ozone ~ Wind, data = airquality )\n  summary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Wind, data = airquality)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-51.57 -18.85  -4.87  15.23  90.00 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    96.87       7.24   13.38  &lt; 2e-16 ***\nWind           -5.55       0.69   -8.04  9.3e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.5 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.362, Adjusted R-squared:  0.356 \nF-statistic: 64.6 on 1 and 114 DF,  p-value: 9.27e-13\n\n\nNote the listing in the summary of number of items deleted. You can find out which rows were deleted:\n\n## which rows/observations were deleted\n  deleted &lt;- na.action(fit)\n  deleted\n\n  5  10  25  26  27  32  33  34  35  36  37  39  42  43  45  46  52  53  54  55 \n  5  10  25  26  27  32  33  34  35  36  37  39  42  43  45  46  52  53  54  55 \n 56  57  58  59  60  61  65  72  75  83  84 102 103 107 115 119 150 \n 56  57  58  59  60  61  65  72  75  83  84 102 103 107 115 119 150 \nattr(,\"class\")\n[1] \"omit\"\n\n  naprint(deleted)\n\n[1] \"37 observations deleted due to missingness\"\n\n\nWe have more incomplete rows if we add Solar.R as predictor.\n\n  fit2 &lt;- lm(Ozone ~ Wind+Solar.R, data=airquality)\n  naprint(na.action(fit2))\n\n[1] \"42 observations deleted due to missingness\"\n\n\nWe can also drop observations with missing data ourselves instead of letting R do it for us. Dropping data preemptively is generally a good idea, especially if you plan on using predict().\n\n## complete cases on all variables in the data set \ncomplete.v1 = filter( airquality, complete.cases(airquality) )\n  \n## drop observations with missing values, but ignoring a specific variable  \ncomplete.v2 = filter(airquality, complete.cases(select( airquality, -Wind) ) )\n\n## drop observations with missing values on a specific variable  \ncomplete.v3 = filter(airquality, !is.na(Ozone))\n\nOnce you have subset your data, you just analyze what is left as normal. Easy as pie!",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html#mean-imputation",
    "href": "intro_missing_data.html#mean-imputation",
    "title": "11  An Introduction to Missing Data",
    "section": "11.4 Mean imputation",
    "text": "11.4 Mean imputation\nInstead of dropping observations with missing values, we can plug in some kind of reasonable value for the missing value, e.g. the grand/global mean. While this can be statistically questionable, it does allow us to use the information provided by that unit’s outcome and other covariates, without, we hope, unduly affecting the analysis of the missing covariate.\nGenerally, people will first plug in the mean value for anything missing, but then also make a dummy variable of whether that observation had a missing value there (or sometimes any missing value). You would then include both the original vector of covariates (with the means plugged in) along with the dummy variable in subsequent regressions and analyses.\n\n11.4.1 Doing Mean Imputation manually\nManually, we can just replace missing values for a variable with the grand/global mean.\n\n## make a new copy of the data\n  data.mean.impute = airquality\n  \n## select the observations with missing Ozone\n  miss.ozone = is.na(data.mean.impute$Ozone)\n  \n## replace those NAs with mean(Ozone)\n  data.mean.impute[miss.ozone,\"Ozone\"] = mean(airquality$Ozone, na.rm=TRUE)\n\nIn a multi-level context, it might make more sense to impute using the group mean rather than the grand mean. Here’s a generic function to do it. Here we group by month:\n\n## a function that replaces missing values in a vector \n## by the mean of the other values\n  mean.impute = function(y) { \n      y[is.na(y)] = mean(y, na.rm=TRUE)\n      return(y)\n  }\n\n  data.mean.impute = airquality %&gt;% group_by(Month) %&gt;%\n    mutate(Ozone = mean.impute(Ozone),\n           Solar.R = mean.impute(Solar.R) ) \n\nWe have mean imputed the Ozone column and the Solar.R column\n\n\n11.4.2 Mean imputation with the Mice package\nWe can use the mice package to do mean imputation. The mice package is a package that can do some quite complex imputation, and so when you call mice() (which says “impute missing values please”) you get back a rather complex object telling you what mice imputed, for whom, etc. This object, which is a mids object (see help(mids)), contains the multiply imputed dataset (or in our case, so far, singly imputed). The mice package then provides a lot of nice functions allowing you to get your imputed information out of this object.\nWe first demonstrate this for the 10 observations sampled above. Mice is generally going to be a two-step process: impute data, get completed dataset.\nFor step 1:\n\n  imp &lt;- mice(airqualitysub, method=\"mean\", m=1, maxit=1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n\nWarning: Number of logged events: 1\n\n  imp\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n  Ozone Solar.R    Wind    Temp   Month     Day \n \"mean\"  \"mean\"      \"\"      \"\"      \"\"      \"\" \nPredictorMatrix:\n        Ozone Solar.R Wind Temp Month Day\nOzone       0       1    1    1     0   1\nSolar.R     1       0    1    1     0   1\nWind        1       1    0    1     0   1\nTemp        1       1    1    0     0   1\nMonth       1       1    1    1     0   1\nDay         1       1    1    1     0   0\nNumber of logged events:  1 \n  it im dep     meth   out\n1  0  0     constant Month\n\n\nFor step 2:\n\n  cmp = complete(imp)\n  cmp\n\n   Ozone Solar.R Wind Temp Month Day\n1   41.0     190  7.4   67     5   1\n2   36.0     118  8.0   72     5   2\n3   12.0     149 12.6   74     5   3\n4   18.0     313 11.5   62     5   4\n5   23.1     173 14.3   56     5   5\n6   28.0     173 14.9   66     5   6\n7   23.0     299  8.6   65     5   7\n8   19.0      99 13.8   59     5   8\n9    8.0      19 20.1   61     5   9\n10  23.1     194  8.6   69     5  10\n\n\nWe see there are no missing values in cmp. They were all imputed with the mean of the other non-missing values. This is mean imputation.\nNow let’s impute the full dataset.\n\n  imp &lt;- mice(airquality, method=\"mean\", m=1, maxit=1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n  cmp = complete( imp )\n\nWe next make a dummy variable for each row of our data noting whether anything was imputed or not. We use the ici (Incomplete Case Indication) function to list all rows with any missing values.\n\n  head( ici(airquality) )\n\n[1] FALSE FALSE FALSE FALSE  TRUE  TRUE\n\n\nNote how we have a TRUE or FALSE for each row of our data.\nWe then store this as a covariate in our completed dataset:\n\n  cmp$imputed = ici(airquality)\n  head( cmp )\n\n  Ozone Solar.R Wind Temp Month Day imputed\n1  41.0     190  7.4   67     5   1   FALSE\n2  36.0     118  8.0   72     5   2   FALSE\n3  12.0     149 12.6   74     5   3   FALSE\n4  18.0     313 11.5   62     5   4   FALSE\n5  42.1     186 14.3   56     5   5    TRUE\n6  28.0     186 14.9   66     5   6    TRUE\n\n\n\n11.4.2.1 How well did mean imputation work?\nMean imputation has problems. The imputed values will all be the same, and thus when we look at how much variation is in our variables after imputation, it will go down. Compare the SD of our completed dataset Ozone values to the SD of the Ozone values for our non-missing values.\n\n  sd( airquality$Ozone, na.rm=TRUE )\n\n[1] 33\n\n  sd( cmp$Ozone )\n\n[1] 28.7\n\n\nNext, let’s look at some plots of our completed data, coloring the points by whether they were imputed.\n\nlibrary(ggplot2)\nggplot( cmp, aes(x=Ozone, col=imputed) ) +\n    stat_bin( geom=\"step\", position=\"identity\",\n              breaks=seq(-20, 200, 10) )\n\n\n\n\n\n\n\nggplot( cmp, aes(y=Ozone, x=Solar.R, col=imputed) ) +\n    geom_point() +\n    labs( y=\"Ozone (ppb)\", x=\"Solar Radiation (lang)\" )\n\n\n\n\n\n\n\n\nWhat we see in the above plots is that our imputed observations do not look like the rest of our data because one (or both) of their values always is in the exact center. This creates the “+” shape. It also gives the big spike at the mean for the histogram.\n\n\n11.4.2.2 Important Aside: Namespaces and function collisions\nWe now need to discuss a sad aspect of R. The short story is, different packages have functions with the same names and so if you have both packages loaded you will need to specify which package to use when calling such a function. You can do this by giving the “surname” of the function at the beginning of the function call (like, I believe, the Chinese). This comes up because for us the method complete() exists both in the tidyverse and in mice. In tidyverse, complete() fills in rows of missing combinations of values. In mice, complete() gives us a completed dataset after we have made an imputation call.\nIt turns out that since we loaded tidyverse first and mice second, the mice’s complete() method is the default. But if we loaded the packages in the other order, we would get strange errors. To be clear, we thus tell R to use mice by writing:\n\n  cmp = mice::complete( imp )\n\nIn general, you can detect such “namespace collisions” by noticing weird error messages all of a sudden when you don’t expect them. You can then type, for example, help( complete ) and it will list all the different completes around.\n\n  help( complete )\n\nAlso when you load a package it will write down what functions are getting mixed up for you. If you were looking at your R code you would get something like this:\ntidyr::complete() masks mice::complete()",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html#regression-imputation",
    "href": "intro_missing_data.html#regression-imputation",
    "title": "11  An Introduction to Missing Data",
    "section": "11.5 Regression imputation",
    "text": "11.5 Regression imputation\nRegression imputation is half way between mean imputation and multiple imputation. In regression imputation we predict what values we expect for anything missing based on the other values of the observation. For example, if we know that urban/rural is correlated with race, we might impute a different value for race if we know an observation came from an urban environment vs. rural. We do this with regression: we fit a model predicting each variable using the others and then use that regression model to predict any missing values.\nWe can do this manually, but then it gets very hard when multiple variables are missing for a given observation. The mice package is more clever: it does variables one at a time, and the cycles around so everything can get imputed.\n\n11.5.1 Manually\nHere is how to use other variables to predict missing values.\n\n  ic( airqualitysub )\n\n   Ozone Solar.R Wind Temp Month Day\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n10    NA     194  8.6   69     5  10\n\n  fit &lt;- lm(Ozone ~ Solar.R, data=airqualitysub)\n\n## predict for missing ozone  \n  need.pred = subset( airqualitysub, is.na( Ozone ) )\n  need.pred\n\n   Ozone Solar.R Wind Temp Month Day\n5     NA      NA 14.3   56     5   5\n10    NA     194  8.6   69     5  10\n\n  pred &lt;- predict(fit, newdata=need.pred)\n  pred\n\n   5   10 \n  NA 23.1 \n\n\nBut now we have to merge back in, and we didn’t solve for case 5 because we are missing the variable we would use to predict the other missing variable. Ick. This is where missing data gets really hard (when we have multiple missing values on multiple variables). So let’s quit now and turn to a package that will handle all of this for us.\n\n\n11.5.2 Mice\nTo do regression imputation using mice, we simply call the mice() method:\n\n  imp &lt;- mice(airquality[,1:2], method=\"norm.predict\", m=1, maxit=3,seed=1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n  2   1  Ozone  Solar.R\n  3   1  Ozone  Solar.R\n\n\nWe have everything! How did it do it? By chaining equations. First we start with mean imputation. Then we use our fit model to predict for one covariate, and then we use those predicted scores to predict for the next covariate, and so forth. We cycle back and then everything is jointly predicting everything else.\nThe complete() method gives us a complete dataset with everything imputed. Like so:\n\n  cdat = mice::complete( imp )\n  head( cdat )\n\n  Ozone Solar.R\n1  41.0     190\n2  36.0     118\n3  12.0     149\n4  18.0     313\n5  42.7     186\n6  28.0     169\n\n  nrow( cdat )\n\n[1] 153\n\n  nrow( airquality )\n\n[1] 153\n\n\nNext we make a variable of which cases have imputed values and not (any row with missing data must have been partially imputed.)\n\n  cdat$imputed = ici( airquality )\n\nAnd see our results! Compare to mean imputation, above.\n\nggplot( cdat, aes(x=Ozone, col=imputed) ) +\n    stat_bin( geom=\"step\", position=\"identity\",\n              breaks=seq(-20, 200, 10) )\n\n\n\n\n\n\n\nggplot( cdat, aes(y=Ozone, x=Solar.R, col=imputed) ) +\n    geom_point() +\n    labs( y=\"Ozone (ppb)\", x=\"Solar Radiation (lang)\" )\n\n\n\n\n\n\n\n\nThis is better than mean imputation. See how we impute different Ozone for different Solar Radiation values, taking advantage of the information of knowing that they are correlated? But it still is obvious what is mean imputed and what is not. Also, the variance of our imputed values still does not contain the residual variation around the predicted values that we would get in real data. We can do one more enhancement to fix this.\n\n\n11.5.3 Stochastic regression imputation\nWe extend regression imputation by randomly drawing observations that look like real ones. See in the two imputations below we get slightly different values for our imputed data.\nHere we do it on our mini-dataset and look at the imputed values for our observations with missing values only:\n\n  imp &lt;- mice(airqualitysub[,1:2],method=\"norm.nob\",m=1,maxit=1,seed=1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n  imp$imp\n\n$Ozone\n       1\n5   8.09\n10 44.58\n\n$Solar.R\n      1\n5 181.2\n6  83.7\n\n  imp &lt;- mice(airqualitysub[,1:2],method=\"norm.nob\",m=1,maxit=1,seed=4)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n  imp$imp\n\n$Ozone\n      1\n5  34.4\n10 31.6\n\n$Solar.R\n    1\n5 381\n6 260\n\n\nNow let’s do it on the full data and look at the imputed values and compare to our plots above.\n\n  imp &lt;- mice(airquality[,1:2],method=\"norm.nob\",m=1,maxit=1,seed=1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n  cdat = mice::complete( imp )\n  cdat$imputed = ici( airquality )\n\n  ggplot( cdat, aes(x=Ozone, col=imputed) ) +\n    stat_bin( geom=\"step\", position=\"identity\",\n              breaks=seq(-20, 200, 10) )\n\n\n\n\n\n\n\n  ggplot( cdat, aes(y=Ozone, x=Solar.R, col=imputed) ) +\n    geom_point() +\n    labs( y=\"Ozone (ppb)\", x=\"Solar Radiation (lang)\" )\n\n\n\n\n\n\n\n\nBetter, but not perfect. What is better? What is still not perfect?",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html#multiple-imputation",
    "href": "intro_missing_data.html#multiple-imputation",
    "title": "11  An Introduction to Missing Data",
    "section": "11.6 Multiple imputation",
    "text": "11.6 Multiple imputation\nIf missing data is a significant issue in your dataset, then mean or regression imputation can be a bit too hacky and approximate. In these contexts, multiple imputation is the way to go.\nWe do this as follows:\n\n  imp &lt;- mice(airqualitysub, seed=2, print=FALSE)\n\nWarning: Number of logged events: 1\n\n  imp\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n  Ozone Solar.R    Wind    Temp   Month     Day \n  \"pmm\"   \"pmm\"      \"\"      \"\"      \"\"      \"\" \nPredictorMatrix:\n        Ozone Solar.R Wind Temp Month Day\nOzone       0       1    1    1     0   1\nSolar.R     1       0    1    1     0   1\nWind        1       1    0    1     0   1\nTemp        1       1    1    0     0   1\nMonth       1       1    1    1     0   1\nDay         1       1    1    1     0   0\nNumber of logged events:  1 \n  it im dep     meth   out\n1  0  0     constant Month\n\n  imp$imp\n\n$Ozone\n    1  2  3  4  5\n5  18 41 28 23 23\n10 36 18 36 19 28\n\n$Solar.R\n    1  2   3  4  5\n5 149 99 194 99 19\n6 194 19 194 19 19\n\n$Wind\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Temp\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Month\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Day\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n\nSee multiple columns of imputed data? (We have 5 here.)\nFirst aside: All variables you’ll be using for your model should be included in the imputation model. Notice we included the full dataset in mice, not just the variables with missing values. This way we can account for associations between all the outcome and the predictors in the model we’ll be fitting. Your imputation model can be more complicated than your model of interest. That is, you can include additional variables that predict missing values but will not be part of your final model of interest.\nSecond aside: All variables in your imputation model should be in the correct functional form! Quadratic, higher order polynomials and interaction terms are just another variable that we need to impute. Although it may seem logical to impute your variables first and then calculate the interaction or non-linear term, this can lead to bias.\nThird aside: The ordering of the variables in the dataset you are feeding into mice can make a difference in results and model convergence. Generally, you want to order your variables from least to most missing. Here, we reorder the variables from least to most missing, and obtain different results.\n\n  test = airqualitysub[,c(6,5,4,3,2,1)]\n  head(test)\n\n  Day Month Temp Wind Solar.R Ozone\n1   1     5   67  7.4     190    41\n2   2     5   72  8.0     118    36\n3   3     5   74 12.6     149    12\n4   4     5   62 11.5     313    18\n5   5     5   56 14.3      NA    NA\n6   6     5   66 14.9      NA    28\n\n  test.imp &lt;- mice(test, seed=2, print=FALSE)\n\nWarning: Number of logged events: 1\n\n  test.imp$imp\n\n$Day\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Month\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Temp\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Wind\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Solar.R\n    1   2   3   4   5\n5 194 118 194 313 190\n6 118 194 118 118 190\n\n$Ozone\n    1  2  3  4  5\n5  18 23 23 23 41\n10 12  8 18 19  8\n\n\nHow to get each complete dataset?\n\n## first complete dataset \n  mice::complete(imp, 1)\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     18     149 14.3   56     5   5\n6     28     194 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    36     194  8.6   69     5  10\n\n## and our second complete dataset\n  mice::complete(imp, 2)\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     41      99 14.3   56     5   5\n6     28      19 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    18     194  8.6   69     5  10\n\n\nSee how they are different? They were randomly imputed. We basically used the stochastic regression thing, above, multiple times.\n\n  mice::complete(imp, 1)[ ici(airqualitysub), ]\n\n   Ozone Solar.R Wind Temp Month Day\n5     18     149 14.3   56     5   5\n6     28     194 14.9   66     5   6\n10    36     194  8.6   69     5  10\n\n  mice::complete(imp, 2)[ ici(airqualitysub), ]\n\n   Ozone Solar.R Wind Temp Month Day\n5     41      99 14.3   56     5   5\n6     28      19 14.9   66     5   6\n10    18     194  8.6   69     5  10\n\n\nOn full data:\n\n  imp &lt;- mice(airquality, seed=1, print=FALSE)\n\nNow we estimate for each imputed dataset using the with() method that, in mice, will do the regression for each completed dataset. See help with.mids.\n\n  fit &lt;- with(imp, lm(Ozone ~ Wind + Temp + Solar.R))\n  fit\n\ncall :\nwith.mids(data = imp, expr = lm(Ozone ~ Wind + Temp + Solar.R))\n\ncall1 :\nmice(data = airquality, printFlag = FALSE, seed = 1)\n\nnmis :\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\nanalyses :\n[[1]]\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R)\n\nCoefficients:\n(Intercept)         Wind         Temp      Solar.R  \n   -66.2402      -2.8219       1.6134       0.0563  \n\n\n[[2]]\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R)\n\nCoefficients:\n(Intercept)         Wind         Temp      Solar.R  \n   -71.2842      -2.9055       1.6749       0.0633  \n\n\n[[3]]\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R)\n\nCoefficients:\n(Intercept)         Wind         Temp      Solar.R  \n   -66.9511      -2.9322       1.6479       0.0543  \n\n\n[[4]]\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R)\n\nCoefficients:\n(Intercept)         Wind         Temp      Solar.R  \n   -33.8480      -3.6628       1.3244       0.0427  \n\n\n[[5]]\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R)\n\nCoefficients:\n(Intercept)         Wind         Temp      Solar.R  \n   -77.4163      -2.7438       1.7264       0.0663  \n\n\nThis can take any function call that takes a formula. So glm, lm, whatever… We can then pool the estimates using the standard theory of combining multiply imputed datasets. The basic idea is to combine the variation/uncertainty of the multiple sets with the average uncertainty we would have for each set if it was truly complete and not imputed.\n\n  tab &lt;- summary(pool(fit))\n  colnames( tab )\n\n[1] \"term\"      \"estimate\"  \"std.error\" \"statistic\" \"df\"        \"p.value\"  \n\n  tab[,c(1:3,5)]\n\n         term estimate std.error   df\n1 (Intercept) -63.1480   26.6769 13.8\n2        Wind  -3.0132    0.6831 24.0\n3        Temp   1.5974    0.2742 19.6\n4     Solar.R   0.0566    0.0222 52.4\n\n\nAside: You will notice that once we fit our model on the imputed data, with() returned an object of class mira. Mira objects can be pooled to get the pooled estimates, whereas objects of class glm, lm, lmer, etc. cannot be pooled. You will also notice that you cannot use predict with a mira object. To use predict, you can stack the imputed datasets and fit your model on this complete dataset. Parameter estimates generated by pool are the average of the parameter estimates from the model fit on each imputed dataset separately. So your coefficients are fine. However, your SEs will be underestimated. How underestimated your SEs will be depends, to an extent, on how much data is missing and whether it is missing at random.\nOur old, sad method:\n\n  fit &lt;- lm(Ozone~Wind+Temp+Solar.R,data=airquality,na.action=na.omit)\n  summary( fit )\n\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R, data = airquality, \n    na.action = na.omit)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-40.48 -14.22  -3.55  10.10  95.62 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -64.3421    23.0547   -2.79   0.0062 ** \nWind         -3.3336     0.6544   -5.09  1.5e-06 ***\nTemp          1.6521     0.2535    6.52  2.4e-09 ***\nSolar.R       0.0598     0.0232    2.58   0.0112 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.2 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.606, Adjusted R-squared:  0.595 \nF-statistic: 54.8 on 3 and 107 DF,  p-value: &lt;2e-16\n\n  round(coef(summary(fit)),3)\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   -64.34     23.055   -2.79    0.006\nWind           -3.33      0.654   -5.09    0.000\nTemp            1.65      0.254    6.52    0.000\nSolar.R         0.06      0.023    2.58    0.011\n\n\nIn this case, the missing data estimates are basically the same as the complete case analysis, it appears. We only had 5% missing data though.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html#extensions",
    "href": "intro_missing_data.html#extensions",
    "title": "11  An Introduction to Missing Data",
    "section": "11.7 Extensions",
    "text": "11.7 Extensions\n\n11.7.1 Non-continuous variables\nEverything shown above can easily be extended to non-continuous variables. The easiest way to do this is using the mice package. It allows you to specify the type of variable you are imputing, e.g. dichotomous or categorical. Mice will automatically detect and handle non-continuous variables. You can also specify these variables yourself. Here is an example using nhanes data (another built-in R dataset).\n\n## load data \n  data(nhanes2)\n  head(nhanes2)\n\n    age  bmi  hyp chl\n1 20-39   NA &lt;NA&gt;  NA\n2 40-59 22.7   no 187\n3 20-39   NA   no 187\n4 60-99   NA &lt;NA&gt;  NA\n5 20-39 20.4   no 113\n6 60-99   NA &lt;NA&gt; 184\n\n## create some missing values for an ordered categorical variable\n  nhanes2$age[1:5] = NA\n  head(nhanes2) \n\n    age  bmi  hyp chl\n1  &lt;NA&gt;   NA &lt;NA&gt;  NA\n2  &lt;NA&gt; 22.7   no 187\n3  &lt;NA&gt;   NA   no 187\n4  &lt;NA&gt;   NA &lt;NA&gt;  NA\n5  &lt;NA&gt; 20.4   no 113\n6 60-99   NA &lt;NA&gt; 184\n\n## impute 5 datasets \n  imp.cat &lt;- mice(nhanes2, m = 5, print=FALSE)     \n  full.cat = mice::complete(imp.cat)           ## print the first imputed data set\n  head(full.cat)\n\n    age  bmi hyp chl\n1 40-59 21.7 yes 187\n2 40-59 22.7  no 187\n3 20-39 27.5  no 187\n4 60-99 24.9 yes 218\n5 40-59 20.4  no 113\n6 60-99 24.9 yes 184\n\n\nWe can check what imputation method mice used for each variable:\n\n  imp.cat$method\n\n      age       bmi       hyp       chl \n\"polyreg\"     \"pmm\"  \"logreg\"     \"pmm\" \n\n\nWe can see that mice used the polyreg imputation method for the variable age, which means it treated it as an unordered categorical variable. But this is an ordered variable: higher values categories signified older age. We can manually force mice to treat age as an ordered categorical variable. We will keep the imputation methods for the remaining variables the same.\n\n  imp.cat2 &lt;- mice(nhanes2, meth=c(\"polr\",\"pmm\",\"logreg\",\"pmm\"), m=5, print=FALSE)\n  head(mice::complete(imp.cat2, 1))\n\n    age  bmi hyp chl\n1 40-59 27.5 yes 184\n2 60-99 22.7  no 187\n3 60-99 20.4  no 187\n4 20-39 35.3  no 184\n5 40-59 20.4  no 113\n6 60-99 22.7  no 184\n\n  imp.cat2$method\n\n     age      bmi      hyp      chl \n  \"polr\"    \"pmm\" \"logreg\"    \"pmm\" \n\n\n\n\n11.7.2 Multi-level data\nMultilevel data gets more tricky: should we impute taking into account cluster? How do we do that?\nFor an initial pass, I would recommend simply doing regression imputation ignoring cluster/grouping, and then adding in that dummy variable of whether a value is imputed.\n\n\n11.7.3 Longitudinal data\nWith longitudinal data we can often use all our data even for individuals with missing data on the outcome, if we assume data are MAR (“Missing at Random”). MAR means that conditional on the observed data, missingness may depend on any observed data, but not on unobserved data. we explore our missing data on individuals over time and on outcomes as above to get a sense of whether MAR is a reasonable assumption or not. Then lmer basically handles the rest for us, as far as we have enough observations per individual, on average, to estimate the number of random effects we are trying to estimate. With respect to missing data on covariates or predictors, you can handle those with one of the methods described above.\nHere we show how to explore missing data in longitudinal analysis using data on toenail detachment, which you will see in the unit on generalized MLMs. The data is from a RCT where patients were getting a different type of drug to prevent toenail detachment (the outcome).\n\n## load data\n  toes = foreign::read.dta( \"data/toenail.dta\" )\n\nFirst, let’s look at how many times patients were observed.\n\n## how many time points per patient?\n  table( table( toes$patient ) )\n\n\n  1   2   3   4   5   6   7 \n  5   3   7   6  10  39 224 \n\n\nWe have 224 patients observed at all 7 time points, and the rest of the patients are observed at fewer time points, between 1 and 6.\n\n## define function \nsummarise.patient = function( patient ) {\n    pat = rep( \".\", 7 )\n    pat[ patient$visit ] = 'X'\n    paste( pat, collapse=\"\" )\n}\n  ## For each patient, this code makes a string of \".\" \n  ## then it replaces all dots with an \"X\" if we have data for that visit\n\n## summarize missingness  \nmiss = toes %&gt;% group_by( patient ) %&gt;%\n    do( pattern = summarise.patient(.) ) %&gt;%\n    unnest(cols = c(pattern))\n  ## Group the data by patient \n  ## Then use the do() command on each chunk of our dataframe\n  ## The \".\" means \"the chunk\" (it is a pronoun, essentially).  \n  ## This code creates a list of character vectors\n  ## The unnest() takes our character vector out of this list made by \"do\"\n\nhead( miss )\n\n# A tibble: 6 × 2\n  patient pattern\n    &lt;dbl&gt; &lt;chr&gt;  \n1       1 XXXXXXX\n2       2 XXXXXX.\n3       3 XXXXXXX\n4       4 XXXXXXX\n5       6 XXXXXXX\n6       7 XXXXXXX\n\n\nHere we see the different patterns of missing outcomes, i.e., when patients leave and if they come back. When patients leave and never come back, regardless of the time point (see lines 4 and 5), we have monotone missingness.\n\n## sort missing patterns in decreasing order\n## starting with no missingness \nsort( table( miss$pattern ), decreasing=TRUE )\n\n\nXXXXXXX XXXXX.X XXXX.XX XXX.... X...... XXXXX.. XXXX... XX..... XXX.XXX XXXXXX. \n    224      21      10       6       5       5       4       3       3       3 \nXXX.X.. XXXX..X X.XXXXX XX..X.. XX.XXX. XX.XXXX XXX..XX XXX.X.X \n      2       2       1       1       1       1       1       1 \n\n## summarize number of data patterns \nmiss = miss %&gt;% group_by( pattern ) %&gt;%\n    summarise( n=n() )\nmiss = arrange( miss, -n )\nmiss\n\n# A tibble: 18 × 2\n   pattern     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 XXXXXXX   224\n 2 XXXXX.X    21\n 3 XXXX.XX    10\n 4 XXX....     6\n 5 X......     5\n 6 XXXXX..     5\n 7 XXXX...     4\n 8 XX.....     3\n 9 XXX.XXX     3\n10 XXXXXX.     3\n11 XXX.X..     2\n12 XXXX..X     2\n13 X.XXXXX     1\n14 XX..X..     1\n15 XX.XXX.     1\n16 XX.XXXX     1\n17 XXX..XX     1\n18 XXX.X.X     1\n\n## percent missing data (224 complete cases)\n224 / sum( miss$n )\n\n[1] 0.762\n\n  ## 76% of patients with complete data\n\nSecond, we look at patterns of missing outcomes. The outcome here is toenail detachment.\n\n## reshape data to wide \n  dat.wide = reshape( toes2, direction=\"wide\", v.names=\"outcome\",\n                    idvar=\"patient\", timevar = \"visit\" )\n  head( dat.wide )\n\n   patient treatment           Tx outcome.1 outcome.2 outcome.3 outcome.4\n1        1         1 Itraconazole         1         1         1         0\n8        2         0  Terbinafine         0         0         1         1\n14       3         0  Terbinafine         0         0         0         0\n21       4         0  Terbinafine         1         0         0         0\n28       6         1 Itraconazole         1         1         1         0\n35       7         1 Itraconazole         0         0         0         0\n   outcome.5 outcome.6 outcome.7\n1          0         0         0\n8          0         0        NA\n14         0         0         1\n21         0         0         0\n28         0         0         0\n35         1         1         1\n\n## looking at missing data with mice package\n  md.pattern( dat.wide )\n\n\n\n\n\n\n\n\n    patient treatment Tx outcome.1 outcome.2 outcome.3 outcome.4 outcome.7\n224       1         1  1         1         1         1         1         1\n21        1         1  1         1         1         1         1         1\n10        1         1  1         1         1         1         1         1\n2         1         1  1         1         1         1         1         1\n3         1         1  1         1         1         1         1         0\n5         1         1  1         1         1         1         1         0\n4         1         1  1         1         1         1         1         0\n3         1         1  1         1         1         1         0         1\n1         1         1  1         1         1         1         0         1\n1         1         1  1         1         1         1         0         1\n2         1         1  1         1         1         1         0         0\n6         1         1  1         1         1         1         0         0\n1         1         1  1         1         1         0         1         1\n1         1         1  1         1         1         0         1         0\n1         1         1  1         1         1         0         0         0\n3         1         1  1         1         1         0         0         0\n1         1         1  1         1         0         1         1         1\n5         1         1  1         1         0         0         0         0\n          0         0  0         0         6        11        22        30\n    outcome.5 outcome.6    \n224         1         1   0\n21          1         0   1\n10          0         1   1\n2           0         0   2\n3           1         1   1\n5           1         0   2\n4           0         0   3\n3           1         1   1\n1           1         0   2\n1           0         1   2\n2           1         0   3\n6           0         0   4\n1           1         1   1\n1           1         1   2\n1           1         0   4\n3           0         0   5\n1           1         1   1\n5           0         0   6\n           31        50 150\n\n## Another way to generating missingness patterns is to create a function\n## This function takes the visits and outcomes and puts a 1 or 0 if there is an\n## outcome and a dot if missing.\nmake.pat = function( visit, outcome ) {\n    pat = rep( \".\", 7 )\n    pat[ visit ] = outcome\n    paste( pat, collapse=\"\" )\n}\n\n## call our function on all our patients.\noutcomes = toes %&gt;% group_by( patient ) %&gt;%\n    summarise( tx = Tx[[1]],\n               num.obs = n(),\n               num.detach = sum( outcome ),\n               out = make.pat( visit, outcome ) )\n\nhead( outcomes, 20 )\n\n# A tibble: 20 × 5\n   patient tx           num.obs num.detach out    \n     &lt;dbl&gt; &lt;fct&gt;          &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;  \n 1       1 Itraconazole       7          3 1110000\n 2       2 Terbinafine        6          2 001100.\n 3       3 Terbinafine        7          1 0000001\n 4       4 Terbinafine        7          1 1000000\n 5       6 Itraconazole       7          3 1110000\n 6       7 Itraconazole       7          3 0000111\n 7       9 Itraconazole       7          0 0000000\n 8      10 Terbinafine        7          0 0000000\n 9      11 Itraconazole       7          4 1111000\n10      12 Terbinafine        7          3 0100110\n11      13 Terbinafine        7          4 1111000\n12      15 Itraconazole       6          2 11000.0\n13      16 Itraconazole       6          1 0000.10\n14      17 Terbinafine        6          4 11110.0\n15      18 Terbinafine        6          1 001.000\n16      19 Itraconazole       7          0 0000000\n17      20 Terbinafine        6          0 000.000\n18      21 Itraconazole       3          2 110....\n19      22 Terbinafine        7          3 1110000\n20      23 Itraconazole       7          0 0000000\n\n## how many folks have no detachments?\ntable( outcomes$num.detach )\n\n\n  0   1   2   3   4   5   6   7 \n163  25  25  31  30   8   4   8 \n\n163 / nrow(outcomes)\n\n[1] 0.554\n\n## how many always detached?\nsum( outcomes$num.detach == outcomes$num.obs )\n\n[1] 16\n\n16 / nrow(outcomes)\n\n[1] 0.0544",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html#further-reading",
    "href": "intro_missing_data.html#further-reading",
    "title": "11  An Introduction to Missing Data",
    "section": "11.8 Further reading",
    "text": "11.8 Further reading\nSome further reading on handling missing data. But this is really a course into itself.\n\nGelman & Hill Chapter 25 has a more detailed discussion of missing data imputation.\nWhite IR, Royston P, Wood AM. Multiple imputation using chained equations: issues and guidance for practice. Statistics in Medicine 2011;30: 377-399.\nGraham, JW, Olchowski, AE, Gilreath, TD, 2007. How Many Imputations are Really Needed? Some Practical Clarifications of Multiple Imputation Theory 206–213. https://doi.org/10.1007/s11121-007-0070-9\nvan Buurin S, Groothuis-Oudshoorn K, MICE: Multivariate Imputation by Chained Equations. Journal of Statistical Software. 2011;45(3):1-68.\nGrund S, Lüdtke O, Robitzsch A. Multiple Imputation of Missing Data for Multilevel Models: Simulations and Recommendations. DOI: 10.1177/1094428117703686",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html#appendix-more-about-the-mice-package",
    "href": "intro_missing_data.html#appendix-more-about-the-mice-package",
    "title": "11  An Introduction to Missing Data",
    "section": "11.9 Appendix: More about the mice package",
    "text": "11.9 Appendix: More about the mice package\nThe mice package gives back a very complex object that has a lot of information about how values were imputed, which values were imputed, and so forth. In the following we unpack the imp variable from above a bit more.\nLooking at the imputation object\nIn the following code, we look at the object we get back from mice(). It has lots of parts that we can peek into.\nFirst, the imp list inside of imp stores all of our newly imputed data. It is itself a list of each variable with their imputed values:\n\n  imp$imp\n\n$Ozone\n      1   2   3   4   5\n5     6  19  14   8  14\n10   12  12   7  23  23\n25   14  19  14  19  14\n26   37  18  32  32  18\n27   11   1  18  13  18\n32   65  45  13  28  29\n33   22  36  12  18  16\n34   13  18   1  13  13\n35   63  35  45  52  71\n36   23  39  20  59  96\n37   24  16  12  34  18\n39   64 135  85  80  91\n42  115  76 115  37  91\n43   66 122  78  64 122\n45   44  28  45  23  16\n46   23  45  46  45  35\n52   20  52  63  47  47\n53   59  59  48 115  37\n54   40  16  35  37  63\n55   40  35  48  39  49\n56   23  39  59  59  16\n57   44  52  40  52  20\n58   30  30  27  14  23\n59   45  32  16  16  46\n60   44  27  34  28  30\n61   89  64  80  37  64\n65   16  16  14  23  29\n72   46  52  65  45  35\n75   35  64  71  18  78\n83   20  40  71  46  59\n84   28  63  37  29  63\n102 115  78  78  37  66\n103  46  29  31  23  40\n107  16  30  13  14  22\n115  41  12  44   7  22\n119  50  78 122  85  50\n150  24  12  27  21  12\n\n$Solar.R\n     1   2   3   4   5\n5    7 313  82  13 314\n6  322 187 222  24 238\n11  66 274 139 135 112\n27  20  24   7 238 193\n96 175 223 284 197 220\n97  51 139 274 237  98\n98  98 203 220 188 276\n\n$Wind\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Temp\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Month\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Day\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n  str( imp$imp )\n\nList of 6\n $ Ozone  :'data.frame':    37 obs. of  5 variables:\n  ..$ 1: int [1:37] 6 12 14 37 11 65 22 13 63 23 ...\n  ..$ 2: int [1:37] 19 12 19 18 1 45 36 18 35 39 ...\n  ..$ 3: int [1:37] 14 7 14 32 18 13 12 1 45 20 ...\n  ..$ 4: int [1:37] 8 23 19 32 13 28 18 13 52 59 ...\n  ..$ 5: int [1:37] 14 23 14 18 18 29 16 13 71 96 ...\n $ Solar.R:'data.frame':    7 obs. of  5 variables:\n  ..$ 1: int [1:7] 7 322 66 20 175 51 98\n  ..$ 2: int [1:7] 313 187 274 24 223 139 203\n  ..$ 3: int [1:7] 82 222 139 7 284 274 220\n  ..$ 4: int [1:7] 13 24 135 238 197 237 188\n  ..$ 5: int [1:7] 314 238 112 193 220 98 276\n $ Wind   :'data.frame':    0 obs. of  5 variables:\n  ..$ 1: logi(0) \n  ..$ 2: logi(0) \n  ..$ 3: logi(0) \n  ..$ 4: logi(0) \n  ..$ 5: logi(0) \n $ Temp   :'data.frame':    0 obs. of  5 variables:\n  ..$ 1: logi(0) \n  ..$ 2: logi(0) \n  ..$ 3: logi(0) \n  ..$ 4: logi(0) \n  ..$ 5: logi(0) \n $ Month  :'data.frame':    0 obs. of  5 variables:\n  ..$ 1: logi(0) \n  ..$ 2: logi(0) \n  ..$ 3: logi(0) \n  ..$ 4: logi(0) \n  ..$ 5: logi(0) \n $ Day    :'data.frame':    0 obs. of  5 variables:\n  ..$ 1: logi(0) \n  ..$ 2: logi(0) \n  ..$ 3: logi(0) \n  ..$ 4: logi(0) \n  ..$ 5: logi(0) \n\n  str( imp$imp$Ozone )\n\n'data.frame':   37 obs. of  5 variables:\n $ 1: int  6 12 14 37 11 65 22 13 63 23 ...\n $ 2: int  19 12 19 18 1 45 36 18 35 39 ...\n $ 3: int  14 7 14 32 18 13 12 1 45 20 ...\n $ 4: int  8 23 19 32 13 28 18 13 52 59 ...\n $ 5: int  14 23 14 18 18 29 16 13 71 96 ...\n\n\nWe see that Ozone and Solar.R have imputed values, and the other variables do not.\nNext, we see two missing observations in our original data and then see the two imputed values for these two missing observations.\n\n  airqualitysub$Ozone\n\n [1] 41 36 12 18 NA 28 23 19  8 NA\n\n  imp$imp$Ozone[,1]\n\n [1]   6  12  14  37  11  65  22  13  63  23  24  64 115  66  44  23  20  59  40\n[20]  40  23  44  30  45  44  89  16  46  35  20  28 115  46  16  41  50  24\n\n\nWe can make (the hard way) a vector of Ozone by plugging our missing values into the original data. But the complete() method, above, is preferred.\n\n  oz = airqualitysub$Ozone\n  oz[ is.na( oz ) ] = imp$imp$Ozone[,1]\n\nWarning in oz[is.na(oz)] = imp$imp$Ozone[, 1]: number of items to replace is\nnot a multiple of replacement length\n\n  oz\n\n [1] 41 36 12 18  6 28 23 19  8 12\n\n\nWhat else is there in imp?\n\n  names(imp)\n\n [1] \"data\"            \"imp\"             \"m\"               \"where\"          \n [5] \"blocks\"          \"call\"            \"nmis\"            \"method\"         \n [9] \"predictorMatrix\" \"visitSequence\"   \"formulas\"        \"post\"           \n[13] \"blots\"           \"ignore\"          \"seed\"            \"iteration\"      \n[17] \"lastSeedValue\"   \"chainMean\"       \"chainVar\"        \"loggedEvents\"   \n[21] \"version\"         \"date\"           \n\n\nWhat was our imputation method?\n\n  imp$method\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n  \"pmm\"   \"pmm\"      \"\"      \"\"      \"\"      \"\" \n\n\nMean imputation for each variable with missing values. Later this will say other thing.\nWhat was used to impute what?\n\n  imp$predictorMatrix\n\n        Ozone Solar.R Wind Temp Month Day\nOzone       0       1    1    1     1   1\nSolar.R     1       0    1    1     1   1\nWind        1       1    0    1     1   1\nTemp        1       1    1    0     1   1\nMonth       1       1    1    1     0   1\nDay         1       1    1    1     1   0",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "intro_missing_data.html#appendix-the-amelia-package",
    "href": "intro_missing_data.html#appendix-the-amelia-package",
    "title": "11  An Introduction to Missing Data",
    "section": "11.10 Appendix: The amelia package",
    "text": "11.10 Appendix: The amelia package\nAmelia is another multiple imputation and missing data package. We do not prefer it, but have some demonstration code in the following, for reference.\n\n  library(Amelia)\n\nWarning: package 'Rcpp' was built under R version 4.3.3\n\n\nFor missingness we can make the following:\n\n  missmap(airquality)\n\n\n\n\n\n\n\n\nEach row of the plot is a row of the data, and missing values are shown in brown. But ugly! And hard to see any trends in the missingness.\nYou can use the Amelia package to do mean imputation.\n\n  library(dplyr)\n\n## exclude variables that do not vary\n  a.airquality = airquality %&gt;% dplyr::select(-Month)\n\n## impute data\n  a.imp &lt;- amelia(a.airquality, m=5)\n\n-- Imputation 1 --\n\n  1  2  3  4  5  6\n\n-- Imputation 2 --\n\n  1  2  3  4  5  6\n\n-- Imputation 3 --\n\n  1  2  3  4  5\n\n-- Imputation 4 --\n\n  1  2  3  4  5  6  7\n\n-- Imputation 5 --\n\n  1  2  3  4  5  6\n\n  a.imp\n\n\nAmelia output with 5 imputed datasets.\nReturn code:  1 \nMessage:  Normal EM convergence. \n\nChain Lengths:\n--------------\nImputation 1:  6\nImputation 2:  6\nImputation 3:  5\nImputation 4:  7\nImputation 5:  6\n\n\nWe can plot our imputed values against our observed values to check that they make sense. We will do this for just one of five datasets we just imputed using Amelia.\n\n## put imputed values from the third dataset in an object\n  one_imp &lt;- a.imp$imputations[[3]]$Ozone\n\n## make object with observed values \n## from observations without missing Ozone values\n  obs_data &lt;- a.airquality$Ozone \n  \n## make a plot overlaying observed and imputed values\n  hist(one_imp[is.na(obs_data)], prob=TRUE, xlab=\"Ozone\",\n       main=\"Histogram of Imputed Values in 3rd Imputation \\nCompared to Density in Observed Data\",\n       col=\"cyan\", ylim=c(0,0.02))\n  lines(density(obs_data[!is.na(obs_data)]), col=\"darkblue\", lwd=2)\n\n\n\n\n\n\n\n\nYou can also do multiple imputation in Amelia. However, Amelia does not have an easy way to combine the estimates from the imputed datasets (no analogue of with() in mice). You can write a function that fits your model of interest in each imputed dataset and then use a package like mitools to pool the estimates and variances.\nMuch easier to use mice!\nAside: A more important limitation of Amelia is that the algorithm it uses to impute missing values assumes multivariate normality, which is often questionable, especially when you have binary variables.",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>An Introduction to Missing Data</span>"
    ]
  },
  {
    "objectID": "r_tips.html",
    "href": "r_tips.html",
    "title": "12  Tips, Tricks, and Debugging in R",
    "section": "",
    "text": "12.1 Some principles to live by",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tips, Tricks, and Debugging in R</span>"
    ]
  },
  {
    "objectID": "r_tips.html#some-principles-to-live-by",
    "href": "r_tips.html#some-principles-to-live-by",
    "title": "12  Tips, Tricks, and Debugging in R",
    "section": "",
    "text": "12.1.1 Watch Tricky letter and number confusion in code\nThe letter “l” looks like the number “1”—watch out for that. Things like “mylm” are usually all letters, with “lm” standing for linear model.\n\n\n12.1.2 Write in a good R style\nTry to do the following\n\nComment your code!\nStructure your R file like so:\n\nDescriptive comments (including date)\nLoad libraries\nConstants and script parameters (# iterations, etc.)\nFunctions (with descriptive comment after first line)\nEverything else\n\nNaming: variableName / variable.name, FunctionVerb, kConstantName. not_like_this\nIndentation: 2-space indents is nice\nSpaces are ok, but don’t go overboard. E.g., y = (a * x) + b + rnorm(1, sd=sigma)\nNever use attach()\n\n\n\n12.1.3 Save and load R objects to save time\nIf you have the result of something that took awhile to run (e.g., a big multilevel model fit to a lot of data) you can try saving it like so:\n\nmyBigThing = lm(mpg ~ disp, data=mtcars) #something slow\nsaveRDS(myBigThing, savedPath)\n\n## Later on:\nmyBigThing &lt;- readRDS(savedPath)\n\n\n\n12.1.4 Reproduce randomness with set.seed\nIf your code uses random numbers, then you should set your seed, which makes your script always generate the same sequence of random numbers.\nFor example, say your code had this:\n\ntryCatch({(1:(1:10)[rpois(1, 3)])}, error=function(e){(e)}) #works?\n\n[1] 1 2 3\n\nset.seed(97)\ntryCatch({(1:(1:10)[rpois(1, 3)])}, error=function(e){(e)}) #fails!\n\n&lt;simpleError in 1:(1:10)[rpois(1, 3)]: argument of length 0&gt;\n\n\n(Note the tryCatch() method is a way of generating errors and not crashing.)\nKey thing to know: Reproducible results help with debugging.\nIf you want to get fancy, try this (after installing the `TeachingDemos’ package):\n\nTeachingDemos::char2seed(\"quinn\") \n# Using your name as a seed says \"nothing up my sleeve\"\n\n\n\n12.1.5 Keep your files organized\nEver seen this?\n\n/My Documents\n\nmy paper.tex\nmy paper draft 2.tex\nmy paper final.tex\nmy paper final revised.tex\nmy paper final revised 2.tex\nscript.r\nscript 2.r\ndata.csv\n\n\nTry instead something like:\n\n/stat 166-Small Data Analysis\n\nstat 166.rproj\n/Empty Project\n\n/code\n/data\n/text\n/figures\nreadme.txt\n\n/HW1\n\n…\n\n\n\nYour readme.txt might have informational notes such as “Got data from bit.ly/XYZ.” to remind you of what you were up to.\nYour figures folder should be full of figures you can easily regenerate with code in your code folder.\n\n\n12.1.6 Make sure your data are numeric\nSometimes when you load data in, R does weird things like decide all your numbers are actually words. This happens if some of your entries are not numbers. Then R makes them all not numbers. You can check this with the str() function:\n\nstr( exp.dat )\n\n'data.frame':   4 obs. of  10 variables:\n $ ID    : chr  \"a\" \"b\" \"c\" \"d\"\n $ cond  : chr  \"AI\" \"DI\" \"DI\" \"AI\"\n $ trial1: chr  \"E\" \"U\" \"U\" \"E\"\n $ dec1  : num  1 1 0 1\n $ trial2: chr  \"U\" \"E\" \"U\" \"E\"\n $ dec2  : num  0 0 0 1\n $ trial3: chr  \"U\" \"E\" \"E\" \"U\"\n $ dec3  : num  0 1 0 1\n $ trial4: chr  \"E\" \"U\" \"E\" \"U\"\n $ dec4  : num  0 1 0 0\n\n\nHere we see that we have factors (categorical variables) and numbers (num). All is well.\nIf something should be a number, then change it like so:\n\nlst &lt;-  c( 1, 2, 3, \"dog\", 5, 6 )\nstr( lst )\n\n chr [1:6] \"1\" \"2\" \"3\" \"dog\" \"5\" \"6\"\n\nlst &lt;- as.numeric( lst )\n\nWarning: NAs introduced by coercion\n\nlst\n\n[1]  1  2  3 NA  5  6\n\nstr( lst )\n\n num [1:6] 1 2 3 NA 5 6\n\n\nNote it warned you that you had non-numbers when you converted. The non-numbers are now missing (NA).\nFor a dataframe, you fix like this:\n\nexp.dat$trial1 = as.numeric( exp.dat$trial1 )\n\nWarning: NAs introduced by coercion\n\n\n\n\n12.1.7 Categories should be words\nFor categorical variables, don’t use numbers, if at all possible. E.g.,\n\nlevels = c( \"Low\", \"Middle\", \"High\", NA )\n\nis better than\n\nlevels = c(1, 2, 3, 99 )",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tips, Tricks, and Debugging in R</span>"
    ]
  },
  {
    "objectID": "r_tips.html#data-wrangling",
    "href": "r_tips.html#data-wrangling",
    "title": "12  Tips, Tricks, and Debugging in R",
    "section": "12.2 Data Wrangling",
    "text": "12.2 Data Wrangling\nWe next give some high level data wrangling advice. But really, check out R for DS for much more and much better on the merging and summarizing topics.\n\n12.2.1 Handling Lagged Data\nSometimes you have multiple times for your units (think country or state), and you want to regress, say, future X on current X. Then you want to have both future and current X for each case.\nHere think of a case as a Country at a point in time. E.g., we might have data like this:\n\ndtw = read.csv( \"data/fake_country_block.csv\", as.is=TRUE )\ndt = pivot_longer( dtw, cols=X1997:X2004,\n                   names_to = \"Year\", names_prefix = \"X\",\n                   values_to = \"X\" )\ndt$Year = as.numeric( dt$Year )\nslice_sample( dt, n=5 )\n\n# A tibble: 5 × 3\n  Country  Year     X\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 China    2000   3.4\n2 England  1999  53  \n3 China    2003   6  \n4 Morocco  1997  31.9\n5 England  2003  57.3\n\n\nWe then want to know what the X will be 2 years in the future. We can do this with the following trick:\n\ndt.fut = dt\ndt.fut$Year = dt.fut$Year - 2\nhead(dt.fut)\n\n# A tibble: 6 × 3\n  Country  Year     X\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 China    1995   0.5\n2 China    1996   1  \n3 China    1997   2  \n4 China    1998   3.4\n5 China    1999   4  \n6 China    2000   5.3\n\nnewdt = left_join( dt, dt.fut, \n                   by=c(\"Country\",\"Year\"), suffix=c(\"\",\".fut\") )\nhead( newdt, 10 )\n\n# A tibble: 10 × 4\n   Country  Year     X X.fut\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 China    1997   0.5   2  \n 2 China    1998   1     3.4\n 3 China    1999   2     4  \n 4 China    2000   3.4   5.3\n 5 China    2001   4     6  \n 6 China    2002   5.3   7  \n 7 China    2003   6    NA  \n 8 China    2004   7    NA  \n 9 Morocco  1997  31.9  33  \n10 Morocco  1998  32    34  \n\n\nHere we are merging records that match Country and Year.\nNote that for the final two China entries, we don’t have a future X value. The merge will make it NA indicating it is missing.\nHow this works: we are tricking the program. We are making a new \\verb|dt.lag| data.frame and then putting all the entries into the past by two years. When we merge, and match on Country and Year, the current dataframe and the lagged dataframe get lined up by this shift. Clever, no?\nNow we could do regression:\n\nmy.lm = lm( X.fut ~ X + Country, data=newdt )\nsummary( my.lm )\n\n\nCall:\nlm(formula = X.fut ~ X + Country, data = newdt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5869 -0.2610  0.0107  0.2753  0.5137 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.8684     0.2128    8.78  2.7e-06 ***\nX                1.0179     0.0582   17.48  2.3e-09 ***\nCountryEngland  -0.8259     2.9704   -0.28     0.79    \nCountryMorocco  -0.7514     1.7603   -0.43     0.68    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.351 on 11 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:     1,  Adjusted R-squared:     1 \nF-statistic: 2.13e+04 on 3 and 11 DF,  p-value: &lt;2e-16\n\n\n\n\n\n12.2.2 Quick overview of merging data\nOften you have two datasets that you want to merge. For example, say you want to merge some data you have on a few states with some SAT information from the mosaic package.\n\nlibrary( mosaicData )\ndata( SAT )\nhead( SAT )\n\n       state expend ratio salary frac verbal math  sat\n1    Alabama   4.41  17.2   31.1    8    491  538 1029\n2     Alaska   8.96  17.6   48.0   47    445  489  934\n3    Arizona   4.78  19.3   32.2   27    448  496  944\n4   Arkansas   4.46  17.1   28.9    6    482  523 1005\n5 California   4.99  24.0   41.1   45    417  485  902\n6   Colorado   5.44  18.4   34.6   29    462  518  980\n\ndf = data.frame( state=c(\"Alabama\",\"California\",\"Fakus\"), \n                A=c(10,20,50), \n                frac=c(0.5, 0.3, 0.4) )\ndf\n\n       state  A frac\n1    Alabama 10  0.5\n2 California 20  0.3\n3      Fakus 50  0.4\n\nmerge( df, SAT, by=\"state\", all.x=TRUE )\n\n       state  A frac.x expend ratio salary frac.y verbal math  sat\n1    Alabama 10    0.5   4.41  17.2   31.1      8    491  538 1029\n2 California 20    0.3   4.99  24.0   41.1     45    417  485  902\n3      Fakus 50    0.4     NA    NA     NA     NA     NA   NA   NA\n\n\nThe records are combined by the “by” variable. I.e., each record in df is matched with each record in SAT with the same value of “state.”\nThings to note: If you have the same variable in each dataframe, it will keep both, and add a suffix of “.x” and “.y” to indicate where they came from.\nThe “all.x” means keep all records from your first dataframe (here df) even if there is no match. If you added “all.y=TRUE” then you would get all 50 states from the SAT dataframe even though df doesn’t have most of them. Try it!\nYou can merge on more than one variable. I.e., if you said \\verb|by=c(“A”,“B”)| then it would match records if they had the same value for both A and B. See below for an example on this.\n\n\n12.2.3 Summarizing/aggregating Data\nSometimes you want to collapse several cases into one. This is called aggregating. If you install a package called “dplyr” (Run install.packages( \"dplyr\" ) once to install, or better yet simply install tidyverse) then you will have great power.\nUsing newdt from above, we can summarize countries across all their time points:\n\nnewdt %&gt;% group_by( Country ) %&gt;% \n    summarise( mean.X = mean(X, na.rm=TRUE ),\n        sd.X = sd( X, na.rm=TRUE ) )\n\n# A tibble: 3 × 3\n  Country mean.X  sd.X\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 China     3.65  2.37\n2 England  54.6   2.43\n3 Morocco  34.0   2.12\n\n\nYou can also augment data by adding new variables. You can even do this within groups. Here we subtract the mean from each group:\n\ndshift = newdt %&gt;% group_by( Country ) %&gt;%\n    mutate( Xm = mean(X, na.rm=TRUE),\n            Xc = X - mean(X, na.rm=TRUE ) )\nhead(dshift)\n\n# A tibble: 6 × 6\n# Groups:   Country [1]\n  Country  Year     X X.fut    Xm    Xc\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 China    1997   0.5   2    3.65 -3.15\n2 China    1998   1     3.4  3.65 -2.65\n3 China    1999   2     4    3.65 -1.65\n4 China    2000   3.4   5.3  3.65 -0.25\n5 China    2001   4     6    3.65  0.35\n6 China    2002   5.3   7    3.65  1.65\n\n\n\n\n12.2.4 Making Data Frames on the fly\nFor small datasets, you can type in data the hard way like so:\n\nexp.dat = data.frame( ID=c(\"a\",\"b\",\"c\",\"d\"), \n      cond = c(\"AI\",\"DI\",\"DI\",\"AI\"),\n            trial1 = c(\"E\",\"U\",\"U\",\"E\"),\n            dec1 = c(1,1,0,1),\n            trial2 = c(\"U\",\"E\",\"U\",\"E\"),\n            dec2 = c(0,0,0,1),\n                trial3 = c(\"U\",\"E\",\"E\",\"U\"),\n            dec3 = c(0,1,0,1),\n                trial4 = c(\"E\",\"U\",\"E\",\"U\"),\n            dec4 = c(0,1,0,0) )\nexp.dat  \n\n  ID cond trial1 dec1 trial2 dec2 trial3 dec3 trial4 dec4\n1  a   AI      E    1      U    0      U    0      E    0\n2  b   DI      U    1      E    0      E    1      U    1\n3  c   DI      U    0      U    0      E    0      E    0\n4  d   AI      E    1      E    1      U    1      U    0\n\n\nThis is for an experiment on 4 subjects. The first and forth subject got the AI treatment, the second two got the DI treatment. The subjects then had 4 trials each, and they received a “E” choice or a “U” choice, and the decision variable is whether they accepted the choice.\nAs you can see, data can get a bit complicated!",
    "crumbs": [
      "R & R MARKDOWN",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tips, Tricks, and Debugging in R</span>"
    ]
  },
  {
    "objectID": "intro_ggplot.html",
    "href": "intro_ggplot.html",
    "title": "13  Intro to ggplot",
    "section": "",
    "text": "13.1 Summarizing\nYou can also automatically add various statistical summaries, such as simple regression lines:\nggplot( dat, aes(y=mathach, x=ses, col=sector ) ) + \n    geom_point() + \n    stat_smooth( method=\"lm\", se = FALSE )\nNotice how it automatically realized you have two subgroups of data defined by sector. It gives you a regression line for each group.\nThe elements of the plot are stacked, and if you remove one of the elements, it will not appear:\nggplot( dat, aes(y=mathach, x=ses, col=sector ) ) + \n  stat_smooth( method=\"lm\" )\nHere we also added some uncertainty bars around the regression lines by not saying se = FALSE. (Including uncertainty is the default; this uncertainty is not to be trusted, especially in this course, as it is not taking clustering into account.)",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to `ggplot`</span>"
    ]
  },
  {
    "objectID": "intro_ggplot.html#grouping",
    "href": "intro_ggplot.html#grouping",
    "title": "13  Intro to ggplot",
    "section": "13.2 Grouping",
    "text": "13.2 Grouping\nCombining these ideas we can make a trend line for each school:\n\nmy.plot = ggplot( dat, aes(y=mathach, x=ses, col=sector, group=schoolid ) ) + \n    stat_smooth( method=\"lm\", alpha=0.5, se = FALSE )\n\nmy.plot\n\n\n\n\n\n\n\n\nThe trendlines automatically extend to the limits of the data they are run on, hence the different lengths.\nAlso, notice we “saved” the plot in the variable my.plot. Only when we “print” the plot will the plot appear on your display. When we type the name of a variable, it prints. Once you have a plot stored in a variable you can augment it very easily.\nAs you may now realize, ggplot2 is very, very powerful.",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to `ggplot`</span>"
    ]
  },
  {
    "objectID": "intro_ggplot.html#customization",
    "href": "intro_ggplot.html#customization",
    "title": "13  Intro to ggplot",
    "section": "13.3 Customization",
    "text": "13.3 Customization\nWe next show some other things you can do. For example, you can make lots of little plots:\n\nmy.plot + \n  facet_grid( ~ female ) + \n    ggtitle(\"School-level trend lines for their male and female students\") +\n    labs(x=\"SES\",y=\"Math Achievement\") \n\n\n\n\n\n\n\n\nOr,\n\n# random subset of schoolid\nsch &lt;- sample( unique( dat$schoolid ), 6 )\n\n# pipe into ggplot \nsch.six &lt;- dat |&gt; \n  filter(schoolid %in% sch)\n\nmy.six.plot &lt;- ggplot( sch.six, aes(y=mathach, x=ses, col=sector ) ) + \n    facet_wrap( ~ schoolid, ncol=3 ) + \n    geom_point() + stat_smooth( method=\"lm\" )\n\nmy.six.plot\n\n\n\n\n\n\n\n\nAlso shown in the above are adding titles.",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to `ggplot`</span>"
    ]
  },
  {
    "objectID": "intro_ggplot.html#themes",
    "href": "intro_ggplot.html#themes",
    "title": "13  Intro to ggplot",
    "section": "13.4 Themes",
    "text": "13.4 Themes\nYou can very quickly change the entire presentation of your plot using themes. There are pre-packaged ones, and you can make your own that you use over and over. Here we set up a theme to be used moving forward\n\nlibrary( ggthemes )\nmy_t = theme_calc() + theme( legend.position=\"bottom\", \n                             legend.direction=\"horizontal\", \n                             legend.key.width=unit(1,\"cm\")  )\ntheme_set( my_t )\n\nCompare the same plot from above, now with a new theme.\n\nmy.six.plot\n\n\n\n\n\n\n\n\nCool, no?",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to `ggplot`</span>"
    ]
  },
  {
    "objectID": "intro_ggplot.html#next-steps",
    "href": "intro_ggplot.html#next-steps",
    "title": "13  Intro to ggplot",
    "section": "13.5 Next steps",
    "text": "13.5 Next steps\nThere is a lot of information out there on ggplot and my best advice is to find code examples, and then modify them as needed. There are tutorials and blogs that walk through building plots (search for “ggplot tutorial” for example), but seeing examples seems to be the best way to learn the stuff. For example, you could use the above code for your project one quite readily. And don’t be afraid to ask how to modify plots on Piazza!\nIn particular, check out the excellent “R for Data Science’’ textbook. It extensively uses ggplot, starting here.",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to `ggplot`</span>"
    ]
  },
  {
    "objectID": "plot_expand_grid.html",
    "href": "plot_expand_grid.html",
    "title": "14  Example of making plots with expand.grid",
    "section": "",
    "text": "14.1 Making plots for the HS&B Dataset\nIn this section we first look at how to plot the model results by making a tiny dataset from the fixed effects, and then we extend to more powerful plotting of individual schools.",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Example of making plots with `expand.grid`</span>"
    ]
  },
  {
    "objectID": "plot_expand_grid.html#making-plots-for-the-hsb-dataset",
    "href": "plot_expand_grid.html#making-plots-for-the-hsb-dataset",
    "title": "14  Example of making plots with expand.grid",
    "section": "",
    "text": "14.1.1 Setting up the HS&B data\nThe “many small worlds” view says each school has its own regression line. We are going to plot them all. See the lecture code files for how to load the HS&B dataset. For clarity it is omitted from the printout. We end up with this for the schools:\n\nhead( sdat )\n\n    id size   sector meanses\n1 1224  842   public  -0.428\n2 1288 1855   public   0.128\n3 1296 1719   public  -0.420\n4 1308  716 catholic   0.534\n5 1317  455 catholic   0.351\n6 1358 1430   public  -0.014\n\n\nand this for students (we merged in the school info already):\n\nhead( dat )\n\n    id minority female    ses mathach size sector meanses\n1 1224        0      1 -1.528   5.876  842 public  -0.428\n2 1224        0      1 -0.588  19.708  842 public  -0.428\n3 1224        0      0 -0.528  20.349  842 public  -0.428\n4 1224        0      0 -0.668   8.781  842 public  -0.428\n5 1224        0      0 -0.158  17.898  842 public  -0.428\n6 1224        0      0  0.022   4.583  842 public  -0.428\n\n\nWe fit a fancy random slopes model with 2nd level covariates that impact both the overall school means and the ses by math achievment slopes. Our model is \\[\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} +  \\epsilon_{ij} \\\\\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} sector_j + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} sector_j + u_{1j}\n\\end{aligned}\n\\] We omit the equations for the random effect distributions. The \\(\\epsilon_{ij}\\) are normal, and the \\((u_{0j},u_{1j})\\) are bivariate normal, as usual. We fit the model as so:\n\nM1 = lmer( mathach ~ 1 + ses*sector + (1 + ses|id), data=dat )\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00578927 (tol = 0.002, component 1)\n\ndisplay( M1 )\n\nlmer(formula = mathach ~ 1 + ses * sector + (1 + ses | id), data = dat)\n                   coef.est coef.se\n(Intercept)        11.75     0.23  \nses                 2.96     0.14  \nsectorcatholic      2.13     0.35  \nses:sectorcatholic -1.31     0.22  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 1.95          \n          ses         0.28     1.00 \n Residual             6.07          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46585.1, DIC = 46557.2\ndeviance = 46563.2 \n\n\n\n\n14.1.2 Plotting the model results\nWe can plot the model results by making a little dataset by hand. This section of the handout illustrates how you can hand-construct plots by directly calculating predicted values from your model. This is a very useful skill, and we recommend studying this area of the handout as a way of learning how to control plotting at a very direct level.\nSo, to continue, we proceed in three steps.\nStep 1: Decide on the plot. Let’s make a plot of outcome vs. ses with two lines (one for catholic and one for public). Sometimes it is worth actually sketching the desired plot on scratch paper, identifying the x and y axes and general lines desired.\nStep 2: calculate some outcomes using our model. We do this by deciding what values we want to plot, and then making the outcome.\n\nquantile( dat$ses, c( 0.05, 0.95 ) )\n\n    5%    95% \n-1.318  1.212 \n\nplt = data.frame( ses = c(-1.5, 1.25, -1.5, 1.25 ),\n                  catholic = c( 0, 0, 1, 1 ) )\ncf = fixef( M1 )\ncf\n\n       (Intercept)                ses     sectorcatholic ses:sectorcatholic \n         11.751789           2.957538           2.129531          -1.313363 \n\nplt = mutate( plt,\n              Y = cf[[1]] + cf[[2]]*ses + cf[[3]]*catholic + cf[[4]]*ses*catholic )\nplt\n\n    ses catholic         Y\n1 -1.50        0  7.315482\n2  1.25        0 15.448711\n3 -1.50        1 11.415057\n4  1.25        1 15.936538\n\n\nNote that we have made a little mini-dataset with just the points we want to put on our plot. We calculated these points “by hand”. There is no shame in this.\nStep 3: plot. We plot using ggplot:\n\nplt$catholic = factor( plt$catholic, \n                       labels=c(\"public\",\"catholic\"),\n                       levels=c(0,1) )\nggplot( plt, aes( ses, Y, col=catholic ) ) +\n    geom_line()\n\n\n\n\n\n\n\n\n\n14.1.2.1 A fancy diversion: categorical variables on the \\(x\\)-axis\nSay we decided to fit a model where we have ses categories:\n\ndat$ses.cat = cut( dat$ses, \n                   breaks=quantile( dat$ses, c( 0, 0.33, 0.67, 1 ) ),\n                   labels = c( \"low\",\"mid\",\"high\"),\n                   include.lowest = TRUE )\ntable( dat$ses.cat )\n\n\n low  mid high \n2371 2462 2352 \n\nM1b = lmer( mathach ~ 1 + ses.cat*sector + (1 + ses|id), data=dat )\ndisplay( M1b )\n\nlmer(formula = mathach ~ 1 + ses.cat * sector + (1 + ses | id), \n    data = dat)\n                           coef.est coef.se\n(Intercept)                 9.19     0.27  \nses.catmid                  2.28     0.25  \nses.cathigh                 5.07     0.29  \nsectorcatholic              3.44     0.42  \nses.catmid:sectorcatholic  -0.98     0.38  \nses.cathigh:sectorcatholic -2.47     0.42  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 2.05          \n          ses         0.47     0.23 \n Residual             6.10          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46691.5, DIC = 46660.7\ndeviance = 46666.1 \n\n\nMake our outcomes:\n\nplt = data.frame( ses.mid = c( 0, 1, 0, 0, 1, 0 ),\n                  ses.high = c( 0, 0, 1, 0, 0, 1 ),\n                  catholic = c( 0, 0, 0, 1, 1, 1 ) )\ncf = fixef( M1b )\ncf\n\n               (Intercept)                 ses.catmid \n                 9.1915044                  2.2808807 \n               ses.cathigh             sectorcatholic \n                 5.0721921                  3.4398984 \n ses.catmid:sectorcatholic ses.cathigh:sectorcatholic \n                -0.9759927                 -2.4707460 \n\nplt = mutate( plt,\n              Y = cf[[1]] + cf[[2]]*ses.mid + cf[[3]]*ses.high +\n                cf[[4]]*catholic + cf[[5]]*ses.mid*catholic + cf[[6]]*ses.high*catholic )\nplt\n\n  ses.mid ses.high catholic         Y\n1       0        0        0  9.191504\n2       1        0        0 11.472385\n3       0        1        0 14.263697\n4       0        0        1 12.631403\n5       1        0        1 13.936291\n6       0        1        1 15.232849\n\n\nAnd plot\n\nplt$catholic = factor( plt$catholic, \n                       labels=c(\"public\",\"catholic\"),\n                       levels=c(0,1) )\nplt$ses = \"low\"\nplt$ses[plt$ses.mid==1] = \"mid\"\nplt$ses[plt$ses.high==1] = \"high\"\nplt$ses = factor( plt$ses, levels=c(\"low\",\"mid\",\"high\") )\nggplot( plt, aes( ses, Y, col=catholic, group=catholic ) ) +\n    geom_line() + geom_point()\n\n\n\n\n\n\n\n\nNote the very important group=catholic line that tells the plot to group everyone by catholic. If not, it will get confused and note that since ses is categorical, try to group on that. Then it cannot make a line since each group has only a single point.\n\n\n\n14.1.3 Plotting individual school regression lines\nWe can plot the individual lines by hand-calculating the school level slopes and intercepts. This code shows how:\n\ncoefs = coef( M1 )$id\nhead( coefs )\n\n     (Intercept)      ses sectorcatholic ses:sectorcatholic\n1224   11.084408 2.863501       2.129531          -1.313363\n1288   12.761032 3.099743       2.129531          -1.313363\n1296    9.193415 2.597052       2.129531          -1.313363\n1308   12.709882 3.092535       2.129531          -1.313363\n1317   10.719013 2.812016       2.129531          -1.313363\n1358   11.478455 2.919031       2.129531          -1.313363\n\ncoefs = rename( coefs, \n                gamma.00 = `(Intercept)`,\n                gamma.10 = `ses`,\n                gamma.01 = `sectorcatholic`,\n                gamma.11 = `ses:sectorcatholic` )\ncoefs$id = rownames( coefs )\ncoefs = merge( coefs, sdat, by=\"id\" )\ncoefs = mutate( coefs,\n                beta.0 = gamma.00 + gamma.01 * (sector==\"catholic\"),\n                beta.1 = gamma.10 + gamma.11 * (sector==\"catholic\") )\n\nNote how we have to add up our gammas to get our betas for each school. See our final betas, one set for each school:\n\nhead( dplyr::select( coefs, -gamma.00, -gamma.10, -gamma.01, -gamma.11 ) )\n\n    id size   sector meanses    beta.0   beta.1\n1 1224  842   public  -0.428 11.084408 2.863501\n2 1288 1855   public   0.128 12.761032 3.099743\n3 1296 1719   public  -0.420  9.193415 2.597052\n4 1308  716 catholic   0.534 14.839413 1.779172\n5 1317  455 catholic   0.351 12.848543 1.498653\n6 1358 1430   public  -0.014 11.478455 2.919031\n\n\nNow let’s plot a subsample of 20 schools\n\nset.seed( 102030 )\nsub20 = sample( unique( dat$id ), 20 )\n\ncoefs.20 = filter( coefs, id %in% sub20 )\n\nggplot( coefs.20, aes( group=id ) ) +\n  geom_abline( aes( slope=beta.1, intercept=beta.0, col=sector) ) +\n  coord_cartesian( xlim=c(-2.5,2), ylim=range(dat$mathach) )\n\n\n\n\n\n\n\n\nCommentary: We need to specify the size of the plot since we have no data, just the intercepts and slopes. We are using the Emperical Bayes estimates of the random effects added to our school level fixed effects to get the \\(\\hat{\\beta}_{0j}, \\hat{\\beta}_{1j}\\) which define the school-specific regression line for school \\(j\\).\nOur two types of school are clearly separated. Catholic schools have higher average performance, and less of a ses-achievement relationship. Since we have merged in our school level data, we can color the lines by catholic vs public, making our plot easier to read.\n\n\n14.1.4 Plotting with predict()\nA more general plotitng approach is to plot using predict(), where for each student we predict the outcome.\n\ndat$math.hat = predict( M1 )\n\nNow let’s plot a subsample of 20 schools\n\ndat.20 = filter( dat, id %in% sub20 )\n\nggplot( dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line()\n\n\n\n\n\n\n\n\nBut look at how the lines don’t go the full distance. What ggplot is doing is plotting the individual students, and connecting them with a line. We can see this by plotting the students as well, like this:\n\nggplot( dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n\nWe have a predicted outcome for each student, which removes the student residual, giving just the school trends. If we don’t have students for some range of ses for a school, we won’t have points in our plot for that range for that school. The lines thus give the ranges (left to right) of the ses values in each school.\n\n\n14.1.5 Making our lines go the same length with expand.grid()\nThe way we fix this is we, for each school, make a bunch of fake students with different SES and predict along all those fake students. This will give us equally spaced lines.\nThat being said: the shorter lines above are also informative, as they give you a sense of what the range of ses for each school actually is. Which approach is somewhat a matter of taste.\nWe can generate fake children of each group for each school using expand.grid(). This method will generate a dataframe with all combinations of the given variables supplied. Here we make all combinations of ses, for a set of ses values, and school id.\n\nsynth.dat = expand_grid( id = unique( dat$id ),\n                         ses = seq( -2.5, 2, length.out=9 ) )\nhead( synth.dat )\n\n# A tibble: 6 × 2\n  id       ses\n  &lt;chr&gt;  &lt;dbl&gt;\n1 1224  -2.5  \n2 1224  -1.94 \n3 1224  -1.38 \n4 1224  -0.812\n5 1224  -0.25 \n6 1224   0.312\n\n\nThe seq() command makes an evenly spaced sequence of numbers going from the first to the last, with 9 numbers. E.g.,\n\nseq( 1, 10, length.out=4 )\n\n[1]  1  4  7 10\n\n\nWe then merge our school info back in to get sector for each school id:\n\nsynth.dat = merge( synth.dat, sdat, by=\"id\", all.x=TRUE )\n\nWe finally predict for each school, predicting outcome for our fake kids in each school.\n\nsynth.dat$math.hat = predict( M1, newdata=synth.dat )\n\nWe have predictions just as above, just for students that we set for each school. The school random effects and everything remain because we are using the original school ids.\nUsing our new data, plot 20 random schools–this code is the same as in the prior subsection.\n\nsynth.dat.20 = filter( synth.dat, id %in% sub20 )\n\nggplot( synth.dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line()\n\n\n\n\n\n\n\n\nBut see our equally spaced students?\n\nggplot( synth.dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n\nWhy do this? The predict() approach allows us to avoid working with the gammas and adding them up like we did above. This is a flexible and powerful approach that avoids a lot of work in many cases. In the next section we illustrate by fitting curves rather than lines. This would be very hard to do directly.\n\n\n14.1.6 Superfancy extra bonus plotting of complex models!\nWe can use predict for weird nonlinear relationships also. This will be important for longitudinal data. To illustrate we fit a model that allows a quadradic relationship between ses and math achievement.\n\ndat$ses2 = dat$ses^2\nM2 = lmer( mathach ~ 1 + (ses + ses2)*sector + meanses + (1 + ses|id), data=dat )\n\ndisplay( M2 )\n\nlmer(formula = mathach ~ 1 + (ses + ses2) * sector + meanses + \n    (1 + ses | id), data = dat)\n                    coef.est coef.se\n(Intercept)         12.17     0.21  \nses                  2.79     0.15  \nses2                 0.04     0.13  \nsectorcatholic       1.23     0.33  \nmeanses              3.14     0.38  \nses:sectorcatholic  -1.35     0.22  \nses2:sectorcatholic  0.06     0.21  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 1.53          \n          ses         0.23     0.49 \n Residual             6.07          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46539.7, DIC = 46495.9\ndeviance = 46506.8 \n\n\nTo fit a quadratic model we need our quadratic ses term, which we make by hand. We could also have used I(ses^2) in the lmer() command directly, but people don’t tend to find that easy to read.\nAnd here we predict and plot:\n\nsynth.dat = expand.grid( id = unique( dat$id ),\n                         ses= seq( -2.5, 2, length.out=9 ) )\nsynth.dat$ses2 = synth.dat$ses^2\nsynth.dat = merge( synth.dat, sdat, by=\"id\", all.x=TRUE )\n\nNote how we make our ses2 variable out of ses just like we did above.\n\nsynth.dat$math.hat = predict( M2, newdata=synth.dat )\n\nsynth.dat.20 = filter( synth.dat, id %in% sub20 )\n\nggplot( synth.dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line()\n\n\n\n\n\n\n\n\nThis code is the same as above. The prediction handles all our model complexity for us.\nAgain, we have our equally spaced students:\n\nggplot( synth.dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line() +\n  geom_point()",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Example of making plots with `expand.grid`</span>"
    ]
  },
  {
    "objectID": "plot_expand_grid.html#longitudinal-data",
    "href": "plot_expand_grid.html#longitudinal-data",
    "title": "14  Example of making plots with expand.grid",
    "section": "14.2 Longitudinal Data",
    "text": "14.2 Longitudinal Data\nWe next do the above, but for longitudinal data. The story is basically the same.\n\n14.2.1 The data\nWe use the “US Sustaining Effects Study” taken from Raudenbush and Bryk (we have not seen these data in class). We have kids in grades nested in schools. So longitudinal data with a clustering on top of that.\n\nhead( dat )\n\n       CHILDID     SCHOOLID YEAR GRADE   MATH FEMALE SIZE RACEETH\n1 101480302    3440         -0.5     1 -1.694      1  588   black\n2 101480302    3440          0.5     2 -0.211      1  588   black\n3 101480302    3440          1.5     3 -0.403      1  588   black\n4 101480302    3440          2.5     4  0.501      1  588   black\n5 173559292    2820         -0.5     1 -0.194      0  678   white\n6 173559292    2820          0.5     2  2.140      0  678   white\n\n\n\n\n14.2.2 A model\nWe will be using the following 3-level quadradic growth model:\n\nM4 = lmer( MATH ~ 1 + (YEAR + I(YEAR^2)) * (FEMALE * RACEETH ) + \n                (YEAR|CHILDID:SCHOOLID) + (YEAR|SCHOOLID), data=dat )\ndisplay( M4 )\n\nlmer(formula = MATH ~ 1 + (YEAR + I(YEAR^2)) * (FEMALE * RACEETH) + \n    (YEAR | CHILDID:SCHOOLID) + (YEAR | SCHOOLID), data = dat)\n                                 coef.est coef.se\n(Intercept)                      -0.90     0.06  \nYEAR                              0.76     0.02  \nI(YEAR^2)                        -0.04     0.01  \nFEMALE                            0.02     0.05  \nRACEETHhispanic                   0.23     0.10  \nRACEETHwhite                      0.79     0.10  \nFEMALE:RACEETHhispanic           -0.01     0.12  \nFEMALE:RACEETHwhite              -0.34     0.12  \nYEAR:FEMALE                       0.01     0.02  \nYEAR:RACEETHhispanic              0.10     0.03  \nYEAR:RACEETHwhite                 0.07     0.03  \nI(YEAR^2):FEMALE                  0.01     0.01  \nI(YEAR^2):RACEETHhispanic        -0.01     0.01  \nI(YEAR^2):RACEETHwhite           -0.02     0.01  \nYEAR:FEMALE:RACEETHhispanic      -0.01     0.04  \nYEAR:FEMALE:RACEETHwhite         -0.02     0.04  \nI(YEAR^2):FEMALE:RACEETHhispanic  0.00     0.02  \nI(YEAR^2):FEMALE:RACEETHwhite     0.02     0.02  \n\nError terms:\n Groups           Name        Std.Dev. Corr \n CHILDID:SCHOOLID (Intercept) 0.79          \n                  YEAR        0.11     0.55 \n SCHOOLID         (Intercept) 0.34          \n                  YEAR        0.10     0.31 \n Residual                     0.54          \n---\nnumber of obs: 7230, groups: CHILDID:SCHOOLID, 1721; SCHOOLID, 60\nAIC = 16259.7, DIC = 16009.6\ndeviance = 16109.7 \n\n\nWe are just taking the model as given; this document is about showing the fit of this model. In particular, if you haven’t seen 3-level models before, just consider the above as some complex model; the nice thing about predict() is you don’t even need to understand the model you are using! Note we do have a lot of fixed effect interaction terms, allowing for systematically different trajectories for groups of kids that are grouped on recorded race and gender.\n\n\n14.2.3 The simple predict() approach\nWe can use our model to predict outcomes for each timepoint in the data. This will smooth out the time to time variation.\n\ndat$Yhat = predict( M4 )\nggplot( dat, aes( YEAR, Yhat, group=CHILDID ) ) +\n  facet_grid( RACEETH ~ FEMALE ) +\n  geom_line( alpha=0.25 )\n\n\n\n\n\n\n\n\nNote how the growth lines don’t go across all years for all kids. This is because we were missing data for those kids in the original dataset at those timepoints, so we didn’t predict outcomes when we used the predict() function, above.\nTo fix this we will add in those missing timepoints so we get predictions for all kids for all timepoints.\n\n\n14.2.4 The expand.grid() function\nWe now want different trajectories for the different groups. We can generate fake children of each group for each school using expand.grid(). This method will generate a dataframe with all combinations of the given variables supplied. Here we make all combinations of year, gender, and race/ethnic group for each school.\n\nsynth.dat = expand.grid( CHILDID = -1,\n                         SCHOOLID = levels( dat$SCHOOLID ),\n                         YEAR = unique( dat$YEAR ),\n                         FEMALE = c( 0, 1 ),\n                         RACEETH = levels( dat$RACEETH ) )\nhead( synth.dat )\n\n  CHILDID     SCHOOLID YEAR FEMALE RACEETH\n1      -1 2020         -0.5      0   black\n2      -1 2040         -0.5      0   black\n3      -1 2180         -0.5      0   black\n4      -1 2330         -0.5      0   black\n5      -1 2340         -0.5      0   black\n6      -1 2380         -0.5      0   black\n\nnrow( synth.dat )\n\n[1] 2160\n\n\nThe CHILDID = -1 line means we are making up a new child (not using one of the real ones) so the child random effects will be set to 0 in the predictions.\nOnce we have our dataset, we use predict to calculate the predicted outcomes for each student type for each year timepoint for each school:\n\nsynth.dat = mutate( synth.dat, MATH = predict( M4, \n                                               newdata=synth.dat,\n                                               allow.new.levels = TRUE) )\n\nNow we can plot with our new predictions\n\nggplot( synth.dat, aes( YEAR, MATH, group=SCHOOLID ) ) +\n  facet_grid( RACEETH ~ FEMALE ) +\n  geom_line( alpha=0.5 )\n\n\n\n\n\n\n\n\nHere we are seeing the different school trajectories for the six types of kid defined by our student-level demographics.\nOr, for a subset of schools\n\nsynth.dat = mutate( synth.dat, GENDER = ifelse( FEMALE, \"female\", \"male\" ) )\nkeepers = sample( unique( synth.dat$SCHOOLID ), 12 )\ns2 = filter( synth.dat, SCHOOLID %in% keepers )\nggplot( s2, aes( YEAR, MATH, col=RACEETH, lty=GENDER ) ) +\n  facet_wrap( ~ SCHOOLID ) +\n  geom_line( alpha=0.5) + geom_point( alpha=0.5 )\n\n\n\n\n\n\n\n\nHere we see the six lines for the six groups within each school, plotted in little tiles, one for each school.\n\n\n14.2.5 Population aggregation\nYou can also aggregate these predictions. This is the easiest way to get what collection of schools, averaging over their random effects, looks like.\nAggregate with the group_by() and the summarise() methods:\n\nagg.dat = synth.dat %&gt;% group_by( GENDER, RACEETH, YEAR ) %&gt;%\n  dplyr::summarise( MATH = mean( MATH ) )\n\n`summarise()` has grouped output by 'GENDER', 'RACEETH'. You can override using\nthe `.groups` argument.\n\nggplot( agg.dat, aes( YEAR, MATH, col=RACEETH, lty=GENDER ) ) +\n  geom_line( alpha=0.5) + geom_point( alpha=0.5 )\n\n\n\n\n\n\n\n\nOr do this via predict directly, using the prior ideas\n\nsynth.dat.agg = expand.grid( CHILDID = -1,\n                             SCHOOLID = -1,\n                             YEAR = unique( dat$YEAR ),\n                             FEMALE = c( 0, 1 ),\n                             RACEETH = levels( dat$RACEETH ) )\nnrow( synth.dat.agg )\n\n[1] 36\n\nsynth.dat.agg = mutate( synth.dat.agg, \n                        MATH = predict( M4, \n                                        newdata=synth.dat.agg,\n                                        allow.new.levels = TRUE) )\nsynth.dat.agg = mutate( synth.dat.agg, GENDER = ifelse( FEMALE, \"female\", \"male\" ) )\n\nggplot( synth.dat.agg, aes( YEAR, MATH, col=RACEETH, lty=GENDER ) ) +\n  geom_line( alpha=0.5) + geom_point( alpha=0.5 )\n\n\n\n\n\n\n\n\nThe above plot suggests that the gender gap only exists for the white children. It also shows that there are racial gaps, and that the Black children appear to be falling further behind as time passes.\nThis block of code is stand-alone, showing the making of fake data and plotting of predictions all in one go. Especially for glms, where there are nonlinearities due to the link function, this will give you the “typical” units, whereas the aggregation method will average over your individuals in the sample.\nFinally, we can also make tables to calculate observed gaps (although in many cases you can just read this sort of thing off the regression table). First spread our data to get columns for each race\n\ns3 = spread( synth.dat.agg, key=\"RACEETH\", value=\"MATH\" )\nhead( s3 )\n\n  CHILDID SCHOOLID YEAR FEMALE GENDER     black  hispanic      white\n1      -1       -1 -2.5      0   male -3.062596 -3.140761 -2.5729365\n2      -1       -1 -2.5      1 female -3.022565 -3.090729 -2.7217908\n3      -1       -1 -1.5      0   male -2.129829 -2.071721 -1.4888251\n4      -1       -1 -1.5      1 female -2.110195 -2.048890 -1.7416637\n5      -1       -1 -0.5      0   male -1.284951 -1.107971 -0.5317704\n6      -1       -1 -0.5      1 female -1.268488 -1.096511 -0.8412431\n\n\nThen summarise:\ntab = s3 %&gt;% group_by( YEAR ) %&gt;% \n  summarise( gap.black.white = mean( white ) - mean( black ),\n             gap.hispanic.white = mean( white ) - mean( hispanic ),\n             gap.black.hispanic = mean( hispanic ) - mean( black ) )\nknitr::kable( tab, digits=2 )\n\n\n\nYEAR\ngap.black.white\ngap.hispanic.white\ngap.black.hispanic\n\n\n\n\n-2.5\n0.40\n0.47\n-0.07\n\n\n-1.5\n0.50\n0.45\n0.06\n\n\n-0.5\n0.59\n0.42\n0.17\n\n\n0.5\n0.65\n0.38\n0.27\n\n\n1.5\n0.69\n0.34\n0.35\n\n\n2.5\n0.70\n0.29\n0.41\n\n\n\nThis again shows widening gap between Black and White students, and the closing gap of Hispanic and White students.\n\n\n14.2.6 Plotting random effects by Level 2 variable\nYou can also look at estimated random effects as a function of level 2 variables. For example, we can see if there is a pattern of average math score for students by year.\n\nranef = ranef( M4 )$SCHOOLID\nranef$SCHOOLID = rownames( ranef )\nschools = dat %&gt;% group_by( SCHOOLID ) %&gt;%\n  summarise( n = n(),\n             size = SIZE[[1]] )\nschools = merge( schools, ranef, by=\"SCHOOLID\" )\nhead( schools )\n\n      SCHOOLID   n size (Intercept)        YEAR\n1 2020          97  380  0.40323536  0.15256665\n2 2040          89  502  0.11548968  0.07546919\n3 2180         168  777 -0.08149782 -0.08226312\n4 2330         150  800  0.32372367 -0.04388941\n5 2340         220 1133 -0.05151408 -0.01128082\n6 2380          87  439 -0.17019248  0.10802309\n\nggplot( schools, aes( size, `(Intercept)` ) ) +\n  geom_point() +\n  geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe see a possible negative trend.",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Example of making plots with `expand.grid`</span>"
    ]
  },
  {
    "objectID": "ggeffects.html",
    "href": "ggeffects.html",
    "title": "15  Easy viz for multilevel models with ggeffects",
    "section": "",
    "text": "15.1 Graph the Results with ggeffects\nIf we just call ggeffect on the model object, we get a bunch of predicted values:\nggeffect(m1)\n\n$ses\n# Predicted values of mathach\n\nses | Predicted |       95% CI\n------------------------------\n -4 |      3.10 |  2.19,  4.00\n -3 |      5.49 |  4.77,  6.21\n -2 |      7.88 |  7.32,  8.43\n -1 |     10.27 |  9.85, 10.69\n  0 |     12.66 | 12.29, 13.03\n  1 |     15.05 | 14.62, 15.47\n  2 |     17.44 | 16.88, 17.99\n  3 |     19.83 | 19.10, 20.55\n\n\nattr(,\"class\")\n[1] \"ggalleffects\" \"list\"        \nattr(,\"model.name\")\n[1] \"m1\"\nWe can pipe that into plot to get a nice plot:\nggeffect(m1) |&gt; \n  plot()\nWith multiple covariates, we can use the terms argument, which allows us to aggregate our data within different groups. The first term will be our \\(x\\)-axis, the second will get mapped to color, the third to facet. This makes visualizing our interactions super easy! Any covariates included in the model but not included in terms are held constant at their means.\nggeffect(m2, terms = c(\"ses\", \"sector\")) |&gt; \n  plot(ci = FALSE, add.data = TRUE)\n\n\n\n\n\n\n\nggeffect(m3, terms = c(\"ses\", \"sector\")) |&gt; \n  plot(ci = FALSE)\n\n\n\n\n\n\n\nggeffect(m4, terms = c(\"ses\", \"sector\", \"female\")) |&gt; \n  plot(ci = FALSE)",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Easy viz for multilevel models with `ggeffects`</span>"
    ]
  },
  {
    "objectID": "coefficient_plots.html",
    "href": "coefficient_plots.html",
    "title": "16  Coefficient Plots",
    "section": "",
    "text": "Coefficient plots provide a visually intuitive way to present the results of regression models. By displaying each coefficient along with its confidence interval, we can quickly discern the significance and magnitude of each coefficient.\nAs usual, we will turn to the tidyverse to make our plots. We will use the broom.mixed package to quickly get our coefficients, and then ggplot to make a nice plot of them. This is a great plot for a lot of final projects.\nTo illustrate, say we have a fit multilevel model such as this one on the Making Caring Common Data (the specific model here is not the best choice for doing actual research):\n\narm::display( fit )\n\nlmer(formula = esafe ~ age + grade + gender + happy + care + \n    (1 | ID), data = dat)\n                coef.est coef.se\n(Intercept)      3.50     0.20  \nage              0.00     0.01  \ngrade11th        0.01     0.03  \ngrade12th        0.07     0.04  \ngrade5th        -0.16     0.08  \ngrade6th        -0.13     0.06  \ngrade7th        -0.12     0.05  \ngrade8th        -0.06     0.04  \ngrade9th         0.01     0.03  \ngenderno reveal -0.28     0.05  \ngenderOther     -0.40     0.06  \ngenderFemale    -0.05     0.02  \nhappy           -0.01     0.01  \ncare            -0.01     0.01  \n\nError terms:\n Groups   Name        Std.Dev.\n ID       (Intercept) 0.25    \n Residual             0.65    \n---\nnumber of obs: 7666, groups: ID, 39\nAIC = 15291.2, DIC = 15103.4\ndeviance = 15181.3 \n\n\nWe first tidy up the model output:\n\nlibrary( broom.mixed )\ntidy_fit &lt;- tidy(fit)\ntidy_fit\n\n# A tibble: 16 × 6\n   effect   group    term             estimate std.error statistic\n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 fixed    &lt;NA&gt;     (Intercept)      3.50       0.204     17.2   \n 2 fixed    &lt;NA&gt;     age             -0.000331   0.0126    -0.0261\n 3 fixed    &lt;NA&gt;     grade11th        0.0130     0.0311     0.417 \n 4 fixed    &lt;NA&gt;     grade12th        0.0674     0.0386     1.74  \n 5 fixed    &lt;NA&gt;     grade5th        -0.157      0.0822    -1.91  \n 6 fixed    &lt;NA&gt;     grade6th        -0.126      0.0600    -2.11  \n 7 fixed    &lt;NA&gt;     grade7th        -0.116      0.0494    -2.34  \n 8 fixed    &lt;NA&gt;     grade8th        -0.0620     0.0395    -1.57  \n 9 fixed    &lt;NA&gt;     grade9th         0.00762    0.0297     0.257 \n10 fixed    &lt;NA&gt;     genderno reveal -0.283      0.0502    -5.64  \n11 fixed    &lt;NA&gt;     genderOther     -0.401      0.0561    -7.15  \n12 fixed    &lt;NA&gt;     genderFemale    -0.0547     0.0165    -3.32  \n13 fixed    &lt;NA&gt;     happy           -0.00805    0.00980   -0.821 \n14 fixed    &lt;NA&gt;     care            -0.00613    0.0112    -0.548 \n15 ran_pars ID       sd__(Intercept)  0.249     NA         NA     \n16 ran_pars Residual sd__Observation  0.647     NA         NA     \n\n\nWe then select which coefficients we want on our plot:\n\ntidy_fit = filter( tidy_fit,\n                   is.na(group),\n                   term != \"(Intercept)\" )\n\nFinally, we make the coefficient plot:\n\nggplot(tidy_fit, aes(x=term, y=estimate)) +\n  geom_point() +\n  geom_errorbar(aes(ymin=estimate - std.error, ymax=estimate + std.error), width=0.25) +\n  coord_flip() +\n  labs(title=\"Coefficient Plot\", y=\"Estimate\", x=\"Variable\") +\n  geom_hline( yintercept = 0, col=\"blue\" ) +\n  theme_minimal()\n\n\n\n\nCoefficient Plot for mtcars dataset\n\n\n\n\nIn general you will want to make sure your plotted variables are on a similar scale, e.g., all categorical levels or, if continuous, standardized on some scale. Otherwise the points will be hard to compare to one another.\nTo do this we might standardsize continuous variables like so:\n\ndat &lt;- dat %&gt;%\n  filter( !is.na(bully), !is.na(psafe), !is.na(esafe) ) %&gt;%\n  mutate(  esafe.std = (esafe - mean(esafe) / sd(esafe) ),\n           bully.std = (bully - mean(bully) / sd(bully) ),\n               psafe.std = (psafe - mean(psafe) / sd(psafe) ) )\n\nWe can then fit a new coefficient plot for a new model:\n\nfit = lmer( esafe.std ~ gender + bully.std + psafe.std + (1+psafe|ID),\n          data=dat )\ntidy_fit &lt;- tidy(fit)\ntidy_fit = filter( tidy_fit,\n                   term != \"(Intercept)\",\n                   term != \"cor__(Intercept).psafe\" )\n\nggplot(tidy_fit, aes(x=term, y=estimate)) +\n  geom_point() +\n  geom_errorbar(aes(ymin=estimate - std.error, ymax=estimate + std.error), width=0.25) +\n  coord_flip() +\n  labs(title=\"Coefficient Plot\", y=\"Estimate\", x=\"Variable\") +\n  geom_hline( yintercept = 0, col=\"blue\" ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere we left our residual variances on to get some scale. E.g., the schools vary more than the girl-boy gap (boys are our reference category). We can now say things like a one standard deviation increase in bullying corresponds to a -0.3 standard deviation change in emotional safety. Physical safety, not unsurprisingly, is heavily predictive of emotional safety.\nThe small group size of those who chose not reveal their gender makes the confidence interval wider than for the other coefficents. Overall, this large survey is giving us good precision.",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Coefficient Plots</span>"
    ]
  },
  {
    "objectID": "double_plot.html",
    "href": "double_plot.html",
    "title": "17  Plotting Two Datasets at Once",
    "section": "",
    "text": "It’s easy (though not always advisable) to plot two data sets at once with ggplot. First, we load tidyverse and our HSB data. We then create a school-level aggregate data set of just the mean SES values.\n\nlibrary(tidyverse)\nlibrary(haven)\n\n# clear memory\nrm(list = ls())\n\ntheme_set(theme_classic())\n\n# load HSB data\nhsb &lt;- read_dta(\"data/hsb.dta\") |&gt; \n  select(mathach, ses, schoolid)\n\nsch &lt;- hsb |&gt; \n  group_by(schoolid) |&gt; \n  summarise(mean_ses = mean(ses),\n            mean_mathach = mean(mathach))\n\nLet’s say we wanted to plot both the individual students and the school means. This is easy enough to do separately:\n\nggplot(hsb, aes(x = ses, y = mathach)) +\n  geom_point(alpha = 0.1)\n\n\n\n\n\n\n\nggplot(sch, aes(x = mean_ses, y = mean_mathach)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe can superimpose both plots as follows. Essentially, the first argument in ggplot provides the data, and by default, this is passed to all subsequent layers of the plot. We can override this behavior by specifying a different data set (and aesthetic mappings, if desired) within an individual layer of ggplot, such as geom_point.\n\nggplot(hsb, aes(x = ses, y = mathach)) +\n  geom_point(alpha = 0.1) +\n  geom_point(data = sch, aes(x = mean_ses, y = mean_mathach), color = \"red\")",
    "crumbs": [
      "USING ggPLOT",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Plotting Two Datasets at Once</span>"
    ]
  },
  {
    "objectID": "examining_shrinkage.html",
    "href": "examining_shrinkage.html",
    "title": "18  How Empirical Bayes over-shrinks",
    "section": "",
    "text": "18.1 Comparing the model to the estimates\nWe can measure how much variation there is in the Empirical Bayes estimated intercepts and slopes, along with the correlation of these effects:\neb_ests = c( sd_int = sd( res$beta0 ),\n             sd_slope = sd( res$beta1 ),\n             cor = cor( res$beta0, res$beta1 ) )\nWe display these estimates alongside the model estimates:\nparameter\nmodel estimate\nEmp Bayes estimate\n\n\n\n\nstdev intercept\n2.20\n2.01\n\n\nstdev slope\n0.64\n0.28\n\n\ncorrelation\n-0.11\n-0.23\nIf we compare the variation in the empirical Bayes estimates to the model estimates, we see that the standard deviations are smaller and the correlation is estimated as larger in magnitude. Importantly, our model does a good job, in general, estimating how much variation in random intercepts and slopes there is; it is the empirical estimates that are over shrunk. Trust the model, not the spread of the empirical estimates.\nIn short, the empirical estimates are good for predicting individual values, but the distribution of the empirical estimates is generally too tidy and narrow, as compared to the truth. The model is what best estimates the population characteristics. That being said, the empirical Bayes estimates are far better than the raw estimates (in the above, for example, trust the red lines more than the dashed lines).",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>How Empirical Bayes over-shrinks</span>"
    ]
  },
  {
    "objectID": "examining_shrinkage.html#plotting-the-individual-schools",
    "href": "examining_shrinkage.html#plotting-the-individual-schools",
    "title": "18  How Empirical Bayes over-shrinks",
    "section": "18.2 Plotting the individual schools",
    "text": "18.2 Plotting the individual schools\nWhen looking at individual schools we have this:\n\nggplot( data=res ) +\n    scale_x_continuous(limits=range( dat$ses ) ) +\n    scale_y_continuous(limits=range( dat$mathach ) ) +\n  geom_abline( aes( intercept = beta0, slope=beta1 ), alpha=0.25) +\n  labs( x=\"SES\", y=\"Math Achievment\" ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCompare that to the messy (and incorrect) raw estimates, that are generated by running a interacted fixed effect regression of:\n\nM = lm( mathach ~ 0 + ses*id - ses, data=dat )\ncc = coef(M)\nhead(cc)\n\n   id1224    id1288    id1296    id1308    id1317    id1358 \n10.805132 13.114937  8.093779 16.188959 12.737763 11.305904 \n\ntail(cc)\n\nses:id9347 ses:id9359 ses:id9397 ses:id9508 ses:id9550 ses:id9586 \n  2.685994  -0.833479   2.446444   3.953791   3.891938   1.672081 \n\nlength(cc)\n\n[1] 320\n\nschools = tibble( beta0 = cc[1:160],\n                  beta1 = cc[161:320] )\n\nggplot( data=schools ) +\n    scale_x_continuous(limits=range( dat$ses ) ) +\n    scale_y_continuous(limits=range( dat$mathach ) ) +\n    geom_abline( aes( intercept = beta0, slope=beta1 ), alpha=0.25) +\n    labs( x=\"SES\", y=\"Math Achievment\" ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe raw estimates are over dispersed; the measurement error is giving a bad picture.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>How Empirical Bayes over-shrinks</span>"
    ]
  },
  {
    "objectID": "examining_shrinkage.html#simulation-to-get-a-just-right-picture",
    "href": "examining_shrinkage.html#simulation-to-get-a-just-right-picture",
    "title": "18  How Empirical Bayes over-shrinks",
    "section": "18.3 Simulation to get a just-right picture",
    "text": "18.3 Simulation to get a just-right picture\nAs discussed in class, empirical Bayes is too smooth. Raw is too disperse. If we want to see a picture of what the population of schools might look like, we can make a plot of 160 NEW schools generated from our model (to see how our partially pooled (Empirical Bayes) estimates are OVER SHRUNK/OVER SMOOTHED).\nWe simulate from our model; we are not using the empirical bayes estimates at all. See the slides and script for Packet 2.4 for how to do this simulation.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>How Empirical Bayes over-shrinks</span>"
    ]
  },
  {
    "objectID": "broom.html",
    "href": "broom.html",
    "title": "19  Extracting information from fitted lmer models with broom",
    "section": "",
    "text": "19.1 Simple Demonstration\nOne of my favorite R packages is broom, which has many awesome convenience functions for regression models, including MLMs. broom.mixed is the extension that specifically works with lmer models. It does this via a few core methods that give you the model parameters and information as a nice data frame that you can then use more easily than the original result from your lmer() call. Let’s see how it works.\nWe first load it (and a few other things, and some data):\n# load libraries\nlibrary(tidyverse)\nlibrary(broom.mixed)\nlibrary(haven)\nlibrary(knitr)\nlibrary(lme4)\n\n# clear memory\nrm(list = ls())\n\n# load HSB data\nhsb &lt;- read_dta(\"data/hsb.dta\")",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Extracting information from fitted `lmer` models with `broom`</span>"
    ]
  },
  {
    "objectID": "broom.html#simple-demonstration",
    "href": "broom.html#simple-demonstration",
    "title": "19  Extracting information from fitted lmer models with broom",
    "section": "",
    "text": "19.1.1 tidy\nThe tidy() method takes a model object and returns the output as a tidy tibble (i.e., a data frame), which makes it very easy to work with. Compare the results below:\n\nols &lt;- lm(mathach ~ ses, hsb)\n\n# ugly!\nsummary(ols)\n\n\nCall:\nlm(formula = mathach ~ ses, data = hsb)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.4382  -4.7580   0.2334   5.0649  15.9007 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.74740    0.07569  168.42   &lt;2e-16 ***\nses          3.18387    0.09712   32.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.416 on 7183 degrees of freedom\nMultiple R-squared:  0.1301,    Adjusted R-squared:   0.13 \nF-statistic:  1075 on 1 and 7183 DF,  p-value: &lt; 2.2e-16\n\n# beautiful!\ntidy(ols)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    12.7     0.0757     168.  0        \n2 ses             3.18    0.0971      32.8 8.71e-220\n\n# even better\nols |&gt; tidy() |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.75\n0.08\n168.42\n0\n\n\nses\n3.18\n0.10\n32.78\n0\n\n\n\n\n# Also works great for MLMs\nmlm &lt;- lmer(mathach ~ ses + mnses + (ses|schoolid), hsb)\n\ntidy(mlm)\n\n# A tibble: 7 × 6\n  effect   group    term                 estimate std.error statistic\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;     (Intercept)            12.7       0.151     84.2 \n2 fixed    &lt;NA&gt;     ses                     2.19      0.122     18.0 \n3 fixed    &lt;NA&gt;     mnses                   3.78      0.383      9.88\n4 ran_pars schoolid sd__(Intercept)         1.64     NA         NA   \n5 ran_pars schoolid cor__(Intercept).ses   -0.212    NA         NA   \n6 ran_pars schoolid sd__ses                 0.673    NA         NA   \n7 ran_pars Residual sd__Observation         6.07     NA         NA   \n\n\n\n\n19.1.2 glance\nWhat about model fit stats? That’s where glance comes in:\n\nglance(ols)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.130         0.130  6.42     1075. 8.71e-220     1 -23549. 47104. 47125.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(mlm) |&gt; \n  kable(digits = 2)\n\n\n\n\nnobs\nsigma\nlogLik\nAIC\nBIC\nREMLcrit\ndf.residual\n\n\n\n\n7185\n6.07\n-23280.71\n46575.42\n46623.58\n46561.42\n7178\n\n\n\n\n\n\n\n19.1.3 augment\nWhat about your estimated random effects? augment to the rescue, giving estimates for each random effect:\n\nmlm |&gt; \n  ranef() |&gt; \n  augment() |&gt; \n  head() |&gt; \n  kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngrp\nvariable\nlevel\nestimate\nqq\nstd.error\nlb\nub\n\n\n\n\nschoolid\n(Intercept)\n8367\n-4.14\n-0.18\n0.78\n-5.43\n-2.85\n\n\nschoolid\n(Intercept)\n4523\n-3.09\n0.02\n0.98\n-4.70\n-1.47\n\n\nschoolid\n(Intercept)\n6990\n-2.98\n-1.46\n0.78\n-4.26\n-1.70\n\n\nschoolid\n(Intercept)\n3705\n-2.81\n0.28\n1.09\n-4.61\n-1.02\n\n\nschoolid\n(Intercept)\n8854\n-2.57\n-0.85\n0.80\n-3.89\n-1.25\n\n\nschoolid\n(Intercept)\n9397\n-2.43\n-0.65\n0.92\n-3.94\n-0.92\n\n\n\n\n\nThe level column are your school IDs, here. If you have multiple sets of random effects, they will all be stacked, and indexed via grp.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Extracting information from fitted `lmer` models with `broom`</span>"
    ]
  },
  {
    "objectID": "broom.html#extracting-lmer-model-info",
    "href": "broom.html#extracting-lmer-model-info",
    "title": "19  Extracting information from fitted lmer models with broom",
    "section": "19.2 Extracting lmer model info",
    "text": "19.2 Extracting lmer model info\n\n19.2.1 Obtaining Fixed Effects\nlmer models are in reduced form, so fixed effects include both L1 and L2 predictors. tidy denotes the type of effect in a column called effect, where fixed means fixed, and ran_pars means random (standing for “random parameters”)\n\nmlm |&gt; \n  tidy() |&gt; \n  filter(effect == \"fixed\")\n\n# A tibble: 3 × 6\n  effect group term        estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  &lt;NA&gt;  (Intercept)    12.7      0.151     84.2 \n2 fixed  &lt;NA&gt;  ses             2.19     0.122     18.0 \n3 fixed  &lt;NA&gt;  mnses           3.78     0.383      9.88\n\n\nWe can use the [[]] notation or a pipeline to extract elements from the data frame:\n\n# within effect of SES\ntidy(mlm)[[2,4]]\n\n[1] 2.190349\n\n# contextual effect of SES\ntidy(mlm)[[3,4]]\n\n[1] 3.781243\n\n# using the variable names in a pipeline\nmlm |&gt; \n  tidy() |&gt; \n  filter(term == \"ses\") |&gt; \n  pull(estimate)\n\n[1] 2.190349\n\n\n\n\n19.2.2 Obtaining Random Effects\ntidy includes the random effects (SDs and correlations) right there in the output. For example, sd__ses is the SD of the SES slope.\n\n# display all random effects\nmlm |&gt; \n  tidy() |&gt; \n  filter(effect == \"ran_pars\")\n\n# A tibble: 4 × 6\n  effect   group    term                 estimate std.error statistic\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 ran_pars schoolid sd__(Intercept)         1.64         NA        NA\n2 ran_pars schoolid cor__(Intercept).ses   -0.212        NA        NA\n3 ran_pars schoolid sd__ses                 0.673        NA        NA\n4 ran_pars Residual sd__Observation         6.07         NA        NA\n\n# pull single number\nmlm |&gt; \n  tidy() |&gt; \n  filter(term == \"sd__ses\") |&gt; \n  pull(estimate)\n\n[1] 0.6730818\n\n\n\n\n19.2.3 Obtaining Empirical Bayes Estimates of the Random Effects\nThis is best done in a pipeline. We first apply ranef, then augment and get the EB estimates in the estimate column, along with the std.error, confidence bounds, and qq statistics.\n\nmlm |&gt; \n  ranef() |&gt; \n  augment() |&gt; \n  head()\n\n       grp    variable level  estimate         qq std.error        lb\n1 schoolid (Intercept)  8367 -4.137656 -0.1811498 0.7845770 -5.428170\n2 schoolid (Intercept)  4523 -3.089835  0.0235018 0.9819306 -4.704967\n3 schoolid (Intercept)  6990 -2.981315 -1.4619679 0.7779876 -4.260991\n4 schoolid (Intercept)  3705 -2.811935  0.2776904 1.0911916 -4.606785\n5 schoolid (Intercept)  8854 -2.569302 -0.8528365 0.8045804 -3.892719\n6 schoolid (Intercept)  9397 -2.431031 -0.6452734 0.9163587 -3.938307\n          ub\n1 -2.8471413\n2 -1.4747032\n3 -1.7016394\n4 -1.0170840\n5 -1.2458846\n6 -0.9237553\n\n\n\n\n19.2.4 Intercept-Slope Correlation\nThe BLUPs are in long form. We can reshape to wide if we want to, for example, visualize the correlation between the random intercepts and slopes.\n\nblups &lt;- mlm |&gt; \n  ranef() |&gt; \n  augment() |&gt; \n  dplyr::select(variable, level, estimate) |&gt; \n  pivot_wider(names_from = variable, values_from = estimate,\n              id_cols = level) |&gt; \n  dplyr::rename(schoolid = 1, random_intercept = 2, random_slope = 3)\n\nhead(blups)\n\n# A tibble: 6 × 3\n  schoolid random_intercept random_slope\n  &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n1 8367                -4.14       0.159 \n2 4523                -3.09       0.272 \n3 6990                -2.98      -0.0353\n4 3705                -2.81      -0.0968\n5 8854                -2.57       0.377 \n6 9397                -2.43       0.174 \n\nggplot(blups, aes(x = random_intercept, y = random_slope)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n19.2.5 Caterpillar Plots\nThe included information as a data frame makes it easy to construct caterpillar plots!\n\nri &lt;- mlm |&gt; \n  ranef() |&gt; \n  augment() \nggplot(ri, aes(x = level, y = estimate,\n               ymin = lb,\n               ymax = ub)) +\n  facet_wrap( ~ variable, nrow = 1 ) +\n  geom_point() +\n  geom_errorbar() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n19.2.6 Fitted Values\nUsing augment directly on the lmer object gives us fitted values (.fitted) and residuals (.resid). We can use this for residual plots or for plotting lines for each school.\n\nmlm |&gt; \n  augment() |&gt; \n  head()\n\n# A tibble: 6 × 15\n  mathach     ses  mnses schoolid .fitted .resid   .hat   .cooksd .fixed   .mu\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    5.88 -1.53   -0.434     1224    7.29 -1.41  0.0325 0.000629    7.68  7.29\n2   19.7  -0.588  -0.434     1224    9.43 10.3   0.0177 0.0175      9.74  9.43\n3   20.3  -0.528  -0.434     1224    9.57 10.8   0.0173 0.0188      9.87  9.57\n4    8.78 -0.668  -0.434     1224    9.25 -0.468 0.0183 0.0000376   9.57  9.25\n5   17.9  -0.158  -0.434     1224   10.4   7.49  0.0164 0.00863    10.7  10.4 \n6    4.58  0.0220 -0.434     1224   10.8  -6.24  0.0170 0.00619    11.1  10.8 \n# ℹ 5 more variables: .offset &lt;dbl&gt;, .sqrtXwt &lt;dbl&gt;, .sqrtrwt &lt;dbl&gt;,\n#   .weights &lt;dbl&gt;, .wtres &lt;dbl&gt;\n\n# fitted lines\nmlm |&gt; \n  augment() |&gt; \n  ggplot(aes(x = ses, y = .fitted, group = schoolid)) +\n  geom_line( alpha=0.5 )\n\n\n\n\n\n\n\n# residuals\nmlm |&gt; \n  augment() |&gt; \n  ggplot(aes(y = .resid, x = .fitted)) +\n  geom_hline(yintercept = 0, color = \"red\") +\n  geom_point(alpha = 0.2)",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Extracting information from fitted `lmer` models with `broom`</span>"
    ]
  },
  {
    "objectID": "broom.html#additional-resources",
    "href": "broom.html#additional-resources",
    "title": "19  Extracting information from fitted lmer models with broom",
    "section": "19.3 Additional Resources",
    "text": "19.3 Additional Resources\nI’ve recently discovered the packaged mixedup that has some excellent additional convenience functions for extracting info from lmer models: https://m-clark.github.io/mixedup/index.html.\nIt might be worth checking out as well!",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Extracting information from fitted `lmer` models with `broom`</span>"
    ]
  },
  {
    "objectID": "lmer_extract.html",
    "href": "lmer_extract.html",
    "title": "20  Extrating information from fitted lmer models using base R",
    "section": "",
    "text": "20.1 Fitting and viewing the model\nNow we fit the random slope model with the level-2 covariates:\nM1 = lmer( mathach ~ 1 + ses + meanses + (1 + ses|id), data=dat )\nIf we just print the object, e.g., by typing the name of the model on the console, we get minimal information:\nM1\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: mathach ~ 1 + ses + meanses + (1 + ses | id)\n   Data: dat\nREML criterion at convergence: 46561.42\nRandom effects:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 1.6417        \n          ses         0.6731   -0.21\n Residual             6.0659        \nNumber of obs: 7185, groups:  id, 160\nFixed Effects:\n(Intercept)          ses      meanses  \n     12.651        2.190        3.781",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Extrating information from fitted `lmer` models using base R</span>"
    ]
  },
  {
    "objectID": "lmer_extract.html#fitting-and-viewing-the-model",
    "href": "lmer_extract.html#fitting-and-viewing-the-model",
    "title": "20  Extrating information from fitted lmer models using base R",
    "section": "",
    "text": "20.1.1 The display() method\nThe arm package’s display() method gives an overview of what our fitted model is:\n\ndisplay( M1 )\n\nlmer(formula = mathach ~ 1 + ses + meanses + (1 + ses | id), \n    data = dat)\n            coef.est coef.se\n(Intercept) 12.65     0.15  \nses          2.19     0.12  \nmeanses      3.78     0.38  \n\nError terms:\n Groups   Name        Std.Dev. Corr  \n id       (Intercept) 1.64           \n          ses         0.67     -0.21 \n Residual             6.07           \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46575.4, DIC = 46552.4\ndeviance = 46556.9 \n\n\n\n\n20.1.2 The summary() method\nWe can also look at the messier default summary() command, which gives you more output. The real win is if we use the lmerTest library and fit our model with that package loaded, our summary() is more exciting and has \\(p\\)-values:\n\nlibrary( lmerTest )\nM1 = lmer( mathach ~ 1 + ses + meanses + (1 + ses|id), data=dat )\nsummary( M1 )\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mathach ~ 1 + ses + meanses + (1 + ses | id)\n   Data: dat\n\nREML criterion at convergence: 46561.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1671 -0.7270  0.0163  0.7547  2.9646 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept)  2.695   1.6417        \n          ses          0.453   0.6731   -0.21\n Residual             36.796   6.0659        \nNumber of obs: 7185, groups:  id, 160\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)  12.6513     0.1506 152.9599  84.000   &lt;2e-16 ***\nses           2.1903     0.1218 178.2055  17.976   &lt;2e-16 ***\nmeanses       3.7812     0.3826 181.7675   9.883   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr) ses   \nses     -0.080       \nmeanses -0.028 -0.256",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Extrating information from fitted `lmer` models using base R</span>"
    ]
  },
  {
    "objectID": "lmer_extract.html#obtaining-fixed-effects",
    "href": "lmer_extract.html#obtaining-fixed-effects",
    "title": "20  Extrating information from fitted lmer models using base R",
    "section": "20.2 Obtaining Fixed Effects",
    "text": "20.2 Obtaining Fixed Effects\nR thinks of all models in reduced form. Thus when we get the fixed effects we get both the level-1 and level-2 fixed effects all together:\n\nfixef( M1 )\n\n(Intercept)         ses     meanses \n  12.651300    2.190350    3.781218 \n\n\nThe above is a vector of numbers. Each element is named, but we can index them as so:\n\nfixef( M1 )[2]\n\n    ses \n2.19035 \n\n\nWe can also use the [[]] which means “give me that element not as a list but as just the element!” When in doubt, if you want one thing out of a list or vector, use [[]] instead of []:\n\nfixef( M1 )[[2]]\n\n[1] 2.19035\n\n\nSee how it gives you the number without the name here?",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Extrating information from fitted `lmer` models using base R</span>"
    ]
  },
  {
    "objectID": "lmer_extract.html#obtaining-variance-and-covariance-estimates",
    "href": "lmer_extract.html#obtaining-variance-and-covariance-estimates",
    "title": "20  Extrating information from fitted lmer models using base R",
    "section": "20.3 Obtaining Variance and Covariance estimates",
    "text": "20.3 Obtaining Variance and Covariance estimates\nWe can get the Variance-Covariance matrix of the random effects with VarCorr.\n\nVarCorr( M1 )\n\n Groups   Name        Std.Dev. Corr  \n id       (Intercept) 1.64174        \n          ses         0.67309  -0.212\n Residual             6.06594        \n\n\nIt displays nicely if you just print it out, but inside it are covariance matrices for each random effect group. (In our model we only have one group, id.) These matrices also have correlation matrices for reference. Here is how to get these pieces:\n\nvc = VarCorr( M1 )$id\nvc\n\n            (Intercept)        ses\n(Intercept)   2.6953203 -0.2339045\nses          -0.2339045  0.4530494\nattr(,\"stddev\")\n(Intercept)         ses \n  1.6417431   0.6730894 \nattr(,\"correlation\")\n            (Intercept)        ses\n(Intercept)   1.0000000 -0.2116707\nses          -0.2116707  1.0000000\n\n\nYou might be wondering what all the attr stuff is. R can “tack on” extra information to a variable via “attributes”. Attributes are not part of the variable exactly, but they follows their variable around. The attr (for attribute) method is a way to get these extra bits of information. In the above, R is tacking the correlation matrix on to the variance-covariance matrix to save you the trouble of calculating it yourself. Get it as follows:\n\nattr( vc, \"correlation\" )\n\n            (Intercept)        ses\n(Intercept)   1.0000000 -0.2116707\nses          -0.2116707  1.0000000\n\n\nYou can also just use the vc object as a matrix. Here we take the diagonal of it\n\ndiag( vc )\n\n(Intercept)         ses \n  2.6953203   0.4530494 \n\n\nIf you want an element from a matrix use row-column indexing like so:\n\nvc[1,2]\n\n[1] -0.2339045\n\n\nfor row 1 and column 2.\n\n20.3.0.1 The sigma.hat() and sigma() methods\nIf you just want the variances and standard deviations of your random effects, use sigma.hat(). This also gives you the residual standard deviation as well. The output is a weird object, with a list of things that are themselves lists in it. Let’s examine it. First we look at what the whole thing is:\n\nsigma.hat( M1 )\n\n$sigma\n$sigma$data\n[1] 6.065939\n\n$sigma$id\n(Intercept)         ses \n  1.6417431   0.6730894 \n\n\n$cors\n$cors$data\n[1] NA\n\n$cors$id\n            (Intercept)        ses\n(Intercept)   1.0000000 -0.2116707\nses          -0.2116707  1.0000000\n\nnames( sigma.hat( M1 ) )\n\n[1] \"sigma\" \"cors\" \n\nsigma.hat( M1 )$sigma\n\n$data\n[1] 6.065939\n\n$id\n(Intercept)         ses \n  1.6417431   0.6730894 \n\n\nOur standard deviations of the random effects are\n\nsigma.hat( M1 )$sigma$id\n\n(Intercept)         ses \n  1.6417431   0.6730894 \n\n\nWe can get our residual variance by this weird thing (we are getting data from the sigma inside of sigma.hat( M1 )):\n\nsigma.hat( M1 )$sigma$data\n\n[1] 6.065939\n\n\nBut here is an easier way using the sigma() utility function:\n\nsigma( M1 )\n\n[1] 6.065939",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Extrating information from fitted `lmer` models using base R</span>"
    ]
  },
  {
    "objectID": "lmer_extract.html#obtaining-empirical-bayes-estimates-of-the-random-effects",
    "href": "lmer_extract.html#obtaining-empirical-bayes-estimates-of-the-random-effects",
    "title": "20  Extrating information from fitted lmer models using base R",
    "section": "20.4 Obtaining Empirical Bayes Estimates of the Random Effects",
    "text": "20.4 Obtaining Empirical Bayes Estimates of the Random Effects\nRandom effects come out of the ranef() method. Each random effect is its own object inside the returned object. You refer to these sets of effects by name. Here our random effect is called id.\n\nests = ranef( M1 )$id\nhead( ests )\n\n     (Intercept)         ses\n1224 -0.26204371  0.08765385\n1288  0.03805199  0.11841938\n1296 -1.91525901  0.03572247\n1308  0.30485682 -0.10500515\n1317 -1.15834807 -0.10815301\n1358 -0.98212459  0.44612877\n\n\nGenerally, what you get back from these calls is a new data frame with a row for each group. The rows are named with the original id codes for the groups, but if you want to connect it back to your group-level information you are going to want to merge stuff. To do this, and to keep things organized, I recommend adding the id as a column to your dataframe:\n\nnames(ests) = c( \"u0\", \"u1\" )\nests$id = rownames( ests )\nhead( ests )\n\n              u0          u1   id\n1224 -0.26204371  0.08765385 1224\n1288  0.03805199  0.11841938 1288\n1296 -1.91525901  0.03572247 1296\n1308  0.30485682 -0.10500515 1308\n1317 -1.15834807 -0.10815301 1317\n1358 -0.98212459  0.44612877 1358\n\n\nWe also renamed our columns of our dataframe to give them names nicer than (Intercept). You can use these names if you wish, however. You just need to quote them with back ticks (this code is not run):\n\nhead( ests$`(Intercept)` )\n\n\n20.4.1 The coef() method\nWe can also get a slighly different (but generally easier to use) version these things through coef(). What coef() does is give you the estimated regression lines for each group in your data by combining the random effect for each group with the corresponding fixed effects. Note how in the following the meanses coefficient is the same, but the others vary due to the random slope and random intercept.\n\ncoefs = coef( M1 )$id\nhead( coefs )\n\n     (Intercept)      ses  meanses\n1224    12.38926 2.278004 3.781218\n1288    12.68935 2.308769 3.781218\n1296    10.73604 2.226072 3.781218\n1308    12.95616 2.085345 3.781218\n1317    11.49295 2.082197 3.781218\n1358    11.66918 2.636479 3.781218\n\n\nNote that if we have level 2 covariates in our model, they are not incorperated in the intercept and slope via coef(). We have to do that by hand:\n\nnames( coefs ) = c( \"beta0.adj\", \"beta.ses\", \"beta.meanses\" )\ncoefs$id = rownames( coefs )\ncoefs = merge( coefs, sdat, by=\"id\" )\ncoefs = mutate( coefs, beta0 = beta0.adj + beta.meanses * meanses )\ncoefs$beta.meanses = NULL\n\nHere we added in the impact of mean ses to the intercept (as specified by our model). Now if we look at the intercepts (the beta0 variables) they will incorperate the level 2 covariate effects. If we then plotted a line using beta0 and beta.ses for each school, we would get the estimated lines for each school including the school-level covariate impacts.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Extrating information from fitted `lmer` models using base R</span>"
    ]
  },
  {
    "objectID": "lmer_extract.html#obtaining-standard-errors",
    "href": "lmer_extract.html#obtaining-standard-errors",
    "title": "20  Extrating information from fitted lmer models using base R",
    "section": "20.5 Obtaining standard errors",
    "text": "20.5 Obtaining standard errors\nWe can get an object with all the standard errors of the coefficients, including the individual Emperical Bayes estimates for the individual random effects. This is a lot of information. We first look at the Standard Errors for the fixed effects, and then for the random effects. Standard errors for the variance terms are not given (this is tricker to calculate).\n\n20.5.1 Fixed effect standard errors\n\nses = se.coef( M1 )\nnames( ses )\n\n[1] \"fixef\" \"id\"   \n\n\nOur fixed effect standard errors:\n\nses$fixef\n\n[1] 0.1506106 0.1218474 0.3826085\n\n\nYou can also get the uncertainty estimates of your fixed effects as a variance-covariance matrix:\n\nvcov( M1 )\n\n3 x 3 Matrix of class \"dpoMatrix\"\n             (Intercept)          ses      meanses\n(Intercept)  0.022683560 -0.001465374 -0.001619405\nses         -0.001465374  0.014846788 -0.011954182\nmeanses     -0.001619405 -0.011954182  0.146389293\n\n\nThe standard errors are the diagonal of this matrix, square-rooted. See how they line up?:\n\nsqrt( diag( vcov( M1 ) ) )\n\n(Intercept)         ses     meanses \n  0.1506106   0.1218474   0.3826085 \n\n\n\n\n20.5.2 Random effect standard errors\nOur random effect standard errors for our EB estimates:\n\nhead( ses$id )\n\n     (Intercept)       ses\n1224   0.7845859 0.5804186\n1288   0.9819216 0.6277115\n1296   0.7779963 0.5766319\n1308   1.0911690 0.6556607\n1317   0.8045695 0.6188535\n1358   0.9163545 0.6173954\n\n\nWarning: these come as a matrix, not data frame. It is probably best to do this:\n\nSEs = as.data.frame( se.coef( M1 )$id )\nhead( SEs )\n\n     (Intercept)       ses\n1224   0.7845859 0.5804186\n1288   0.9819216 0.6277115\n1296   0.7779963 0.5766319\n1308   1.0911690 0.6556607\n1317   0.8045695 0.6188535\n1358   0.9163545 0.6173954",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Extrating information from fitted `lmer` models using base R</span>"
    ]
  },
  {
    "objectID": "lmer_extract.html#generating-confidence-intervals",
    "href": "lmer_extract.html#generating-confidence-intervals",
    "title": "20  Extrating information from fitted lmer models using base R",
    "section": "20.6 Generating confidence intervals",
    "text": "20.6 Generating confidence intervals\nWe can compute profile confidence intervals (warnings have been suppressed)\n\nconfint( M1 )\n\n                 2.5 %     97.5 %\n.sig01       1.4012799  1.8897548\n.sig02      -0.8761947  0.1946551\n.sig03       0.2165284  0.9849953\n.sigma       5.9659922  6.1689341\n(Intercept) 12.3559620 12.9462385\nses          1.9512025  2.4296954\nmeanses      3.0278220  4.5329237",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Extrating information from fitted `lmer` models using base R</span>"
    ]
  },
  {
    "objectID": "lmer_extract.html#obtaining-fitted-values",
    "href": "lmer_extract.html#obtaining-fitted-values",
    "title": "20  Extrating information from fitted lmer models using base R",
    "section": "20.7 Obtaining fitted values",
    "text": "20.7 Obtaining fitted values\nFitted values are the predicted value for each individual given the model.\n\nyhat = fitted( M1 )\nhead( yhat )\n\n        1         2         3         4         5         6 \n 7.290105  9.431429  9.568109  9.249189 10.410971 10.821011 \n\n\nResiduals are the difference between predicted and observed:\n\nresids = resid( M1 )\nhead( resids )\n\n         1          2          3          4          5          6 \n-1.4141055 10.2765710 10.7808908 -0.4681887  7.4870293 -6.2380113 \n\n\nWe can also predict for hypothetical new data. Here we predict the outcome for a random student with ses of -1, 0, and 1 in a school with mean ses of 0:\n\nndat = data.frame( ses = c( -1, 0, 1 ), meanses=c(0,0,0), id = -1 )\npredict( M1, newdata=ndat, allow.new.levels=TRUE )\n\n       1        2        3 \n10.46095 12.65130 14.84165 \n\n\nThe allow.new.levels=TRUE bit says to predict for a new school (our fake school id of -1 in ndat above). In this case it assumes the new school is typical, with 0s for the random effect residuals.\nIf we predict for a current school, the random effect estimates are incorporated:\n\nndat$id = 1296\npredict( M1, newdata=ndat )\n\n        1         2         3 \n 8.509969 10.736041 12.962114",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Extrating information from fitted `lmer` models using base R</span>"
    ]
  },
  {
    "objectID": "lmer_extract.html#appendix-the-guts-of-the-object",
    "href": "lmer_extract.html#appendix-the-guts-of-the-object",
    "title": "20  Extrating information from fitted lmer models using base R",
    "section": "20.8 Appendix: the guts of the object",
    "text": "20.8 Appendix: the guts of the object\nWhen we fit our model and store it in a variable, R stores a lot of stuff. The following lists some other functions that pull out bits and pieces of that stuff.\nFirst, to get the model matrix (otherwise called the design matrix)\n\nmm = model.matrix( M1 )\nhead( mm )\n\n  (Intercept)    ses meanses\n1           1 -1.528  -0.428\n2           1 -0.588  -0.428\n3           1 -0.528  -0.428\n4           1 -0.668  -0.428\n5           1 -0.158  -0.428\n6           1  0.022  -0.428\n\n\nThis can be useful for predicting individual group mean outcomes, for example.\nWe can also ask questions such as number of groups, number of individuals:\n\nngrps( M1 )\n\n id \n160 \n\nnobs( M1 )\n\n[1] 7185\n\n\nWe can list all methods for the object (merMod is a more generic version of lmerMod and has a lot of methods we can use)\n\nclass( M1 )\n\n[1] \"lmerModLmerTest\"\nattr(,\"package\")\n[1] \"lmerTest\"\n\nmethods(class = \"lmerMod\")\n\n [1] coerce      coerce&lt;-    contest     contest1D   contestMD   display    \n [7] getL        mcsamp      se.coef     show        sim         standardize\nsee '?methods' for accessing help and source code\n\nmethods(class = \"merMod\")\n\n [1] anova          as.function    coef           confint        cooks.distance\n [6] deviance       df.residual    display        drop1          extractAIC    \n[11] extractDIC     family         fitted         fixef          formula       \n[16] fortify        getData        getL           getME          hatvalues     \n[21] influence      isGLMM         isLMM          isNLMM         isREML        \n[26] logLik         mcsamp         model.frame    model.matrix   ngrps         \n[31] nobs           plot           predict        print          profile       \n[36] ranef          refit          refitML        rePCA          residuals     \n[41] rstudent       se.coef        show           sigma.hat      sigma         \n[46] sim            simulate       standardize    summary        terms         \n[51] update         VarCorr        vcov           weights       \nsee '?methods' for accessing help and source code",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Extrating information from fitted `lmer` models using base R</span>"
    ]
  },
  {
    "objectID": "interpreting_coefficients.html",
    "href": "interpreting_coefficients.html",
    "title": "21  Interpreting Coefficients",
    "section": "",
    "text": "21.1 Interpreting your models\nSo, multilevel models sure are great, but they can also make interpretations much more challenging. You’ve done OLS regression, so you have an understanding of how to interpret regression coefficients. However, adding additional levels means that some of our interpretations also need to change. This document is intended to provide a brief guide to how to do that.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Interpreting Coefficients</span>"
    ]
  },
  {
    "objectID": "interpreting_coefficients.html#interpreting-your-models",
    "href": "interpreting_coefficients.html#interpreting-your-models",
    "title": "21  Interpreting Coefficients",
    "section": "",
    "text": "21.1.1 Coefficients and indices at various levels of the model\nBut before we even start, we need to talk about how we use different coefficients and letters at different levels of the model. There isn’t a single convention for how to do this, but we’ll try to be consistent at least in this class.\nWe’ll distinguish between two basic types of models, those that are multilevel and not longitudinal, and those that are longitudinal.\nAs a canonical example of the first type, let’s consider the model we use in class, namely\n\\[\\begin{aligned}\nmathach_{ij} &= \\beta_{0j[i]} + \\beta_{1j[i]}SES_i + \\varepsilon_i, \\\\\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01}sector_j + u_{0j},\\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11}sector_j + u_{1j},\\\\\n\\varepsilon_i &\\sim Normal(0, \\sigma^2_\\varepsilon) \\\\\n\\begin{pmatrix}\nu_{0j}\\\\\nu_{1j}\\\\\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\sigma^2_0 & \\rho\\sigma_0\\sigma_1\\\\\n\\rho\\sigma_0\\sigma_1 & \\sigma^2_1\n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned}\\]\nHere are the features of the model to attend to. When referring to students (or other first-level units), we will use \\(i\\) as a subscript. \\(X_i\\) will indicate a measurement taken for the \\(i\\)th student. When referring to schools (or other second-level units), we will \\(j\\) as a subscript. \\(X_j\\) will indicate a measurement taken for the \\(j\\)th school. When we expand these models to include third-level units (e.g., districts), we will use the subscript \\(k\\) for these units. I don’t intend to go past that, although we could. When we introduce cross-classified models (i.e., models will non-nested hierarchies) we’ll pick subscripts that are intended to be evocative.\nWe’ll also try to be consistent when using coefficients. We’ll use the letter \\(\\beta\\) (beta) to indicate regression coefficients measured at the first level. We’ll use the letter \\(\\gamma\\) (gamma) to indicate regression coefficients measured at the second level. Eventually we’ll use the letter \\(\\xi\\) (xi, or ksi) to indicate regression coefficients measured at the third level.\nWhen we subscript regression coefficients, we’ll need a number of subscripts equal to the level of the model at which this coefficient has been entered. The first subscript will indicate the level-1 coefficient with which this particular coefficient is associated, the second subscript will indicates the level-2 coefficient with which it is associated, and so on. This means that each coefficient will have a number of subscripts equal to the level of the model. As a really complicated example, if a coefficient is labeled as \\(\\xi_{021}\\), this indicates that the coefficient is the first slope coefficient (the 1 at the end) in a model for the second level-2 slope coefficient (the 2 in the second position) in a model for the level-1 intercept. Similarly, the first subscript in a random effect will indicate the level-1 coefficient with which it is associated, and the second will indicate the level-2 coefficient with which is is associated. Random effects will always have one fewer subscript than the coefficients at that level. As you can imagine, subscripts quickly get out of hand as we introduce more and more levels to a model.\nWe’ll use \\(\\sigma^2_p\\) to indicate the variance of the level-2 residual for the \\(p\\)th random effect (starting at 0 for the intercept). I’m not yet sure how to do the subscripting at level-3, and for now am hoping to just wing it. The correlation between the \\(p\\)th and \\(q\\)th random effects will be subscripted \\(pq\\), and correlations will always be identified with a \\(\\rho\\) (rho, not p).\nLongitudinal models are similar, except for the subscripting. I’ll always (probably) subscript the first level with \\(t\\), for time. The second level will become \\(i\\) (assuming that we’re looking at growth in students or other individuals), followed by \\(j\\) for the third level (we probably won’t include a fourth level).\n\n\n21.1.2 Interpreting fixed effects\nOkay, that was complicated, although I think writing down definitions and rules is often more challenging than applying them. Now let’s practice some interpretations, going back to our model.\nAt the first level, we interpret (almost) exactly as we would in a standard regression model. If we have\n\\[mathach_{ij} = \\beta_{0j[i]} + \\beta_{1j[i]}SES_i + \\varepsilon_i,\\]\nthen we interpret \\(\\beta_{0j}\\) as the predicted value of \\(mathach\\) for a student of 0 SES (which represents the grand mean) who is located in school \\(j\\). Because this is a multilevel model, different schools have different intercepts. Similarly, we can interpret \\(\\beta_{1j[i]}\\) as the expected difference in math achievement associated with a one-unit difference in SES for students in school \\(j\\). We don’t interpret it, but \\(\\varepsilon_i\\) indicates the difference between what we observed for this student and what we predicted based on her or his school and SES.\nWe interpret the level-2 units depending on the coefficients they predict. For the school-intercept we have\n\\[\\beta_{0j} = \\gamma_{00} + \\gamma_{01}sector_j + u_{0j}.\\]\nWe can interpret \\(\\gamma_{00}\\) as the predicted intercept for schools for which \\(sector = j\\) (i.e., public schools). We can interpret \\(\\gamma_{01}\\) as the predicted difference in school intercepts between Catholic and public schools. Although it’s less common, we can also interpret the residual for school \\(j\\), \\(u_{0j}\\), because you can’t tell me what to do. \\(u_{0j}\\) represents the difference between the observed/inferred intercept for school \\(j\\) and the predicted intercept.\nTurning to the model for the slope, we have\n\\[\\beta_{1j} = \\gamma_{10} + \\gamma_{11}sector_j + u_{0j}.\\]\nHere \\(\\gamma_{10}\\) is the predicted slope for SES in public schools, while \\(\\gamma_{11}\\) is the mean difference in slopes between Catholic and public school. Finally, \\(u_1j\\) is the difference between the slope observed/inferred for school \\(j\\) and the slope predicted by the model.\nWe can also interpret these coefficients at the student level. Rewrite the model by substituting \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}sector_j + u_{0j}\\) and \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}sector_j + u_{1j}\\) to obtain\n\\[\\begin{aligned}\nmathach_i &= \\gamma_{00} + \\gamma_{01}sector_{j[i]} + u_{0j[i]} + (\\gamma_{10} + \\gamma_{11}sector_{j[i]} + u_{1j[i]})SES_i + \\varepsilon_i \\\\\n&= \\gamma_{00} + \\gamma_{01}sector_{j[i]} + \\gamma_{10}SES_i + \\gamma_{11}sector_{j[i]}SES_i + (u_{0j[i]} + u_{1j[i]}SES_i + \\varepsilon_i).\n\\end{aligned}\\]\nNow we can interpret these coefficients as in a typical one-level linear regression model.\n\n\\(\\gamma_{00}\\) is the predicted mean value of \\(mathach\\) for students of \\(SES = 0\\) in public schools;\n\\(\\gamma_{01}\\) is the predicted difference in \\(mathach\\) between students of \\(SES = 0\\) in Catholic schools and similar peers in public schools;\n\\(\\gamma_{10}\\) is the predicted difference in \\(mathach\\) associated with a one-unit difference in SES for students in public schools; and\n\\(\\gamma_{11}\\) is the predicted difference in the above difference between students in Catholic schools and students in public schools.\n\nEither interpretation is acceptable, and you should base your decision on how you’re framing your question.\n\n\n21.1.3 Interpreting variance-covariance parameters\nNow we’re going to turn to the variance-covariance matrix for the random offsets, namely\n\\[\\begin{aligned}\n\\Sigma = \\begin{pmatrix}\nu_{0j}\\\\\nu_{1j}\\\\\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\sigma^2_0 & \\rho\\sigma_0\\sigma_1\\\\\n\\rho\\sigma_0\\sigma_1 & \\sigma^2_1\n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned}\\]\nThe variance of a random offset (e.g., \\(\\sigma_0^2\\), the variance of \\(u_{0j}\\)) represents how variable the coefficient associated with that coefficient is, conditional on the variables in the model. The correlations (e.g., \\(\\rho\\), the only correlation in this model) represent the tendency of the random offsets to covary, i.e., to be associated with each other.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Interpreting Coefficients</span>"
    ]
  },
  {
    "objectID": "within_v_between.html",
    "href": "within_v_between.html",
    "title": "22  Within, Between, and Contextual Effects",
    "section": "",
    "text": "22.1 Fitting the Models\nWe next fit a variety of models to compare:\nols &lt;- lm(mathach ~ ses, hsb)\nfe &lt;- lm(mathach ~ ses + factor(schoolid), hsb)\nri &lt;- lmer(mathach ~ ses + (1|schoolid), hsb)\nri_within &lt;- lmer(mathach ~ grp_center_ses + (1|schoolid), hsb)\nri_between &lt;- lmer(mathach ~ grp_mean_ses + (1|schoolid), hsb)\nre_wb &lt;- lmer(mathach ~ grp_center_ses + grp_mean_ses + (1|schoolid), hsb)\ncontextual &lt;- lmer(mathach ~ ses + grp_mean_ses + (1|schoolid), hsb)\n\ntab_model(ols, fe, ri, ri_within, ri_between, re_wb, contextual,\n          p.style = \"stars\",\n          show.ci = FALSE,\n          show.se = TRUE,\n          keep = \"ses\",\n          show.dev = TRUE,\n          dv.labels = c(\"OLS\",\n                        \"Fixed Effects\",\n                        \"Rand. Int.\",\n                        \"RI Within\",\n                        \"RI Between\",\n                        \"REWB\",\n                        \"Mundlak\"))\n\n\n\n \nOLS\nFixed Effects\nRand. Int.\nRI Within\nRI Between\nREWB\nMundlak\n\n\nPredictors\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\n\n\nses\n3.18 ***\n0.10\n2.19 ***\n0.11\n2.39 ***\n0.11\n\n\n\n\n\n\n2.19 ***\n0.11\n\n\ngrp center ses\n\n\n\n\n\n\n2.19 ***\n0.11\n\n\n2.19 ***\n0.11\n\n\n\n\ngrp mean ses\n\n\n\n\n\n\n\n\n5.86 ***\n0.36\n5.87 ***\n0.36\n3.68 ***\n0.38\n\n\nRandom Effects\n\n\n\nσ2\n \n \n37.03\n37.01\n39.16\n37.02\n37.02\n\n\n\nτ00\n \n \n4.77 schoolid\n8.67 schoolid\n2.64 schoolid\n2.69 schoolid\n2.69 schoolid\n\n\nICC\n \n \n0.11\n0.19\n0.06\n0.07\n0.07\n\n\nN\n \n \n160 schoolid\n160 schoolid\n160 schoolid\n160 schoolid\n160 schoolid\n\nObservations\n7185\n7185\n7185\n7185\n7185\n7185\n7185\n\n\nR2 / R2 adjusted\n0.130 / 0.130\n0.235 / 0.218\n0.077 / 0.182\n0.044 / 0.225\n0.123 / 0.179\n0.167 / 0.224\n0.167 / 0.224\n\n\nDeviance\n295643.779\n259918.446\n46641.008\n46720.415\n46959.128\n46563.821\n46563.821\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Within, Between, and Contextual Effects</span>"
    ]
  },
  {
    "objectID": "within_v_between.html#interpretation",
    "href": "within_v_between.html#interpretation",
    "title": "22  Within, Between, and Contextual Effects",
    "section": "22.2 Interpretation",
    "text": "22.2 Interpretation\n\n22.2.1 OLS\n\n\nlm(formula = mathach ~ ses, data = hsb)\n\n\nIgnoring school membership, students who are 1-unit higher in SES are predicted to score 3.18 points higher in math. This is generally not a preferred model.\n\n\n22.2.2 Fixed Effects\n\n\nlm(formula = mathach ~ ses + factor(schoolid), data = hsb)\n\n\nFor students within a given school, students who are 1-unit higher in SES are predicted to score 2.19 points higher in math. Fixed effects models focus on within-school comparisons: we are looking at how students within schools relate to each other, and then averaging this relationship across all our schools to get our final estimate.\n\n\n22.2.3 Random Intercepts\n\n\nlmer(formula = mathach ~ ses + (1 | schoolid), data = hsb)\n\n\nStudents who are 1-unit higher in SES are predicted to score 2.39 points higher in math; schools that are 1-unit higher in mean SES are predicted to have mean math scores 2.39 points higher.\nThe random intercept model gives a precision-weighted average of the within and between effects. Looking at the other models, note that our within effect is 2.19 and our between effect is 5.86. If the RE assumption holds, these are the same in the population, so we get more precision by averaging them together. However, in social science, they are rarely the same, making this model provide a weird blend of two kinds of mechanism.\n\n\n22.2.4 Random Intercepts, Within Effect\n\n\nlmer(formula = mathach ~ grp_center_ses + (1 | schoolid), data = hsb)\n\n\nHolding constant school, students who are 1-unit higher in SES are predicted to score 2.19 points higher in math. This is the same coefficient as the FE model, but in an RI framework. We have “controlled for school” manually by demeaning the SES variable.\n\n\n22.2.5 Random Intercepts, Between\n\n\nlmer(formula = mathach ~ grp_mean_ses + (1 | schoolid), data = hsb)\n\n\nSchools that are 1 unit higher in mean SES are predicted to have mean math scores 5.86 points higher. This looks very large, but remember that variation in school mean SES is typically much less than the variation in student ses scores. In particular, we can calculate the standard deviation of school mean ses’es to get:\n\nschools &lt;- hsb |&gt;\n  dplyr::select( schoolid, grp_mean_ses ) |&gt;\n  unique() |&gt;\n  summarise( n = n(),\n             sd = sd( grp_mean_ses ) )\nschools\n\n# A tibble: 1 × 2\n      n    sd\n  &lt;int&gt; &lt;dbl&gt;\n1   160 0.414\n\n\n\n\n22.2.6 Random Effects within and Between\n\n\nlmer(formula = mathach ~ grp_center_ses + grp_mean_ses + (1 | \n    schoolid), data = hsb)\n\n\nHolding constant school, students who are 1-unit higher in SES are predicted to score 2.19 points higher in math; schools that are 1-unit higher in mean SES are predicted to have mean math scores 5.86 points higher. We get the within and between effects in a single model!\n\n\n22.2.7 Contextual/Mundlak\n\n\nlmer(formula = mathach ~ ses + grp_mean_ses + (1 | schoolid), \n    data = hsb)\n\n\nHolding constant school, students who are 1-unit higher in SES are predicted to score 2.19 points higher in math; holding constant student SES, a student that attends a school with 1-unit higher in mean SES are predicted to have mean math scores 3.68 points higher. The contextual effect is the difference in the within and between effects (note that 5.86 - 2.19 = 3.68, up to rounding), and, in principle, its significance test allows us to determine if having both is necessary.\nMathematically, the Mundlak model and REWB are identical, as you can see from the deviance statistics. You would choose one over the other depending on your preferred interpretation.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Within, Between, and Contextual Effects</span>"
    ]
  },
  {
    "objectID": "within_v_between.html#further-reading",
    "href": "within_v_between.html#further-reading",
    "title": "22  Within, Between, and Contextual Effects",
    "section": "22.3 Further Reading",
    "text": "22.3 Further Reading\nCheck out the Raudenbush and Bryk pages on within vs. between. Also see, if desired, read Antonakis, Bastardoz, and Rönkkö (2019).\n\n\n\n\nAntonakis, John, Nicolas Bastardoz, and Mikko Rönkkö. 2019. “On Ignoring the Random Effects Assumption in Multilevel Models: Review, Critique, and Recommendations.” Organizational Research Methods 24 (2): 443–83. https://doi.org/10.1177/1094428119877457.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Within, Between, and Contextual Effects</span>"
    ]
  },
  {
    "objectID": "visual_tour_of_nulls.html",
    "href": "visual_tour_of_nulls.html",
    "title": "23  A visual guide to parameters",
    "section": "",
    "text": "23.1 Null hypotheses on slopes\nNow consider three different null hypothesis for the slope:\nWe are going to generate data where everything is as the original model except for the null. We will then see how the data look different. Witness!\nNo random slope still gives different lines for each school, but they are very similar. First, our catholic schools all have one slope and the public schools another. The only difference is we allow the intercepts to vary, which gives the two bundles of lines.\nNo average slope means our public school slopes are 0, on average. Note the Catholic schools have a positive slope on average–this is due to the \\(\\gamma_{11}\\) term.\nFinally, if \\(\\gamma_{11} = 0\\), then our Catholic and public slopes are all centered around the average slope of \\(\\gamma_{10}\\)–but each school still has its own slope and the Catholic schools are still shifted higher",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A visual guide to parameters</span>"
    ]
  },
  {
    "objectID": "visual_tour_of_nulls.html#null-hypotheses-on-slopes",
    "href": "visual_tour_of_nulls.html#null-hypotheses-on-slopes",
    "title": "23  A visual guide to parameters",
    "section": "",
    "text": "\\(\\tau_{11} = 0\\): This removes the random slope term. Note that this also implies \\(\\tau_{01}\\) = 0.\n\\(\\gamma_{10} = 0\\): This removes the overall average slope. We still allow individual schools to vary, and also for Catholic schools to be systematically different from public.\n\\(\\gamma_{11} = 0\\): This removes systematic differences between Catholic and public schools.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A visual guide to parameters</span>"
    ]
  },
  {
    "objectID": "visual_tour_of_nulls.html#and-what-about-intercepts",
    "href": "visual_tour_of_nulls.html#and-what-about-intercepts",
    "title": "23  A visual guide to parameters",
    "section": "23.2 And what about intercepts?",
    "text": "23.2 And what about intercepts?\nLet’s do things to the school level intercepts:\n\n\\(\\tau_{00} = 0\\): This removes the random intercept, but still lets the slopes vary. This is not something we would normally think would happen in practice, but it helps us see how the different parameters matter.\n\\(\\gamma_{01} = 0\\): This means there is no shift in intercepts between Catholic and public schools.\n\\(\\gamma_{00} = 0\\): This means that the public schools overall grand intercept is 0.\n\nIn all of the above, we are leaving the slope part of our model alone. Each school’s intercept is calculated from the grand intercept, the shift due to being Catholic, and the random intercept. Changing them changes things like this:\n\n\n\n\n\n\n\n\n\nIn the first plot, note how all the lines go through the same intercept point. The varying slopes give different lines.\nThe second plot has the Catholic and public schools sharing the same intercepts, but the Catholic schools have steeper slopes in general.\nThe third plot lowers the schools so the public school intercepts are 0 on average. The Catholic schools are still shifted higher by \\(\\gamma_{01}\\).",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A visual guide to parameters</span>"
    ]
  },
  {
    "objectID": "visual_tour_of_nulls.html#and-what-are-the-taus",
    "href": "visual_tour_of_nulls.html#and-what-are-the-taus",
    "title": "23  A visual guide to parameters",
    "section": "23.3 And what are the taus?",
    "text": "23.3 And what are the taus?\nLet’s drop all the Catholic and public differences (i.e., \\(\\gamma_{01} = \\gamma_{11} = 0\\)) and crank up the \\(tau\\) values:\n\n\\(\\tau_{00} = BIG\\): The intercepts vary a lot.\n\\(\\tau_{11} = BIG\\): The slopes vary a lot.\n\\(\\tau_{01} = BIG\\): The covariance is large (i.e., the correlation of the random intercepts and slopes is very positive)\n\n\n\n\n\n\n\n\n\n\nIn the first plot, our lines are scattered vertically a lot, and in the second plot our slopes are all over the place. In the third plot, the steepest slope has the highest intercept.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>A visual guide to parameters</span>"
    ]
  },
  {
    "objectID": "reg_assumptions.html",
    "href": "reg_assumptions.html",
    "title": "24  MLM Assumptions",
    "section": "",
    "text": "24.1 Omitted variable bias\nConsider the following numerical example:\nN = 100\ndat = data.frame( X1 = rnorm( N ) )\ndat = mutate( dat, \n              X2 = X1 + rnorm( N ),\n              Y = 3 + 0.5 * X1 + 1.5 * X2 + rnorm( N ) )\nThe above code makes a dataset with X2 correlated with X1, and a Y that is a function of both. The true model here is \\[ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_{i} \\]with coefficients \\(\\beta = (3, 0.5, 1.5)\\).\nWe fit two models, one including both covariates, and one including only one:\nM0 = lm( Y ~ 1 + X1 + X2 , data = dat )\nM1 = lm( Y ~ 1 + X1, data = dat )\nOur results:\ntab_model(M0, M1, p.style = \"stars\",\n          show.ci = FALSE, show.se = TRUE)\n\n\n\n\n \nY\nY\n\n\nPredictors\nEstimates\nstd. Error\nEstimates\nstd. Error\n\n\n(Intercept)\n3.10 ***\n0.11\n2.95 ***\n0.18\n\n\nX1\n0.44 **\n0.15\n1.92 ***\n0.16\n\n\nX2\n1.51 ***\n0.11\n\n\n\n\nObservations\n100\n100\n\n\nR2 / R2 adjusted\n0.856 / 0.853\n0.584 / 0.580\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\nNote our coefficient for the kept variable is completely wrong when we omit a correlated variable. This is omitted variable bias, and in terms of our assumptions we are in a circumstance where the true residuals in our model are not centered around 0 for all values of X1, since they include the X2 effect which is correlated with X1. We can see this graphically by calculating the true residuals for our data (when we do not include X2) and then plotting them vs. X1:\ndat = mutate( dat, e = Y - 3 - 0.5 * X1 )\nggplot( dat, aes( X1, e ) ) +\n    geom_point() +\n    geom_hline( yintercept = 0 )\nNote how our residuals (which includes X2) are positive for bigger X1, due to the correlation of X1 and X2. We do not have independence between X1 and e, or mathematically put \\(E[ e | X_1 ] \\neq 0\\) for some values of \\(X_1\\).\nIn math we can write this for our “no X2” model:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\tilde{\\epsilon}_i = \\beta_0 + \\beta_1 X_{1i} + (\\beta_2 X_{2i} + \\epsilon_{i})\n\\]\nI.e., our residual in our model is actually the secret \\(X_2\\) effect and the original residual. This means our \\(\\tilde{\\epsilon}_i\\) are correlated with \\(X_{1i}\\)!\nConclusion: On one hand, we have the wrong estimate for \\(\\beta_1\\). On the other, the estimate we do get is fine if we view it as the best description of the data. In our model without X2, we are getting the best description of our data using the model we fit to it. We just need to remember that the interpretation of our coefficient includes any confounding effect of X2 on X1. In other words, omitted variable bias is usually part of a critique about causal claims, not descriptive ones.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>MLM Assumptions</span>"
    ]
  },
  {
    "objectID": "reg_assumptions.html#independence-assumptions",
    "href": "reg_assumptions.html#independence-assumptions",
    "title": "24  MLM Assumptions",
    "section": "24.2 Independence assumptions",
    "text": "24.2 Independence assumptions\nThe independence assumptions are key. When we do not take violations of independence into account, we can be overly confident of our estimates in that our standard errors can be very, very wrong.\nGenerally with MLM we should think of these assumptions in terms of how we sampled our data. If we sampled our data by sampling a collection of schools, and then individuals within those schools, then we have two levels. We then need to ask two questions:\n\nWere the schools sampled independently?\nWere the students sampled independently within the schools?\n\nIf yes to both, we have met both our independence assumptions! We have met them even if the students are clustered in classes within their schools. As long as we did not sample using those classes (or other clusters), we are ok as our sample of students will be representative of the school they are in.\nTo be crystal clear, if some clustering is not part of how units are sampled, then it can be ignored. So if you are sampling kids from a school district at random, and later learn they are in different neighborhoods (or are grouped in some other natural way like households), you do not need to cluster by neighborhood. That said, you might want to model neighborhood as a cluster to investigate how things vary across those clusters.\nAnd what about if you sampled at the school level and surveyed all students within each sampled school. Do you need to worry about natural clustering such as classrooms in the school? In this case we are somewhat ok. First, we can pretend our students come from some hypothetical larger population of students. This is of course odd if we sampled all in a school, but we can think of this as something like “we have this collection of students, but we want to understand how much uncertainty we have regarding the students around their school mean if these students are here in some part due to random chance.” This explanation is admittedly hand-wavy, but it is implicitly done all the time. An important note is this treats the classrooms as fixed aspects of the school: we are estimating an average across the school’s classrooms, and thinking of students as sampled, but not the classroom experiences.\nThat said, the school intercept might not fully capture complex dependence within the school (e.g., from student spillover within classrooms); to be 100% safe, use cluster robust standard errors as these allow for arbitrary correlation of students within school. By contrast, the random intercept model says student residuals are independent within school, meaning the shared school effect captures all the correlation of students.\nFor further discussion, see this blog post/document from the World bank which says clustered SEs are not necessary (in OLS) unless sampling was conducted at the cluster-level and that econometricians often overuse them.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>MLM Assumptions</span>"
    ]
  },
  {
    "objectID": "reg_assumptions.html#number-of-clusters-needed",
    "href": "reg_assumptions.html#number-of-clusters-needed",
    "title": "24  MLM Assumptions",
    "section": "24.3 Number of clusters needed?",
    "text": "24.3 Number of clusters needed?\nNeeded number of clusters is not reall an assumption per se, but onwards!\nHere is a quick FAQ:\nQ: Why should you worry if the number of group is small?\nA: With few clusters, estimation is hard just like having a small dataset with OLS. The variance parameters in particular are difficult. The standard errors can be wildly off.\nQ: When you say “at least 20” you mean for the number of j’s, right?\nA: Yes, number of clusters. Mostly Harmless Econometrics readers might recall a discussion of 42 clusters (8.2.3), which contributes to this debate of the appropriate number of level two units.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>MLM Assumptions</span>"
    ]
  },
  {
    "objectID": "reg_assumptions.html#a-note-on-testing-assumptions",
    "href": "reg_assumptions.html#a-note-on-testing-assumptions",
    "title": "24  MLM Assumptions",
    "section": "24.4 A note on testing assumptions",
    "text": "24.4 A note on testing assumptions\nIn this class we do not really talk about how to test these assumptions. In general, we usually test with plots, like with classic OLS. For example we can plot a histogram of the residuals and see if they are normally distributed. We can plot them vs. some covariate to check for heteroskedasticity as well.\nSimilarly, we can also plot a histogram of empirical bayes estimated random effects to see if they are normally distributed, or plot those against (level 2) covariates to check for heteroskedasticity.\nYou can also plot residuals by level two unit to look for heteroskedasticity. Make a boxplot for each level two unit and see if they are all the same size (roughly).\nSee Raudenbush and Bryk for more discussion of what to check.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>MLM Assumptions</span>"
    ]
  },
  {
    "objectID": "model_representations.html",
    "href": "model_representations.html",
    "title": "25  Model Representations",
    "section": "",
    "text": "25.1 The Two-Level Random Intercept Model\nWe will use the “double-indexing” that is the most common notation for multilevel models (not the Gelman and Hill bracket (\\(j[i]\\)) notation). Treatment is at the school level: let \\(Z_j\\) be an indicator of whether school \\(j\\) was treated (so a 0/1 variable). Then, for student \\(i\\) in school \\(j\\) we have \\[\\begin{aligned}\nY_{ij} &= \\alpha_{j} + \\beta_{1} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n\\alpha_{j} &= \\gamma_{0} + \\gamma_{1} Z_{j} + \\gamma_{2} S_{j} + u_{j} \\\\\n\\end{aligned}\\] with \\(Y_{ij}\\) being the reading level of the student, \\(R_{ij}\\) being a dummy variable of student’s “at risk” status, \\(X_{ij}\\) being an important student demographic variable (e.g., prior reading level), and \\(S_j\\) being a school-level covariate (such as a school quality measure).\nThis is the two-level model. Level 1 is the first equation with the distribution on the residuals of \\(\\epsilon_{ij} \\sim N( 0, \\sigma^2 )\\). Level 2 is the second equation with a distribution of random effects of \\[u_{j} \\sim N( 0, \\sigma^2_\\alpha ) .\\] The \\(\\sigma^2_\\alpha\\) is the variance of the random intercept.\nCall the \\(\\beta_{0j}\\) the random intercept and \\(u_{j}\\) the random effect. The \\(u_{j}\\) is the residual of the level 2 model,. In R, we would say coef() for the intercept (including the mean \\(\\gamma_0\\)) and ranef() for the random effect. In math, coef() gives \\(\\gamma_0 + u_j\\) and ranef() gives only \\(u_j\\). Neither include the \\(\\gamma_1\\) or \\(\\gamma_2\\); these will be separate columns you get from coef().\nRemarks:",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model Representations</span>"
    ]
  },
  {
    "objectID": "model_representations.html#the-two-level-random-intercept-model",
    "href": "model_representations.html#the-two-level-random-intercept-model",
    "title": "25  Model Representations",
    "section": "",
    "text": "We have completely pooled the coefficient for \\(R_{ij}\\) and \\(X_{ij}\\): we are assuming all the schools have the same relationship between the outcome and these covariates.\nThe intercept \\(\\alpha_{j}\\) is the expected (predicted average) outcome of a not-at-risk student with \\(X_{ij} = 0\\). Different schools have different means. In particular, treatment schools have a mean of \\(\\gamma_1\\) more than control; this is the treatment impact.\n\n\n25.1.1 The Reduced Form Model\nIf we plug in our 2nd level into the first we get the following: \\[\\begin{aligned}\nY_{ij} &= \\beta_{0j} + \\beta_{1} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n&= (\\gamma_{0} + \\gamma_{1} Z_{j} + \\gamma_{2} S_{j} + u_{j}) + \\beta_1 R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n&= \\gamma_{0} + \\gamma_{1} Z_{j} + \\gamma_{2} S_{j} + \\beta_{1} R_{ij} + \\beta_{2} X_{ij} + (u_{j} + \\epsilon_{ij})\n\\end{aligned}\\] The \\(u_{0j} + \\epsilon_{ij}\\) is our total random error. It is how much our prediction of a new, unknown student, would differ from their actual score if we didn’t know the school’s random effect. The rest of the model is the mean model or structural portion of the model.\nThis is also called the reduced form; it is what econometricians work with. They will write the entire residual as \\(\\varepsilon_{ij}\\), however: \\[\\begin{aligned}\nY_{ij} &= \\gamma_{0} + \\gamma_{1} Z_{j} + \\gamma_{2} S_{j} + \\beta_{1} R_{ij} + \\beta_{2} X_{ij} + \\varepsilon_{ij}\n\\end{aligned}\\]\nRemarks:\n\nThe reduced form helps us see our treatment effect more clearly. It is a shift in outcome of \\(\\gamma_1\\) for treated students.\nThe \\(\\gamma_{0}\\) is the overall mean reading level for students with \\(X_{ij}=0\\) for not-at-risk students (\\(R_{ij}=0\\)) in control schools with \\(S_j = 0\\).\nWe subscript school-level covariates with only a \\(j\\) vs. individual-level covariates get an \\(ij\\). If you want, you can index everything by \\(ij\\); the fact that \\(S_{ij}\\) will then be the same for all students \\(i\\) in school \\(j\\) is hidden in the data. But it does make it look very much like OLS with a weird error term: \\[\nY_{ij} = \\gamma_{0} + \\gamma_{1} Z_{ij} + \\gamma_{2} S_{ij} + \\beta_{1} R_{ij} + \\beta_{2} X_{ij} + (u_{j} + \\epsilon_{ij})\n\\]\nYou can call all the different pieces by different letters to indicate whether you care about them or not. E.g., \\[Y_{ij} = \\mu + \\tau Z_{ij}  + \\beta_1 S_{ij} + \\beta_2 R_{ij} + \\beta_3 X_{ij} + (u_{0j} +  \\epsilon_{ij}) .\\] Here \\(\\tau\\) is our treatment effects of interest. The \\(\\beta\\)’s are just adjustments to be ignored. The \\(\\mu\\) is the grand mean (for those not treated, with \\(S_{ij} = 0\\) and \\(R_{ij} = 0\\) and \\(X_{ij} = 0\\)). People often use \\(\\mu\\) for mean and \\(\\tau\\) for treatment.\n\n\n\n25.1.2 Fitting it in lmer\nWe fit it as:\n    lmer( Y ~ R + Z + X + S + (1|id), data=dat )",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model Representations</span>"
    ]
  },
  {
    "objectID": "model_representations.html#the-two-level-random-slopes-model",
    "href": "model_representations.html#the-two-level-random-slopes-model",
    "title": "25  Model Representations",
    "section": "25.2 The Two-Level Random Slopes Model",
    "text": "25.2 The Two-Level Random Slopes Model\nNow let’s get very complex to really unpack notational stuff. We are going to let treatment not only impact the average outcome in schools, but also allow treatment to differentially impact students who are “at risk”. I.e., we are going to have two treatment impacts, one for not at risk, and one for at risk. This is an interaction of risk status and treatment.\nFurthermore, we are going to let different schools have different gaps between at risk and not at risk, but allowing a random effect for the at risk coefficient.\nUsing our same variables as above, we have, for student \\(i\\) in school \\(j\\) \\[\\begin{aligned}\nY_{ij} &= \\beta_{0j} + \\beta_{1j} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} Z_{j} + \\gamma_{02} S_{j} + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j} .\n\\end{aligned}\\]\nThis is the two-level model. Level 1 is the first equation with the distribution on the residuals of \\(\\epsilon_{ij} \\sim N( 0, \\sigma^2 )\\). Level 2 are the second and third equations, and the distribution of random effects of \\[\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{10} \\\\\n\\tau_{10} & \\tau_{11} \\\\\n\\end{pmatrix}\n\\end{bmatrix}\n\\] The \\(\\tau_{00}\\) is the variance of the random intercept. \\(\\tau_{11}\\) is the variance of the random slope. \\(\\tau_{10}\\) is the covariance (not correlation) of the random effects. To get the correlation of random effects we have \\(\\rho = \\tau_{10} / \\sqrt{ \\tau_{00} } \\sqrt{ \\tau_{11} }\\). (Note that \\(\\tau_{10} = \\tau_{01}\\), meaning the covariance of A and B is the same as covariance of B and A, so we just write one of them.)\nCall the \\(\\beta_{0j}\\) the random intercept and \\(\\beta_{1j}\\) a random coefficient. We might call them both random coefficients. Call the \\(u_{0j}, u_{1j}\\), which are the residuals of the level 2 models, the random effects. In R, we would say coef() for the coefficients (including the means) and ranef() for the random effects.\nRemarks:\n\nWe have completely pooled the coefficient for \\(X_{ij}\\): we are assuming all the schools have the same relationship between the outcome and \\(X_{ij}\\). This is why we have no level 2 equation for \\(\\beta_{2}\\) and we do not index \\(\\beta_2\\) as \\(\\beta_{2j}\\).\nThe intercept \\(\\beta_{0j}\\) is the expected (predicted average) outcome of a not-at-risk student with \\(X_{ij} = 0\\). Different schools have different means.\nThe achievement gap of at-risk and not-at-risk students for control schools is measured by \\(\\gamma_{10}\\). For treatment schools it is \\(\\gamma_{10} + \\gamma_{11}\\).\nThe \\(\\gamma_{01}\\) is the average treatment effect for not-at-risk students.\nThe \\(\\gamma_{01} + \\gamma_{11}\\) is the average treatment effect for the at-risk students.\nIf we find \\(\\gamma_{11} \\neq 0\\) then the average effects differ for our two types of students, and the change in the achievement gap induced by treatment is measured by \\(\\gamma_{11}\\).\nIn this model, the school-level covariate explains overall differences in reading between schools, but does not relate to the size of treatment impact in a school, or relate to the at-risk vs. not-at-risk achievement gap.\n\n\n25.2.1 The level 2 covariate matrix.\nSometimes people like to write the correlation matrix using other parameterizations. E.g., we might see \\[\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\sigma^2_\\alpha & \\rho \\sigma_\\alpha \\sigma_R \\\\\n& \\sigma^2_R \\\\\n\\end{pmatrix}\n\\end{bmatrix}\n\\] to indicate the cross-school variation in the intercept (\\(\\alpha\\)) and the risk gap (\\(R\\)). Now we specifically have written our correlation of random effects as \\(\\rho\\).\n\n\n25.2.2 The Reduced Form Model\nIf we plug in our 2nd level into the first we have to plug in both equations. If we do we get… a mess: \\[\\begin{aligned}\nY_{ij} &= \\beta_{0j} + \\beta_{1j} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n&= (\\gamma_{00} + \\gamma_{01} Z_{j} + \\gamma_{02} S_{j} + u_{0j}) + (\\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j}) R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n&= \\gamma_{00} + \\gamma_{01} Z_{j} + \\gamma_{02} S_{j} + u_{0j} + \\gamma_{10} R_{ij} + \\gamma_{11} Z_{j} R_{ij} + u_{1j} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n&= \\gamma_{00} + \\gamma_{01} Z_{j} + \\gamma_{02} S_{j} + \\gamma_{10} R_{ij} + \\gamma_{11} Z_{j} R_{ij} + \\beta_{2} X_{ij} + u_{0j} +  u_{1j} R_{ij} + \\epsilon_{ij} \\\\\n&= \\gamma_{00} + (\\gamma_{01} + \\gamma_{11} R_{ij} ) Z_{j}  + \\gamma_{02} S_{j} + \\gamma_{10} R_{ij} + \\beta_{2} X_{ij} + (u_{0j} +  u_{1j} R_{ij} + \\epsilon_{ij}) \\\\\n\\end{aligned}\\] The \\(u_{0j} + u_{1j} R_{ij} + \\epsilon_{ij}\\) is our total random error. It is how much our prediction of a new, unknown student, would differ from their actual score if we didn’t know the school’s random effect. The rest of the model is the mean model or structural portion of the model.\nThis is our reduced form; it is what econometricians work with. They will write the entire residual as \\(\\varepsilon_{ij}\\), however: \\[\\begin{aligned}\nY_{ij} &= \\gamma_{00} + (\\gamma_{01} + \\gamma_{11} R_{ij} ) Z_{j}  + \\gamma_{02} S_{j} + \\gamma_{10} R_{ij} + \\beta_{2} X_{ij} + \\varepsilon_{ij} \\\\\n\\end{aligned}\\]\nRemarks:\n\nThe reduced form helps us see our treatment effects and treatment variation across groups more clearly. We can put both terms involving the treatment indicator in parenthesis (final line above) to show how treatment is different by \\(\\gamma_{11}\\) for the at-risk students.\nThe difference in treatment effects between at-risk and not at-risk is an interaction between student risk and treatment assignment of the school (note the \\(Z_j R_{ij}\\) term).\nThe \\(\\gamma_{00}\\) is the overall mean reading level for students with \\(X_{ij}=0\\) for not-at-risk students in control schools.\nThe \\(\\gamma_{10}\\) is the average difference between at-risk and not-at-risk students in control schools, across all schools.\nWe can rearrange our equations above to get \\[Y_{ij} = ( \\gamma_{00} + u_{0j}) + (\\gamma_{01} + \\gamma_{11} R_{ij} + u_{1j} ) Z_{j} + \\gamma_{02} S_{j} + \\gamma_{10} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} .\\] This shows the random intercept and random slope all bundled up.\nAs with the intercept model, you might call all the different pieces by different letters to indicate whether you care about them or not. E.g., \\[Y_{ij} = \\mu + \\tau Z_{ij}  + \\beta_1 S_{ij} + \\beta_2 R_{ij} + \\beta_3 X_{ij} + \\tau_{R} Z_{ij} R_{ij}  + (u_{0j} +  u_{1j} R_{ij} + \\epsilon_{ij}) .\\] Here \\(\\tau\\) and \\(\\tau_R\\) are our treatment effects of interest. The \\(\\beta\\)’s are just adjustments to be ignored. The \\(\\mu\\) is the grand mean. This model is the same as above, we are just changing names around.\n\n\n\n25.2.3 Fitting it in lmer\nWe fit it as:\n    lmer( Y ~ R * Z + X + S + (R|id), data=dat )\nTwo other ways of saying the same thing:\n    lmer( Y ~ 1 + R * Z + X + S + (1 + R|id), data=dat )\nand\n    lmer( Y ~ 1 + Z + S + R + Z:R + X + (1 + R|id), data=dat )\nRemarks:\n\nSee how the reduced form and lmer() align, especially if we write out what R automatically does with R * Z (R will expand R*Z into R + Z + R:Z automatically).\n\n\n\n25.2.4 The bracket-subscript notation from Gelman and Hill\nThe above is the “double-subscript” way of writing a model. By contrast, Gelman and Hill index with a nifty “bracket notation.” First, let \\(j[i]\\) indicate the school student \\(i\\) is attending. Then we have: \\[\\begin{aligned}\nY_{i} &= \\beta_{0j[i]} + \\beta_{1j[i]} R_{i} + \\beta_{2} X_{i} + \\epsilon_{i} \\\\\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} Z_{j} + \\gamma_{02} S_{j} + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j},\n\\end{aligned}\\] This is basically identical to the above, but if you are not familiar with the bracketing then things can get messy.\nThe advantage of this is we can then imagine each student gets their own unique id, \\(i\\), and then we can query where that student is via \\(j[i]\\). This can be useful when looking at crossed effects models, where units have different random effects for different things (e.g., for a test we might have observation \\(k\\) corresponding to a single answer for a test question, with \\(i[k]\\) being the student who answered it and \\(q[k]\\) being the question item).",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Model Representations</span>"
    ]
  },
  {
    "objectID": "model_cheat_sheet.html",
    "href": "model_cheat_sheet.html",
    "title": "26  Connecting the three dots: An HSB Model",
    "section": "",
    "text": "26.1 The mathematical model\nLevel 1 models: \\[\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} + \\beta_2 female_{ij} +  \\epsilon_{ij} \\\\\n\\epsilon_{ij} &\\sim N( 0, \\sigma^2_y ) \\\\\n\\end{aligned}\n\\]\nLevel 2 models: \\[\n\\begin{aligned}\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} sector_j + \\gamma_{02} meanSES_j + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} sector_j + u_{1j}\n\\end{aligned}\n\\] with \\[\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{01}\\\\\n& \\tau_{11} \\\\\n\\end{pmatrix}\n\\end{bmatrix} .\n\\]\nThe \\(\\tau_{01}\\) is the covariance of the random intercept and random slope. We usually look at the correlation of\n\\[ \\rho = \\frac{ \\tau_{01} }{ \\sqrt{ \\tau_{00} \\tau_{11} } } . \\]\nThe estimated \\(\\rho\\) is what R gives us in the printed output, rather than \\(\\tau_{01}\\).\nThe derivation of the reduced form is:\n\\[\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} + \\epsilon_{ij}\\\\\n&= \\left( \\gamma_{00} + \\gamma_{01} sector_j + \\gamma_{02} meanSES_j + u_{0j} \\right)+ (\\gamma_{10} + \\gamma_{11} sector_j + u_{1j}) ses_{ij} + \\beta_2 female_{ij} + \\epsilon_{ij} \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j + \\gamma_{02} meanSES_j  + u_{0j}  + \\gamma_{10}ses_{ij} + \\gamma_{11} sector_j ses_{ij} + u_{1j} ses_{ij} + \\beta_2 female_{ij} +  \\epsilon_{ij}  \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j + \\gamma_{02} meanSES_j +  \\gamma_{10}ses_{ij} + \\gamma_{11} sector_j ses_{ij} + \\beta_2 female_{ij}  + \\left(u_{0j} + u_{1j} ses_{ij} + \\epsilon_{ij} \\right)\n\\end{aligned}\n\\] This formula is what we will give to lmer() in R’s formula notation.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Connecting the three dots: An HSB Model</span>"
    ]
  },
  {
    "objectID": "model_cheat_sheet.html#how-many-parameters",
    "href": "model_cheat_sheet.html#how-many-parameters",
    "title": "26  Connecting the three dots: An HSB Model",
    "section": "26.2 How many parameters?",
    "text": "26.2 How many parameters?\nIt is useful to be able to identify all the parameters being estimated, which is why I frequently ask to count the number of parameters. Let’s do that for the above.\nThere are generally two kinds of parameters: the regression coefficients and the variance parameters. The regression coefficients are all the parameters that have no letter subscripts, since they are fixed parameters that describe our entire population. All the things with letter subscripts, e.g., \\(\\beta_{0j}\\), are specific to some group–we would estimate those with empirical bayes after we fit our model, but we are not estimating those parameters directly when we first fit our model. So in the above model, we would have two cluster-specific parameters for each cluster. 160 clusters, so 320 such parameters, none of which are part of our main model.\nSo in the model above we have a \\(\\beta_2\\) at level 1 (so it is the same for all the clusters) and 5 \\(\\gamma_{\\cdot\\cdot}\\) parameters at level 2.\nFor the variances, we often have a level one residual variance (unless we have a generalized model such as logistic or poisson where there is no variance term for level 1), and then the variances of the random effects. Each level will have their own variances, and the number of parameters depends on the size of the matrix (a 2x2 matrix has 3 parameters, 2 on the diagonal and 1 off-diagonal, for example).\nIn the case above, this would give 1 + 3 = 4 more variance parameters.\nTotal parameters is therefore 1+5 = 6 regression coefficients, and 1+3 = 4 variance, for a total of 10 parameters.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Connecting the three dots: An HSB Model</span>"
    ]
  },
  {
    "objectID": "model_cheat_sheet.html#the-lmer-code",
    "href": "model_cheat_sheet.html#the-lmer-code",
    "title": "26  Connecting the three dots: An HSB Model",
    "section": "26.3 The lmer code",
    "text": "26.3 The lmer code\n\nM1 = lmer( mathach ~ 1 + female + ses*sector + \n             meanses + (1+ses|id),\n           data = dat )\n\nThis code is the exact same model, using the fact that ses*sector means ses + sector + ses:sector. I.e., the above is exactly the same as this more explicitly written R code:\n\nM1 = lmer( mathach ~ 1 + sector + meanses + ses + sector:ses + female + (1+ses|id),\n           data = dat )\n\nEach term in the expanded formula corresponds to a math symbol in the mathematical model. The (1+ses|id) make our random effects, and tie to all the \\(\\tau\\) terms. The residual variance \\(\\sigma^2_y\\) is the only parameter not explicitly listed in the above model.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Connecting the three dots: An HSB Model</span>"
    ]
  },
  {
    "objectID": "model_cheat_sheet.html#the-output",
    "href": "model_cheat_sheet.html#the-output",
    "title": "26  Connecting the three dots: An HSB Model",
    "section": "26.4 The output",
    "text": "26.4 The output\n\ndisplay( M1 )\n\nlmer(formula = mathach ~ 1 + sector + meanses + ses + sector:ses + \n    female + (1 + ses | id), data = dat)\n            coef.est coef.se\n(Intercept) 12.79     0.21  \nsector       1.29     0.29  \nmeanses      3.04     0.37  \nses          2.73     0.14  \nfemale      -1.18     0.16  \nsector:ses  -1.31     0.21  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 1.45          \n          ses         0.18     0.65 \n Residual             6.05          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46482.9, DIC = 46445.1\ndeviance = 46454.0 \n\n\nNow, using this output, we have estimates for all our mathematical modeling parameters:\n\n\\(\\gamma_{00} = 12.79\\) - The overall average math achievement for a student with 0 ses in a public school with 0 mean SES.\n\\(\\gamma_{01} = 1.29\\) - The average difference between otherwise equivilent catholic and public schools.\n\\(\\gamma_{02} = 3.04\\) - The impact on average achievement due to mean SES of schools. Higher SES schools have higher achievement.\n\\(\\gamma_{10} = 2.73\\) - The average slope of ses vs. math achievement in public schools.\n\\(\\beta_2 = -1.18\\) - The gender gap; girls have lower math scores on average.\n\\(\\gamma_{11} = -1.31\\) - The difference in slope between public and catholic schools (catholic schools have flatter slopes).\n\\(\\tau_{00} = 1.45^2\\) - Variation in overall intercept of schools (within category of public or catholic, and beyond mean SES).\n\\(\\tau_{11} = 0.18^2\\) - The variation in the random slopes for ses vs. math achievement.\n\\(\\rho = 0.65\\) - The random intercepts are correlated with random slopes. High achievement schools have more discrepancy between low and high ses students.\n\\(\\sigma_y = 6.05\\) - The unexplained student variation within school.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Connecting the three dots: An HSB Model</span>"
    ]
  },
  {
    "objectID": "growth_models_predictors.html",
    "href": "growth_models_predictors.html",
    "title": "27  Predictors in Longitudinal Growth Models",
    "section": "",
    "text": "27.1 Tips for growth models\nStart with an unconditional growth model, i.e., don’t include any level-1 or level-2 predictors. This model provides useful empirical evidence for determining a proper specification of the individual growth equation and baseline statistics for evaluating more complicated level-2 models.\nThe nature of the predictor in longitudinal analysis determines where it gets added to the model: Time-invariant predictors always go in level-2 (subject level) model Time-varying predictors can go in level-1 and/or level-2. The level of the predictor dictates which variance component it seeks to describe: Level-2 describes level-2 variances and Level-1 describes level-1 variances. Although the order in which you add these predictors (in a series of successive models) may not ultimately matter, general practice is to add level-2 (time-invariant) predictors first.\nHow to decide where to add predictors? One strategy:\nBecause the time-specific subscript t can only appear in the level-1 model, all time-varying predictors must appear in the level-1 individual growth model. That is, person-specific predictors that vary over time appear at level-1, not level-2. Time-invariant predictors go in level-2. Furthermore, because they are time-invariant, this means they have no within-person variation to allow for a level-2 residual; thus, the level-2 growth rate parameter corresponding to this time-invariant predictor will not have an error term (i.e. it’s assumed to be zero). Interpretation wise, this assumes the effect of a person-specific effect is constant across population members. For a time-varying predictor, however, the associated level-2 growth parameter equation would have a residual term. This allows the effect of the time-varying predictor to vary randomly across the individuals in the population.\nWith only a few measurement points per person, we often lack sufficient data to estimate many variance components. Thus, it’s suggested that we resist the temptation to automatically allow the effects of time-varying predictors to vary at level-2 unless you have a good reason, and enough data, to do so.\nSo far in class, we’ve seen person-specific variables appear in level-2 submodels as predictors for level-1 growth parameters. You might therefore think that substantive predictors must always appear at level-2, but this isn’t true!\nHow inclusion of predictors affect variance components: Generally, when we include time-invariant predictors:\nWhen we include time-varying predictors:",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Predictors in Longitudinal Growth Models</span>"
    ]
  },
  {
    "objectID": "growth_models_predictors.html#tips-for-growth-models",
    "href": "growth_models_predictors.html#tips-for-growth-models",
    "title": "27  Predictors in Longitudinal Growth Models",
    "section": "",
    "text": "First fit an unconditional (i.e. no predictors) random intercept model. This isn’t really predictive, but we can use it as a baseline model that partitions variance into between and within-person variances. Singer & Willett (2003) call this the “unconditional means model”.\nCalculate the ICC\n\nIf most of the variance is between-persons in the random intercept (level-2), then you’ll use person-level predictors to reduce that variance (i.e., account for inter-person differences)\nIf most of the variance is within-person (level-1 residual variance), you’ll need time-level predictors to reduce that variance (i.e. account for intra-person differences)\n\n\n\n\n\n\n\nthe level-1 variance component, \\(\\sigma^2_e\\), remains pretty stable because time-invariant predictors can’t explain any within-person variation\nthe level-2 variance components, \\(\\tau_{00}\\) and \\(\\tau_{01}\\), will decrease if the time-invariant predictors explain some of the between-person variation in initial status or rates of change, respectively.\n\n\n\nboth level-1 and level-2 variance components might be affected because time-varying predictors vary both within a person and between people\nwe can interpret the resulting decrease in the level-1 variance component as amount of variation in the outcome explained by the time-varying predictors; however, it isn’t meaningful to interpret subsequent changes in level-2 variance components because adding the time-varying predictor changes the meaning of the individual growth parameters, which consequently alters the meaning of the level-2 variances, so it doesn’t make sense to compare the magnitude of these level-2 variances across successive models.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Predictors in Longitudinal Growth Models</span>"
    ]
  },
  {
    "objectID": "growth_models_predictors.html#additional-resources",
    "href": "growth_models_predictors.html#additional-resources",
    "title": "27  Predictors in Longitudinal Growth Models",
    "section": "27.2 Additional Resources",
    "text": "27.2 Additional Resources\n\nhttps://books.google.com/books?i d=PpnA1M8VwR8C&pg=PA168&lpg=PA168&dq=longitudinal+data+analysis+level1+level2+predictors&source=bl&ots=N4p8yFdyuL&sig=wWjmaEeqakD040s4B9-QquJF1eE&hl=en&sa=X&ved=0CCwQ6AEwAWoVChMI5ZLsjKDjyAIVzB0-Ch1s6wGV#v=onepage&q=longitudinal%20data%20analysis%20level1%20level2%20predictors&f=false\nhttp://jonathantemplin.com/files/mlm/mlm12uga/mlm12uga_section06.pdf\nhttp://www.lesahoffman.com/944/944_Lecture07_Time-Invariant.pdf",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Predictors in Longitudinal Growth Models</span>"
    ]
  },
  {
    "objectID": "interpreting_glms.html",
    "href": "interpreting_glms.html",
    "title": "28  Interpreting GLMs",
    "section": "",
    "text": "28.1 Dichotomous regression models (logistic regression)\nWhen predicting either successes and failures, or proportions, we can use a model with a binomial outcome. Here we’ll focus on models where the data is represented as individual successes and failures. The canonical model for these data is logistic regression, where\n\\[logit(E[Y|X]) \\equiv \\log\\left(\\frac{P(Y=1|X)}{1-P(Y=1|X)}\\right) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\\] \\[Y \\sim Binomial(1, E[Y|X])\\]\nWe can rewrite this model as\n\\[odds(Y) = \\frac{P(Y=1|X)}{1-P(Y=1|X)} = e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}\\]\nor\n\\[P(Y=1|X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}\\]\nWe can interpret \\(\\beta_0\\) as follows: for observations which are 0 on all of the predictors, we estimate that the mean value of the outcome will be \\(\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\\). That is, we estimate that the probability of the outcome being a ‘success’ (assuming ‘success’ is coded as a 1) will be \\(\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\\).\nWe can interpret \\(\\beta_1\\) as follows: adjusting for the other predictors, a one-unit difference in \\(X_1\\) predicts a \\(\\beta_1\\) difference in the log-odds of the outcome being one, or a \\((e^{\\beta_1}-1)\\times100\\%\\) difference in the odds of the outcome. Unfortunately, the change in probability of a unit change depends on where the starting point is, so there is no easy way to interpret these coefficients in terms of direct probability. One can calculate the estimated change for specific units, however, and look at the distribution of those changes.\nOther possible link functions include the probit (which uses a Normal CDF to link \\(\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\\) to \\(P(Y=1|X)\\)), or the complementary log-log (which allows \\(P(Y = 1|X)\\) to be asymmetric in the predictors), among others.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting GLMs</span>"
    ]
  },
  {
    "objectID": "interpreting_glms.html#dichotomous-regression-models-logistic-regression",
    "href": "interpreting_glms.html#dichotomous-regression-models-logistic-regression",
    "title": "28  Interpreting GLMs",
    "section": "",
    "text": "28.1.1 How to fit a GLM\nWe can fit a logistic regression model by writing\nglm(Y \\(\\sim\\) X, family = binomial(link = ‘logit’))\nWe can fit a probit regression model by writing\nglm(Y \\(\\sim\\) X, family = binomial(link = ‘probit’))\nWe can fit a complementary log-log model by writing\nglm(Y \\(\\sim\\) X, family = binomial(link = ‘cloglog’))\nWe can allow a random slope and intercept by writing\nglmer(Y \\(\\sim\\) 1 + X + (1 + X|grp), family = binomial(link = ‘logit’))",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting GLMs</span>"
    ]
  },
  {
    "objectID": "interpreting_glms.html#interpreting-multilevel-logistic-regressions",
    "href": "interpreting_glms.html#interpreting-multilevel-logistic-regressions",
    "title": "28  Interpreting GLMs",
    "section": "28.2 Interpreting multilevel logistic regressions",
    "text": "28.2 Interpreting multilevel logistic regressions\nIn this section, we give some further discussion about logistic regression and interpretation. This section supplements Packet 6.2 (logistic and longitudinal, or the toenail data lecture). But also see the code supplement for Packet 6.2 for more in-depth analysis and commentary.\nWe first fit our toenail data using a random intercept model:\n\nM1 = glmer( outcome ~ Tx * month + (1|patient),\n            family=binomial,\n            data=toes, \n            control=glmerControl(optimizer=\"bobyqa\", \n                                 optCtrl=list(maxfun=100000) ))\n\ndisplay( M1 )\n\nglmer(formula = outcome ~ Tx * month + (1 | patient), data = toes, \n    family = binomial, control = glmerControl(optimizer = \"bobyqa\", \n        optCtrl = list(maxfun = 1e+05)))\n                     coef.est coef.se\n(Intercept)          -2.51     0.76  \nTxItraconazole       -0.30     0.69  \nmonth                -0.40     0.05  \nTxItraconazole:month -0.14     0.07  \n\nError terms:\n Groups   Name        Std.Dev.\n patient  (Intercept) 4.56    \n Residual             1.00    \n---\nnumber of obs: 1908, groups: patient, 294\nAIC = 1265.6, DIC = -25.6\ndeviance = 615.0 \n\n\nNow let’s interpret. We have three different ways of looking at these model results, log-odds (or logits), odds, or probabilities themselves.\nlog-odds: The predicted values and coefficients are in the log-odds space for a logistic model. The coefficient of month means each month the log-odds goes down by 0.40. The baseline intercept of -2.51 mean that a control patient at month=0 has a log-odds of detachment of -2.51.\nUsing our model, if we wanted to know the chance of detachment for a median treated patient 3 months into the trial we could calculate:\n\nfes = fixef(M1)\nlog_odds = fes[[1]] + fes[[2]] + (fes[[3]] + fes[[4]])*3\nlog_odds\n\n[1] -4.43\n\n\nFor a patient who has a 1 SD above-average proclivity for detachment, we would add our standard deviation of 4.56:\n\nlog_odds + 4.56\n\n[1] 0.135\n\n\nodds: The odds of something happening are the chance of happening divided by the chance of not happening, or \\(odds = p/(1-p)\\). To convert log-odds to odds we just exponentiate:\n\nORs = exp( fixef( M1 ) )\nORs\n\n         (Intercept)       TxItraconazole                month \n              0.0813               0.7372               0.6705 \nTxItraconazole:month \n              0.8719 \n\n\nThe intercept is our base odds: the odds of detachment at month=0 for a control patient. The rest of the coefficients are odds multipliers, multiplying our baseline (starting) odds. For example, each month a control patient’s odds of detachment gets multiplied by 0.671.\nNote that exponentiation and logs play like this (for a control patient at 2 months, in this example) \\[ odds = exp( -2.51 + 2 * -0.40 ) = exp( -2.51 ) \\cdot exp( 2 * -0.40 ) = exp( -2.51 ) \\cdot exp( -0.40 )^2 \\] See how \\(exp( -0.40 )\\) is a multiplier on the baseline \\(exp( -2.51 )\\)?\nWe can look at the math to get a bit more here:\n\\[\n\\begin{aligned}\nlogit( Pr( Y_{ij} = 1 ) ) &= \\log odds( Pr( Y_{ij} = 1 ) ) = \\gamma_{00} + \\gamma_{01} Z_j + \\gamma_{10} Time_{ij} + \\gamma_{11} Z_j Time_{ij} + u_j\n\\end{aligned}\n\\] (logit means log odds)\nWe can rewrite this as \\[\n\\begin{aligned}\nodds( Y_{ij} = 1 ) &= \\exp\\left[ \\gamma_{00} + \\gamma_{01} Z_j + \\gamma_{10} Time_{ij} + \\gamma_{11} Z_j Time_{ij} + u_j \\right] \\\\\n&= e^{\\gamma_{00}} + e^{\\gamma_{01} Z_j} + e^{\\gamma_{10} Time_{ij}} + e^{\\gamma_{11} Z_j Time_{ij}} + e^{u_j} \\\\\n&= e^{\\gamma_{00}} \\cdot e^{\\gamma_{01} Z_j} \\cdot \\left(e^{\\gamma_{10}}\\right)^{Time_{ij}} \\cdot \\left(e^{\\gamma_{11}}\\right)^{Z_j Time_{ij}} \\cdot e^{u_j}\n\\end{aligned}\n\\] See how all our additive covariates turn into multiplicative factors? And time exponentiates our factors, so we keep multiplying by the factor for each extra month.\nFor our two 3 month, treated patients, we have the odds of detachment of\n\nexp( c( log_odds, log_odds + 4.56 ) )\n\n[1] 0.012 1.144\n\n\nMultipliers that are less than 1 correspond to reductions in the odds. A multiplier of 0.67 (the month coefficient) is a 33% reduction, for example. For the treatment group, the multipliers get multiplied, giving 0.67 * 0.87 = 0.58, or a 42% reduction. We can use these calculations to discuss the impact of treatment. For example, we might say, “We estimate that taking the treatment reduces the odds of detachment by 42% per month, vs. only 33% for the control.”\nProbabilities: Finally, we have probabilities, which we can calculate directly with invlogit in the arm package or plogis in the base package:\n\nplogis( c( log_odds, log_odds + 4.56 ) )\n\n[1] 0.0118 0.5336\n\n\nHere we have a 1% chance of detachment at baseline for our median patient, and 53% chance for our 1SD above average patient.\n\n28.2.1 Some math formula for reference\nThe relevant formula are:\n\\[\nodds = \\frac{ prob }{1 - prob}\n\\]\ngiving (letting \\(\\eta\\) denote our log odds) \\[\nprob = \\frac{ odds }{ 1 + odds } = \\frac{ \\exp( \\eta ) }{ 1 + \\exp(\\eta)} = \\frac{1}{1 + \\exp(-\\eta) }\n\\] The second equality is a simple algebraic trick to write the probability as a function where the log-odds (\\(\\eta\\)) appears only once.\n\n\n28.2.2 More on the random intercept\nThe random intercepts represent each patients overall proclivity to have a detachment. High values means that patient just has a higher odds of detachment, and low values means less.\nIf we exponentiate our Empirical Bayes estimated random intercepts, we get multiplicative factors of how each patient’s odds are just shifted by some amount. E.g.,\n\nREs = ranef( M1 )$patient$`(Intercept)`\nhead( REs )\n\n[1] 4.81 2.83 1.81 1.82 4.31 4.42\n\nsummary( REs )\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -1.28   -1.03   -0.97    1.35    3.90   10.26 \n\nquantile( exp(REs), c( 0.05, 0.95 ) )\n\n     5%     95% \n  0.291 469.145 \n\n\nThis means the odds of detachment for some patients (the 5% least likely to have detachment) is 30% of the baseline detachment. For a 95th percentile patient, we have an odds multiplier of around 470—they are much, much more likely to have detachment at any moment in time. This is why the curves for the patients in the main lecture are so different.\nTo recap: our model says the baseline median patient has a very low chance of detachment. For many patients it is even lower than that, but many other patients have very high random intercepts which makes their chance of detachment much, much higher.\n\n\n28.2.3 Growth should have random slopes?\nWe can try to fit a random slope model, allowing for each patient’s growth in their log-odds to be different. This is a longitudinal linear growth model, with binary outcome:\n\nM2 = glmer( outcome ~ Tx * month + (1+month|patient),\n            family=binomial,\n            data=toes )\n\n\ndisplay( M2 )\n\nglmer(formula = outcome ~ Tx * month + (1 + month | patient), \n    data = toes, family = binomial)\n                     coef.est coef.se\n(Intercept)          -9.38     0.86  \nTxItraconazole        0.02     1.03  \nmonth                -0.30     0.23  \nTxItraconazole:month -0.46     0.35  \n\nError terms:\n Groups   Name        Std.Dev. Corr  \n patient  (Intercept) 23.43          \n          month        3.84    -0.87 \n Residual              1.00          \n---\nnumber of obs: 1908, groups: patient, 294\nAIC = 996.6, DIC = -640\ndeviance = 171.4 \n\nanova( M1, M2 )\n\nData: toes\nModels:\nM1: outcome ~ Tx * month + (1 | patient)\nM2: outcome ~ Tx * month + (1 + month | patient)\n   npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    \nM1    5 1266 1293   -628     1256                        \nM2    7  997 1036   -491      983   273  2     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe random slope model is strongly preferred, and also has a larger estimated effect of treatment, although the standard errors have also grown considerably. What is likely happening is the autoregressive pattern in our outcomes (note how we tend to see 1s followed by 0s, with not a lot of back and forth) coupled with the limited information we have for each patient, makes it hard to nail down differences in individual student growth vs. differences in treatment and control average growth. The random intercept model focuses on within-person change, but is an easier to estimate model. Due to randomization, it is also trustworthy–we do not have to worry much about the assumptions.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting GLMs</span>"
    ]
  },
  {
    "objectID": "interpreting_glms.html#poisson-regression-models",
    "href": "interpreting_glms.html#poisson-regression-models",
    "title": "28  Interpreting GLMs",
    "section": "28.3 Poisson regression models",
    "text": "28.3 Poisson regression models\nPoisson regression is sometimes used to model count data. The canonical form of a Poisson (log-linear) regression model is \\[\\log(E[Y|X]) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\\] \\[Y \\sim Poisson(E[Y|X])\\]\nThe Poisson distribution has only one parameter, the mean, which is also the variance of the distribution. So in estimating \\(E[Y|X]\\), we are also estimating \\(Var(Y|X)\\). This is a potential drawback to the Poisson model, because there is no variance parameter to estimate, and so incorrect models can give wildly inaccurate standard errors (frequently unrealistically small). A better model is a quasi-Poisson model, for which the variance is proportional to the mean, but not necessarily equal to it. The negative binomial regression model is also commonly used to address over-dispersed count data where the variance exceeds the mean.\nThe canonical link function for Poisson outcomes is the natural logarithm. When we use a log-link, we can write\n\\[E[Y|X] = e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}.\\]\nWe can interpret \\(\\beta_0\\) as follows: for observations which are 0 on all of the predictors, we estimate that the mean (expected) value of the outcome will be \\(e^{\\beta_0}\\).\nWe can interpret \\(\\beta_1\\) as follows: adjusting for the other predictors, a one-unit difference in \\(X_1\\) predicts a \\((e^{\\beta_1}-1)\\times100 \\%\\) difference in the outcome.\nGenerally, when using a log-link, we assume that differences in the predictors are associated with multiplicative differences in the outcome.\nSome advantages to using an exponential link are\n\nthe model is mathematically more tractable and simpler to fit\nthe model parameters are easy to interpret\nthe mean of \\(Y\\) is guaranteed to be positive for all values of \\(X\\), which is required by the Poisson distribution\n\n\n28.3.1 How to fit a poisson regression\nWe can fit a Poisson log-linear regression by writing\nglm(Y \\(\\sim\\) X, family = poisson(link = ‘log’))\nTo fit a quasi-Poisson model, write\nglm(Y \\(\\sim\\) X, family = quasipoisson(link = ‘log’))\nTo fit a negative binomial regression model, write (after loading the MASS library)\nglm.nb(Y \\(\\sim\\) X, link=‘log’)\nTo fit a Poisson regression with an identity link (where coefficients are interpreted as expected differences in the outcome associated with unit differences in the predictor), write\nglm(Y \\(\\sim\\) X, family = poisson(link = ‘identity’))\nTo fit a Poisson regression with a square root link, which is vaguely like a compromise between an identity link and a log link (and is harder to interpret than either), write\nglm(Y \\(\\sim\\) X, family = poisson(link = ‘sqrt’))\nTo fit a Poisson log-linear model with a random intercept and slope, write\nglmer(Y \\(\\sim\\) X + (X|grp), family = poisson(link = ‘log’))",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting GLMs</span>"
    ]
  },
  {
    "objectID": "interpreting_glms.html#glms-vs.-transformations",
    "href": "interpreting_glms.html#glms-vs.-transformations",
    "title": "28  Interpreting GLMs",
    "section": "28.4 GLMs vs. Transformations",
    "text": "28.4 GLMs vs. Transformations\nThose of you coming from S40 and S52 may recall that when we have non-linear relationships between \\(X\\) and \\(Y\\), we can apply a transformation, such as taking the log, to linearize the relationship. In the words of Jimmy Kim, “with transformations, we use the machinery of linear regression to model non-linear relationships.” If that’s the case, then what is Poisson regression about, which deals with log counts? This is a topic that confused me for many years so hopefully I can clear it up here.\n\n28.4.1 Making and Graphing the Data\nLet’s start by making some fake data. Here’s the data-generating function, which has the relationship that a 1-unit increase in x will increase the expected count by \\(e^.5 = 1.65\\).\n\\[\ny = Poisson(e^{0.5x})\n\\]\n\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(ggeffects)\n\ntheme_set(theme_classic())\n\nrm(list = ls())\n\ndat &lt;- tibble(\n  x = runif(1000, 0, 5),\n  y = rpois(1000, exp(0.5*x))\n)\n\nIn the graph, we can see that the relationship between x and y is clearly non linear!\n\nggplot(dat, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\nLet’s plot log_y + 1 on x. Amazing! The relationship is basically linear, which suggests that a 1-unit increase in x has some multiplicative effect on y.\n\nggplot(dat, aes(x = x, y = log(y + 1))) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\n\n28.4.2 Fitting the Regression Models\nLet’s use both OLS and Poisson regression to fit the data. We see a few things:\n\nThe Poisson model fits drastically better, both in terms of \\(R^2\\) and that the coefficients are close to the data-generating values\nThe transformed OLS model understates the slope\nBoth models have (seemingly) similar interpretations: a 1-unit increase in x causes an \\(e^\\beta\\) increase in y. How is this possible?\n\nSo what’s going on?\nThe answer is that there is a very subtle difference between a transformed OLS regression and a Poisson regression. In transformed OLS, we are modeling the mean of the log of Y, or \\(E(ln(y|x))\\). In Poisson, we’re modeling the log of the mean of Y, or \\(ln(E(y|x))\\). These are not equivalent! In essence, Poisson regression is a model for the arithmetic mean, whereas OLS is a model for the geometric mean. This means that when we exponentiate the Poisson model, we can get predicted counts, but this is not true of the OLS model.\n\nm1 &lt;- lm(log(y + 1) ~ x, dat)\nm2 &lt;- glm(y ~ x, dat, family = poisson)\n\ntab_model(m1, m2,\n          p.style = \"stars\",\n          show.ci = FALSE,\n          show.se = TRUE,\n          digits = 3,\n          transform = NULL,\n          dv.labels = c(\"Log(Y+1)\", \"Poisson\"))\n\n\n\n\n \nLog(Y+1)\nPoisson\n\n\nPredictors\nEstimates\nstd. Error\nLog-Mean\nstd. Error\n\n\n(Intercept)\n0.375 ***\n0.029\n-0.074 \n0.045\n\n\nx\n0.420 ***\n0.010\n0.516 ***\n0.012\n\n\nObservations\n1000\n1000\n\n\nR2 / R2 adjusted\n0.631 / 0.631\n0.906\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n\n\n28.4.3 More Intuition: An Example with Means\nLet’s create a super simple data set, s.\n\ns &lt;- c(1, 10, 100)\n\nIt’s clearly skewed. But I can still take the mean. I could take the arithmetic mean, or the geometric mean. These are clearly different quantities.\n\nmean(s) # arithmetic\n\n[1] 37\n\nexp(mean(log((s)))) # geometric\n\n[1] 10\n\n\nThe idea of Poisson is to take the log of the mean and fit a linear model for that:\n\nlog_mean &lt;- log(mean(s))\nlog_mean\n\n[1] 3.61\n\n\nThe idea of transformed OLS is to take the mean of the log and fit a linear model for that:\n\nmean_log &lt;- mean(log(s))\nmean_log\n\n[1] 2.3\n\n\nWhen I exponentiate the log of the mean, I get back the original arithmetic mean. This is what Poisson is doing:\n\nexp(log_mean)\n\n[1] 37\n\n\nWhen I exponentiate the mean of the log, I get back the original geometric mean. This is what transformed OLS is doing:\n\nexp(mean_log)\n\n[1] 10\n\n\n\n\n28.4.4 Further Reading\nhttps://www.theanalysisfactor.com/the-difference-between-link-functions-and-data-transformations/",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting GLMs</span>"
    ]
  },
  {
    "objectID": "lr_test.html",
    "href": "lr_test.html",
    "title": "29  Likelihood Ratio Tests",
    "section": "",
    "text": "29.1 Why LR Tests?\nOur fixed effects coefficients have SEs, z-statistics, and p-values, which allow us to easily test the null hypothesis that the slopes are 0 in the population. No such quantities, however, are provided for the random effects of our model. We can use LR tests to address this issue and test the statistical significance of the various random portions of our model.\nWe can also use LR tests on fixed effects or sets of fixed effects (like a nested F-test in OLS), but 99.9% of the time, the conclusion will be the same as using the z-statistics.\nLR tests require that the models are nested, meaning that they use the same data, and one model can be expressed as a constrained version of the other.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Likelihood Ratio Tests</span>"
    ]
  },
  {
    "objectID": "lr_test.html#hsb-example",
    "href": "lr_test.html#hsb-example",
    "title": "29  Likelihood Ratio Tests",
    "section": "29.2 HSB Example",
    "text": "29.2 HSB Example\nWe fit 3 models:\n\nRandom intercept model\nRandom slope model with no correlation between intercepts and slopes (you probably have not seen this before, but we will use it to test whether the slopes are correlated with the intercepts).\nRandom slope model\n\nWe can see from the model output that the point estimates for the random slope variance \\(\\tau_{11}\\) and the correlation \\(\\rho_{01}\\) are non-zero, but how can we get p-values for these quantities?\n\nm1 &lt;- lmer(mathach ~ ses + (1|schoolid), hsb)\nm2 &lt;- lmer(mathach ~ ses + (1|schoolid)+(0+ses|schoolid), hsb)\nm3 &lt;- lmer(mathach ~ ses + (1+ses|schoolid), hsb)\n\ntab_model(m1, m2, m3,\n          p.style = \"stars\",\n          show.se = TRUE,\n          show.ci = FALSE,\n          dv.labels = c(\"RI\", \"No Rho\", \"RS\"))\n\n\n\n \nRI\nNo Rho\nRS\n\n\nPredictors\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\n\n\n(Intercept)\n12.66 ***\n0.19\n12.65 ***\n0.19\n12.67 ***\n0.19\n\n\nses\n2.39 ***\n0.11\n2.40 ***\n0.12\n2.39 ***\n0.12\n\n\nRandom Effects\n\n\n\nσ2\n37.03\n36.82\n36.83\n\n\n\nτ00\n4.77 schoolid\n4.85 schoolid\n4.83 schoolid\n\n\nτ11\n \n0.42 schoolid.ses\n0.41 schoolid.ses\n\n\nρ01\n \n \n-0.11 schoolid\n\n\nICC\n0.11\n0.12\n0.12\n\n\nN\n160 schoolid\n160 schoolid\n160 schoolid\n\nObservations\n7185\n7185\n7185\n\n\nMarginal R2 / Conditional R2\n0.077 / 0.182\n0.077 / 0.185\n0.077 / 0.189\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n29.2.1 Are random ses slopes necessary?\nWe use anova to perform the LR test comparing m1 and m3 to see if we need random slopes. We see that the random slopes are not statistically significant.\n\nanova(m1, m3)\n\nData: hsb\nModels:\nm1: mathach ~ ses + (1 | schoolid)\nm3: mathach ~ ses + (1 + ses | schoolid)\n   npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nm1    4 46649 46677 -23320    46641                     \nm3    6 46648 46690 -23318    46636 4.5354  2     0.1035\n\n\n\n\n29.2.2 Is there a correlation between the random intercept and slope for ses?\nWe next can compare model 2 and model 3 to see if the correlation is needed, given the random slope model. We see it is not:\n\nanova(m2, m3)\n\nData: hsb\nModels:\nm2: mathach ~ ses + (1 | schoolid) + (0 + ses | schoolid)\nm3: mathach ~ ses + (1 + ses | schoolid)\n   npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nm2    5 46647 46681 -23318    46637                     \nm3    6 46648 46690 -23318    46636 0.2762  1     0.5992",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Likelihood Ratio Tests</span>"
    ]
  },
  {
    "objectID": "lr_test.html#technical-notes",
    "href": "lr_test.html#technical-notes",
    "title": "29  Likelihood Ratio Tests",
    "section": "29.3 Technical Notes",
    "text": "29.3 Technical Notes\nTL/DR: The traditional LR test provided by anova is likely to be conservative for testing the significance of variance components. For the purposes of this course, it is fine.\nThere is a lot of multilevel literature arguing that testing a null hypothesis on variance components with LR tests is not the best approach. The reason is that variances cannot be negative, so the null hypothesis exists on the “boundary of the parameter space” and therefore is “likely to be conservative” (to use the warning that Stata gives you, i.e., the p-values are too high). The true distribution of a 0 variance component is not a normal distribution, but a mixture distribution with half of the probability mass at 0 and the other half \\(\\chi^2\\). When you’re testing the significance of the random intercepts model, you can divide the p-value by 2 to get the right answer (Stata default), though for more complex models it’s not so simple. There are some R packages that use simulation-based approaches to provide more robust results such as pbkrtest::PBmodcomp, but we won’t go into them here. See RH&S pp. 88-89 for a more thorough discussion of this issue. Despite this, the standard LR test remains common in practice.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Likelihood Ratio Tests</span>"
    ]
  },
  {
    "objectID": "aic_check.html",
    "href": "aic_check.html",
    "title": "30  AIC, BIC, and Deviance",
    "section": "",
    "text": "In this section, we briefly walk through how to find AIC, BIC, and Deviance to compare models. We have a simple multilevel dataset (we generate through a utility package, blkvar, that Miratrix and the C.A.R.E.S. lab has used to explore how multilevel modeling works in practice), and generate a few variables that we will use as predictors. Only the fourth variable is actually useful for prediction! Let’s see if our AIC, etc., measures identify which model is superior.\nTo install a “working package” we use devtools:\n\ndevtools::install_github(\"https://github.com/lmiratrix/blkvar\" )\n\n\nlibrary( blkvar )\ndd = generate_multilevel_data( J = 40 )\nhead( dd )\n\n    sid         Y0         Y1 Z       Yobs          W\n1     1 -0.8491674  0.1793707 0 -0.8491674 -0.3047711\n1.1   1 -0.7800720  0.2484662 1  0.2484662 -0.3047711\n1.2   1 -2.2845610 -1.2560229 1 -1.2560229 -0.3047711\n1.3   1 -1.6637536 -0.6352155 0 -1.6637536 -0.3047711\n2     2  1.0160661  2.1959872 0  1.0160661  1.4354998\n2.1   2  1.2583262  2.4382473 1  2.4382473  1.4354998\n\ndd$X1 = rnorm( nrow(dd) )\ndd$X2 = rnorm( nrow(dd) )\ndd$X3 = rnorm( nrow(dd) )\ndd$X4 = dd$Yobs + rnorm(nrow(dd))\n \nM1 = lmer( Yobs ~ 1 + (1|sid), data=dd )\nM2 = lmer( Yobs ~ 1 + X1 + X2 + X3 + (1|sid), data=dd )\nM3 = lmer( Yobs ~ 1 + X1 + X2 + X3 + X4 + (1|sid), data=dd )\n\nlibrary( arm )\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is /Users/lmiratrix/Dropbox/MLM Course F2024/MLM textbook\n\ndisplay(M1)\n\nlmer(formula = Yobs ~ 1 + (1 | sid), data = dd)\ncoef.est  coef.se \n    0.05     0.14 \n\nError terms:\n Groups   Name        Std.Dev.\n sid      (Intercept) 0.84    \n Residual             0.61    \n---\nnumber of obs: 423, groups: sid, 40\nAIC = 912.7, DIC = 902.5\ndeviance = 904.6 \n\ndisplay(M2)\n\nlmer(formula = Yobs ~ 1 + X1 + X2 + X3 + (1 | sid), data = dd)\n            coef.est coef.se\n(Intercept)  0.05     0.14  \nX1           0.03     0.03  \nX2          -0.01     0.03  \nX3           0.00     0.03  \n\nError terms:\n Groups   Name        Std.Dev.\n sid      (Intercept) 0.84    \n Residual             0.62    \n---\nnumber of obs: 423, groups: sid, 40\nAIC = 933.1, DIC = 886.3\ndeviance = 903.7 \n\nlibrary( texreg )\n\nVersion:  1.39.3\nDate:     2023-11-09\nAuthor:   Philip Leifeld (University of Essex)\n\nConsider submitting praise using the praise or praise_interactive functions.\nPlease cite the JSS article in your publications -- see citation(\"texreg\").\n\nscreenreg( list( M1, M2, M3 ) )\n\n\n=====================================================\n                      Model 1   Model 2   Model 3    \n-----------------------------------------------------\n(Intercept)              0.05      0.05      0.03    \n                        (0.14)    (0.14)    (0.09)   \nX1                                 0.03      0.03    \n                                  (0.03)    (0.03)   \nX2                                -0.01     -0.01    \n                                  (0.03)    (0.03)   \nX3                                -0.00      0.00    \n                                  (0.03)    (0.03)   \nX4                                           0.29 ***\n                                            (0.02)   \n-----------------------------------------------------\nAIC                    912.74    933.13    788.79    \nBIC                    924.89    957.42    817.12    \nLog Likelihood        -453.37   -460.57   -387.39    \nNum. obs.              423       423       423       \nNum. groups: sid        40        40        40       \nVar: sid (Intercept)     0.71      0.70      0.30    \nVar: Residual            0.38      0.38      0.28    \n=====================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>AIC, BIC, and Deviance</span>"
    ]
  },
  {
    "objectID": "lmer_optimization.html",
    "href": "lmer_optimization.html",
    "title": "31  Optimization Algorithms for MLMs",
    "section": "",
    "text": "31.1 What to do when your model won’t converge\nIf your error won’t converge, you might get a warning message like this:\nThis warning message tells us two things. First, remember that we are trying to find the maximum of the likelihood function, or the place where the slope = 0, where the “slope” is how much the likelihood function is changing. In the warning, the tol = 0.001 tells us that R will be happy if it finds estimates where the slope \\(\\leq\\) 0.001. It’s also saying that our slope when R stopped converging was 0.0463355.\nWhen convergence fails, there are a few steps you can take to try and get it to do better:\nFor #1, calculate the standard deviation of all your variables and compare them to one another. If you have one variable with a very large sd relative to the others, try rescaling it. For example, if you had income as a covariate, you might divide it by 1000 to get “income in thousands of dollars,” which might make the scale more similar to other variables.\nFor #2, you add a Control option into your lme, lmer, or glmer function. Each of those functions has its own option, but they all take the same arguments:\nBelow are some other optimizer options that you can try. For simplicity, we’re specifying them all as “glmer” options, but you could easily adjust them to match whichever model you are trying (but failing) to fit:\nThe following is a good default that often works:\nlog_mod &lt;- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 control = glmerControl(optimizer = 'bobyqa'))\nYou can also try optimizer = 'Nelder_Mead'. Sometimes the syntax is a bit different:\n## Use a BFGS optimizer \nlog_mod &lt;- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 control = glmerControl(optimizer=\"optim\", optimMethod = \"BFGS\"))\nIf these aren’t working, you can downlaod a special package to use the optimx optimizer:\n#install.packages(‘optimx’)\nlibrary(optimx)\nlog_mod &lt;- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 glmerControl(optimizer = 'optimx', calc.derivs = FALSE,\n                              optCtrl = list(method = \"L-BFGS-B\", \n                                             starttests = FALSE, \n                                             kkt = FALSE)))\nThere are many other ways to adjust your optimization commands, which can be found here: https://rdrr.io/cran/lme4/man/lmerControl.html",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Optimization Algorithms for MLMs</span>"
    ]
  },
  {
    "objectID": "lmer_optimization.html#what-to-do-when-your-model-wont-converge",
    "href": "lmer_optimization.html#what-to-do-when-your-model-wont-converge",
    "title": "31  Optimization Algorithms for MLMs",
    "section": "",
    "text": "Warning message: In checkConv(attr(opt, \"derivs\"), opt$par, \nctrl = control$checkConv, : Model failed to converge with \nmax\\|grad\\| = 0.0463355 (tol = 0.001, component 1)\n\n\n\nTry rescaling variables and refitting your model.\nTry changing your optimizer settings.\n\n\n\n\nlme: lmeControl()\nlmer: lmerControl()\nglmer: glmerControl()",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Optimization Algorithms for MLMs</span>"
    ]
  },
  {
    "objectID": "lmer_optimization.html#technical-appendix-understanding-the-types-of-optimization-algorithms",
    "href": "lmer_optimization.html#technical-appendix-understanding-the-types-of-optimization-algorithms",
    "title": "31  Optimization Algorithms for MLMs",
    "section": "31.2 Technical Appendix: Understanding the Types of Optimization Algorithms",
    "text": "31.2 Technical Appendix: Understanding the Types of Optimization Algorithms\nThere are generally four “types” of algorithms employed to find MLE/REML solutions:\n\nNewton methods\nQuasi-Newton methods\nEM algorithm\nOther\n\n\n31.2.1 Newton Methods\nNewton’s method is the most “pure” of these approaches; essentially Newton’s method uses a Taylor series approximation to approximate a quadratic function and find its maxima. It involves finding the Hessian (a matrix containing all the second and partial derivatives from your likelihood). An advantage of this approach is that it is theoretically the best of the three named approaches because it will often require fewer iterations to converge. However, there are two drawbacks:\n\nWhen there are a large number of parameters, it is time-consuming to analytically calculate or numerically approximate all second order and mixed derivatives needed for the Hessian matrix.\nIn regions where the log-likelihood function is not sufficiently concave down, there is a tendency to dramatically overshoot because the step size to the next point is proportional to the inverse of the second derivative, resulting in pathological oscillations that would amplify if allowed to continue. Thus, where the log-likelihood function is not well approximated by a second order Taylor expansion, the method tends to fail miserably. This would be the case, for example, if the log-likelihood function was a standard normal density and you started out 2 SD from the mean. \\end{enumerate}\n\n\n\n31.2.2 Quasi-Newton Methods\nQuasi-Newton methods start with a “guess” for the Hessian, apply the quadratic formula to attain a new point, update the guess of the Hessian, and repeat until convergence is attained. Importantly, the approximated Hessian will converge to the Hessian so long as the Wolfe conditions (a set of conditions on the likelihood) are satisfied. The easiest guess for the initial Hessian is the identity matrix, making the first step simply a gradient descent. When the identity matrix is used as an initial guess, the quasi-Newton methods converge “super-linearly”–that is it displays linear convergece initially, but approach quadratic convergence as the approximated Hessian updates itself. There are many quasi-Newton methods, but the most common is the “BFGS” updating method.\nIn terms of time to convergence, quasi-Newton is typically much faster than pure Newton methods. This addresses the first drawback listed for Newton’s method, but it is still susceptible to the second issue. The other potential challenge with Quasi-Newton methods occurs when the Wolfe conditions are not satisfied - the method will typically not converge to the Hessian within a reasonable number of iterations, and can often exceed the maximum iterations set by a program.\n\n\n31.2.3 EM (Expectation-Maximiation) Algorithm\nThe EM algorithm is another way of approximating the likelihood function and maximizing that approximation. It does this in a repeating series of stseps: the E (Expectation) step and the M (Maximization) step. In random effect models, where normality is assumed, the E-step results in an a quadratic function to be maximized in the M-step. Importantly, each iteration of the EM algorithm is guaranteed to increase the likelihood function, a feature that that may be too difficult to attain with the Newton methods when a quadratic function is not yet a good approximation. Thus, even if the likelihood function not well approximated by a quadratic function, we are assured to be getting closer to a maximum with the EM algorithm. Thus the EM algorithm fixes the second issue from Newton’s method. However, it only displays linear convergence (as opposed to “super linear” or “quadratic”) and can therefore take a very long time to converge.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Optimization Algorithms for MLMs</span>"
    ]
  },
  {
    "objectID": "lmer_optimization.html#optimizer-implementation-in-different-programs",
    "href": "lmer_optimization.html#optimizer-implementation-in-different-programs",
    "title": "31  Optimization Algorithms for MLMs",
    "section": "31.3 Optimizer Implementation in Different Programs",
    "text": "31.3 Optimizer Implementation in Different Programs\n\n31.3.1 Stata/MPlus/HLM\nStata, Mplus, and HLM, each use a combination of the EM and the quasi-Newton methods when estimating models with random effects. The algorithms start with the EM algorithm and proceed until there is sufficient concavity to switch a quasi-Newton method. Using a combination of the EM and quasi-Newton methods minimizes computational time while maximizing the opportunity that the algorithm will converge to a maximum. Mplus and HLM will even switch back to the EM algorithm if the Wolfe conditions are not attained in a set amount of time; thus, my experience has been that Mplus and HLM tend to converge the fastest and tend to minimize convergence issues.\nDisclaimer: sometimes you may need to manually increase the number of EM iterations allowed to acheive convergence.\n\n\n31.3.2 R\nIf I am interpreting the lmerControls documentation correctly, this method starts with the EM algorithm and then applies “unconstrained and box-constrained optimization using PORT routines” from the nlminb function. I’ll classify this algorithm as “other”, as opposed to the three named approaches above.\nIn my opinion, lme’s optimization algorithm is less than ideal for two reasons. First, the number of initial EM steps is fixed and who’s to say that the default number of EM iterations will bring us to a region where the log-likelihood function is sufficiently concave?\nSecond, HLM and Mplus have been estimating random effect models for a long time, and developers from both have come to the conclusion that the quasi-Newton method as the second method in a combination is the best for these models. I’ll assume this is a very informed decision on the end of these developers. Yet, it does not appear that this is what is occuring in R. Instead, R uses “unconstrained and box-constrained optimization using PORT routines,” whatever that is.\nEven the according to the “See Also” section in the nlminb help file, the optim function is listed as preferred over the nlminb function. As it turns out, the optim function applies the “BFGS” quasi-Newton method as the default, which is consistent with Stata’s approach.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Optimization Algorithms for MLMs</span>"
    ]
  },
  {
    "objectID": "lmer_optimization.html#footnotes",
    "href": "lmer_optimization.html#footnotes",
    "title": "31  Optimization Algorithms for MLMs",
    "section": "",
    "text": "“Closed form” means that there is a formula you can use to simply and directly calculate your estimates. For example, in OLS your matrix equation for \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\)↩︎",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Optimization Algorithms for MLMs</span>"
    ]
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "32  Bootstrapping clustered data",
    "section": "",
    "text": "32.1 Bootstrapping\nTo bootstrap we need to sample our clusters with replacement, making a new dataset like the old one, but with a random set of clusters. We want the same number of clusters, so we will end up with some clusters multiple times, and some not at all.\nTo see bootstrapping in action, we first look at a toy example of 5 tiny clusters:\nset.seed( 40404 )\ntoy = tibble( id = rep(c(\"A\",\"B\",\"C\",\"D\",\"E\"), c(1,2,3,1,1)),\n              y = 1:8 )\ntoy\n\n# A tibble: 8 × 2\n  id        y\n  &lt;chr&gt; &lt;int&gt;\n1 A         1\n2 B         2\n3 B         3\n4 C         4\n5 C         5\n6 C         6\n7 D         7\n8 E         8\nLet’s take a single bootstrap sample of it:\ntt &lt;- toy %&gt;%\n  group_by( id ) %&gt;%\n  nest() %&gt;%\n  ungroup()\nt_star = sample_n( tt, 5, replace=TRUE )\nt_star$new_id = 1:nrow(t_star)\nnew_dat &lt;- unnest(t_star, cols=data)\nnew_dat\n\n# A tibble: 8 × 3\n  id        y new_id\n  &lt;chr&gt; &lt;int&gt;  &lt;int&gt;\n1 B         2      1\n2 B         3      1\n3 E         8      2\n4 D         7      3\n5 B         2      4\n6 B         3      4\n7 B         2      5\n8 B         3      5\nThis code is technical (and annoying) but it does a single cluster bootstrap. We first collapse our data so each row is a cluster. We then sample clusters with replacement, and then give each sampled cluster a new ID. We finally unpack our data to get the same number of clusters (but the clusters themselves are randomly sampled). Note how we are re-using “B” three times, but give unique ids to each of our three draws.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Bootstrapping clustered data</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#bootstrapping-hsb",
    "href": "bootstrap.html#bootstrapping-hsb",
    "title": "32  Bootstrapping clustered data",
    "section": "32.2 Bootstrapping HS&B",
    "text": "32.2 Bootstrapping HS&B\nWe can do the same thing with our data. We make a function to do it, since we will be wanting to do the entire process over and over. Here goes!\n\nboot_once &lt;- function( dat ) {\n  tt &lt;- dat %&gt;%\n    group_by( id ) %&gt;%\n    nest() %&gt;%\n    ungroup()\n  t_star = sample_n( tt, nrow(tt), replace=TRUE )\n  t_star$id = 1:nrow(t_star)\n  t_star &lt;- unnest(t_star, cols=data)\n  \n  M = lmer( mathach ~ 1 + ses*sector + (1+ses|id),\n            data = t_star )\n  \n  tidy( M )\n}\n\nLet’s try it out!\n\nboot_once( dat )\n\n# A tibble: 8 × 6\n  effect   group    term                 estimate std.error statistic\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;     (Intercept)            11.7       0.246     47.4 \n2 fixed    &lt;NA&gt;     ses                     2.74      0.141     19.5 \n3 fixed    &lt;NA&gt;     sector                  2.17      0.383      5.67\n4 fixed    &lt;NA&gt;     ses:sector             -1.15      0.222     -5.20\n5 ran_pars id       sd__(Intercept)         2.19     NA         NA   \n6 ran_pars id       cor__(Intercept).ses    0.955    NA         NA   \n7 ran_pars id       sd__ses                 0.402    NA         NA   \n8 ran_pars Residual sd__Observation         6.02     NA         NA   \n\n\nNote how our estimates are similar to our original data ones. But not quite–we are analyzing data that is like our original data. Seeing how much everything varies is the point of the bootstrap. Here we go (the map_dfr() command is a way of rerunning our boot_once code 100 times):\n\nset.seed( 40404 )\nboots = map_dfr( 1:100, \\(.) boot_once( dat ) )\n\nIf you run this, you will get a whole bunch of convergence warnings and whatnot. Each bootstrap sample has a different difficult time. But we want to see how estimates vary across all of that, so we don’t care!\nOnce done, we can see how all our estimates varied. Let’s make a histogram of all our estimates for all our parameters:\n\nggplot( boots, aes( estimate ) ) +\n  facet_wrap( ~ term, scales=\"free\" ) +\n  geom_histogram( bins=20 )\n\n\n\n\n\n\n\n\nNote how our correlation is usually 1, but sometimes can be -1. To get a confidence interval, we can use the quantile function and see the middle 95% range of our estimates:\n\nboots %&gt;%\n  group_by( term ) %&gt;%\n  summarize( q025 = quantile(estimate, 0.025),\n             q975 = quantile(estimate, 0.975) )\n\n# A tibble: 8 × 3\n  term                    q025   q975\n  &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)          11.3    12.2  \n2 cor__(Intercept).ses  0.242   1    \n3 sd__(Intercept)       1.65    2.26 \n4 sd__Observation       5.93    6.19 \n5 sd__ses               0.0249  0.667\n6 sector                1.47    2.71 \n7 ses                   2.72    3.19 \n8 ses:sector           -1.68   -0.988\n\n\nOur correlation is likely positive, but could be as low as 0.24. Our confidence on our random slope variation is quite wide, 0.02 to 0.67 or so.\nOur standard errors are the standard deviations of our estimates:\n\nSEs &lt;- boots %&gt;%\n  group_by( term ) %&gt;%\n  summarize( SE_boot = sd(estimate) )\n\nests &lt;- left_join( ests, SEs, by=\"term\" ) %&gt;%\n  mutate( ratio = SE_boot / std.error )\nests\n\n# A tibble: 8 × 8\n  effect   group    term             estimate std.error statistic SE_boot  ratio\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 fixed    &lt;NA&gt;     (Intercept)        11.8       0.232     50.7   0.224   0.967\n2 fixed    &lt;NA&gt;     ses                 2.96      0.143     20.7   0.145   1.01 \n3 fixed    &lt;NA&gt;     sector              2.13      0.346      6.16  0.326   0.942\n4 fixed    &lt;NA&gt;     ses:sector         -1.31      0.216     -6.09  0.201   0.932\n5 ran_pars id       sd__(Intercept)     1.95     NA         NA     0.166  NA    \n6 ran_pars id       cor__(Intercept…    1.00     NA         NA     0.320  NA    \n7 ran_pars id       sd__ses             0.275    NA         NA     0.166  NA    \n8 ran_pars Residual sd__Observation     6.07     NA         NA     0.0663 NA    \n\n\nIn this case, our bootstrap SEs are about the same as the ones we originally got from our model, for our fixed effects. We also have SEs for the variance parameters!",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Bootstrapping clustered data</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#the-lmeresampler-package-to-help",
    "href": "bootstrap.html#the-lmeresampler-package-to-help",
    "title": "32  Bootstrapping clustered data",
    "section": "32.3 The lmeresampler package to help",
    "text": "32.3 The lmeresampler package to help\nWe can also use the lmeresampler package to do the above. You write a function to calculate the statistics (estimates) that you care about, and then you bootstrap to get their uncertainty:\n\nlibrary( lmeresampler )\n\nsum_func &lt;- function( x ) {\n  t &lt;- tidy( x )\n  tt &lt;- t$estimate\n  names(tt) &lt;- t$term\n  tt\n}\nsum_func( M )\n\n         (Intercept)                  ses               sector \n              11.752                2.958                2.130 \n          ses:sector      sd__(Intercept) cor__(Intercept).ses \n              -1.313                1.955                1.000 \n             sd__ses      sd__Observation \n               0.275                6.065 \n\nbres &lt;- lmeresampler::bootstrap( M, type = \"case\", \n                                 .f = sum_func,\n                                 resample = c( TRUE, FALSE ),\n                                 B = 100 )\n\nbres\n\nBootstrap type: case \n\nNumber of resamples: 100 \n\n                  term observed rep.mean     se    bias\n1          (Intercept)   11.752   11.710 0.1818 -0.0421\n2                  ses    2.958    2.921 0.1184 -0.0368\n3               sector    2.130    2.169 0.2730  0.0394\n4           ses:sector   -1.313   -1.340 0.2006 -0.0263\n5      sd__(Intercept)    1.955    2.090 0.1245  0.1349\n6 cor__(Intercept).ses    1.000    0.395 0.1371 -0.6049\n7              sd__ses    0.275    0.842 0.1295  0.5662\n8      sd__Observation    6.065    6.020 0.0538 -0.0449\n\nThere were 0 messages, 0 warnings, and 0 errors.\n\n\nWe can get confidence intervals as well:\n\nlmeresampler:::confint.lmeresamp( bres )\n\n# A tibble: 24 × 6\n   term                 estimate  lower   upper type  level\n   &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 (Intercept)            11.8   11.4   12.2    norm   0.95\n 2 ses                     2.96   2.76   3.23   norm   0.95\n 3 sector                  2.13   1.55   2.63   norm   0.95\n 4 ses:sector             -1.31  -1.68  -0.894  norm   0.95\n 5 sd__(Intercept)         1.95   1.58   2.06   norm   0.95\n 6 cor__(Intercept).ses    1.00   1.34   1.87   norm   0.95\n 7 sd__ses                 0.275 -0.545 -0.0369 norm   0.95\n 8 sd__Observation         6.07   6.00   6.22   norm   0.95\n 9 (Intercept)            11.8   11.4   12.2    basic  0.95\n10 ses                     2.96   2.76   3.21   basic  0.95\n# ℹ 14 more rows\n\n\nNice!",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Bootstrapping clustered data</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#side-note-parametric-bootstrapping",
    "href": "bootstrap.html#side-note-parametric-bootstrapping",
    "title": "32  Bootstrapping clustered data",
    "section": "32.4 Side note: Parametric bootstrapping",
    "text": "32.4 Side note: Parametric bootstrapping\nSome will instead use a parameteric bootstrap, where you generate data from your estimated model and then re-estimate to see how your estimates change. You can do this with lmeresampler, or you can use the merTools package (which also offers a bunch of other utilities and may be worth checking out):\n\nlibrary(lme4)\nlibrary(merTools)\n\n# Example data\ndata(sleepstudy)\n\n# Fit a multilevel model\nmodel &lt;- lmer(Reaction ~ Days + (1 | Subject), data = sleepstudy)\n\n# Perform parametric bootstrapping\nboot_results &lt;- bootMer(\n  model,\n  FUN = fixef,  # Extract fixed effects\n  nsim = 1000,  # Number of bootstrap samples\n  use.u = TRUE,  # Include random effects uncertainty\n  type = \"parametric\"\n)\n\n# View bootstrap results\nsummary(boot_results$t)  # Summary of bootstrap fixed effects\n\n  (Intercept)       Days      \n Min.   :236   Min.   : 7.37  \n 1st Qu.:248   1st Qu.: 9.89  \n Median :252   Median :10.45  \n Mean   :251   Mean   :10.47  \n 3rd Qu.:254   3rd Qu.:11.01  \n Max.   :265   Max.   :13.11",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Bootstrapping clustered data</span>"
    ]
  },
  {
    "objectID": "survey_weights.html",
    "href": "survey_weights.html",
    "title": "33  Survey Weights",
    "section": "",
    "text": "33.1 Multilevel modeling and survey weights\nIn many circumstances you may be faced with a multilevel modeling project where you also have survey weights. Unfortunately R does not have good support for this hybrid of two worlds (although if you go the econometric direction you can incorporate weights into your robust standard errors).\nYou basically have two options at this point: you can ignore the weights (defendable, but often upsetting to reviewers and colleagues), or switch to Stata, which allows for both.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Survey Weights</span>"
    ]
  },
  {
    "objectID": "survey_weights.html#topline-advice",
    "href": "survey_weights.html#topline-advice",
    "title": "33  Survey Weights",
    "section": "33.2 Topline advice",
    "text": "33.2 Topline advice\nIgnore the weights for your final project and worry about extending to your general population later on, depending on where your research takes you. More important than the weights is making sure you have random effects corresponding to all clustering involved in the way the data were collected. For example, if the program was a sample of states and then a sample of villages, and then households, you would have a 4-level model: states, villages, households, and individuals. You would want a random effect for each level. If you had few states, you could back off and have fixed effects for state and generalize only to the sampled states rather than the full country.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Survey Weights</span>"
    ]
  },
  {
    "objectID": "survey_weights.html#what-are-survey-weights",
    "href": "survey_weights.html#what-are-survey-weights",
    "title": "33  Survey Weights",
    "section": "33.3 What are survey weights?",
    "text": "33.3 What are survey weights?\nThe easy way to think of sample weights (survey weights) is the answer to “how many people does this individual represent in the population?” (Although note that weights will generally be proportional to the answer to this question rather than literally that value.)\nFor example, if you had three people, the first with a weight of 1, the second with a weight of 0.5 and the third with a weight of 3, then we would think of our population as being \\(1 + 0.5 + 3 = 4.5\\) people, 3 of whom are people similar to our third sampled person, 0.5 of which our second sampled person, and 1 of whom is similar to our first person.\nIn other words, our first person is sampled proportional to their prevalence in the population. The 0.5 weight person is “oversampled”—we have too many people like this in our sample, as compared to the population so we “downweight” them. The third person is underrepresented, by contrast. We should have had three times as many of these types of people in our sample as we have.\nThus, sampling weights adjust for the probability of selecting an individual from the population when that probability is not constant (this could be due either by design or by chance). For nationally representative data surveys often select a sample where individuals have an unequal probability of being selected. This is done to increase the number of individuals and reduce sampling variability, particularly for certain areas or subgroups of the population. In some cases, corrections for non-response are also built into the weights. Sampling weights are then inversely proportional to the probability of selection.\nIn some complex surveys, there may be more than one sampling weight when different subsamples are selected. For example, the Demographic and Health Surveys (DHS)1 select a subsample of adults to be tested for HIV. If 1 in 5 households is selected for HIV testing, then no weighting is needed. But if 1 in 5 urban households and 1 in 2 rural households are selected, then sampling weights need to be applied to both descriptive statistics and model estimates to estimate at national level.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Survey Weights</span>"
    ]
  },
  {
    "objectID": "survey_weights.html#what-happens-if-you-ignore-the-weights",
    "href": "survey_weights.html#what-happens-if-you-ignore-the-weights",
    "title": "33  Survey Weights",
    "section": "33.4 What happens if you ignore the weights?",
    "text": "33.4 What happens if you ignore the weights?\nIn this case (as long as you are modeling the clustering correctly) you are estimating relationships on your sample, rather than the target population. This can be a totally fine thing: if you are interested in how some variables interrelate you might reasonably believe that a found pattern of relationships in the sample is very indicative of how things may play out on a wider stage. It would be odd for (statistically significant) relationships in the sample to not be at least somewhat similar to the population the sample came from. The true magnitudes may shift, but the story should be the same.\nFor example, if, after ignoring weights, you find an impact of some treatment, then you know the treatment works, at least for those in your sample. Even if your sample is a nonrepresentative sample of your population, it is still some sort of representation, and thus you would believe that your treatment would work to some degree more generally. In this case, any differences between your sample and population would be due to treatment variation, i.e., some in your sample respond differently than some in the population, and so your results in the sample would be weighting some people more than we “should,” causing the discrepancy.\nSurvey weights are usually much more important when trying to estimate level, or prevalence, of an outcome. If, for example, you are attempting to measure the average literacy in a population, then survey weights will be very important: if the weights of those systematically more (or less) literate are different, then ignoring the weights can cause bias. In addition, if some types of groups or areas are oversampled, then your estimates will tend to be biased towards the levels and relationships in the oversampled group/area. But if you are interested in the relationship between literacy and some covariate, the weights will matter less: it is only if the relationship between these variables is different in your high weight and low weight individuals where you will get bias. This is arguably a less natural phenomenon.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Survey Weights</span>"
    ]
  },
  {
    "objectID": "survey_weights.html#how-to-apply-weights",
    "href": "survey_weights.html#how-to-apply-weights",
    "title": "33  Survey Weights",
    "section": "33.5 How to apply weights?",
    "text": "33.5 How to apply weights?\nAs a rule of thumb, you want to first read any available documentation for the data you are using. You want to understand how the sample was obtained and how the weights were calculated. Publicly available data often comes with manuals on how to handle weights. Some manuals even come with R and Stata code! This is a very important step as sometimes you have to manipulate the weights before you can use them! For example, when using DHS data, you have to divide the weight by 1,000,000 before use. This is a function of how the weights are calculated. In addition, many complex surveys that use weights may also have stratified the sample, and that is also something to account for in your analysis.\nWhen using survey weights it is always advisable to compare the results that include weights with those without them. In general, one should not expect see substantive changes in the point estimates of regression coefficients to the point of dramatically changing one’s interpretation of one’s results. The model itself is supposed to capture structural relationships between covariates and outcomes, and under correct model specification the weights are superfluous with regards to these coefficients. Where weights could cause change is with descriptives such as the overall averages (e.g., the intercepts, in particular, could be different. We may also see changes in the variance parameters. Finally, with weights, one usually sees an increase in the the standard errors.\nA limitation with some packages is one might not have an easy way to obtain model fit statistics to help compare models. A clean way to avoid this is to go through the process of model selection using the data in the sample (ignoring the weights) and using the packages and approaches we have seen in class. The findings from such an exploration would be valid for the structure of the data in our sample. Then, as a second step, include the survey weights to move to inference to a larger population (for which the sample is supposed to be representative), taking the preferred model choices from step one and fitting them through a package that allows for survey weights.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Survey Weights</span>"
    ]
  },
  {
    "objectID": "survey_weights.html#further-references",
    "href": "survey_weights.html#further-references",
    "title": "33  Survey Weights",
    "section": "33.6 Further references",
    "text": "33.6 Further references\nFor some good resources see (Asparouhov and Muthen 2006; Carle 2009; Rabe-Hesketh and Skrondal 2006). Prior students previously also used Laukaityte and Wiberg (2018) and Lorah (2020) for some guidance. They then worked with the BIFIEsurvey R package to fit multi-level models with survey weights to account for the complex sampling design in their data.\n\n\n\n\nAsparouhov, Tihomir, and Bengt Muthen. 2006. “Multilevel Modeling of Complex Survey Data.” ASA Section on Survey Research Methods, 2718–26.\n\n\nCarle, Adam C. 2009. “Fitting Multilevel Models in Complex Survey Data with Design Weights: Recommendations.” BMC Medical Research Methodology 9 (49): 1–13. https://doi.org/10.1186/1471-2288-9-49.\n\n\nLaukaityte, Inga, and Marie Wiberg. 2018. “Importance of Sampling Weights in Multilevel Modeling of International Large-Scale Assessment Data.” Communications in Statistics-Theory and Methods 47 (20): 4991–5012.\n\n\nLorah, Julie. 2020. “Estimating a Multilevel Model with Complex Survey Data: Demonstration Using TIMSS.” Journal of Modern Applied Statistical Methods 18 (2): 24.\n\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2006. “Multilevel Modelling of Complex Survey Data.” Journal of the Royal Statistical Society, 805–27.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Survey Weights</span>"
    ]
  },
  {
    "objectID": "survey_weights.html#footnotes",
    "href": "survey_weights.html#footnotes",
    "title": "33  Survey Weights",
    "section": "",
    "text": "These are nationally representative surveys conducted in low- and middle-income countries collecting data primarily on maternal and child health.↩︎",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Survey Weights</span>"
    ]
  },
  {
    "objectID": "longitudinal.html",
    "href": "longitudinal.html",
    "title": "34  A flexible longitudinal model",
    "section": "",
    "text": "34.1 A nonparametric growth model\nFor the first idea, we make each age a factor, and then fit our model:\nM0 = lmer( ATTIT ~ 0 + age_fac + (1|ID), data=nys1 )\narm::display( M0 )\n\nlmer(formula = ATTIT ~ 0 + age_fac + (1 | ID), data = nys1)\n          coef.est coef.se\nage_fac11 0.21     0.02   \nage_fac12 0.23     0.02   \nage_fac13 0.33     0.02   \nage_fac14 0.41     0.02   \nage_fac15 0.45     0.02   \n\nError terms:\n Groups   Name        Std.Dev.\n ID       (Intercept) 0.19    \n Residual             0.18    \n---\nnumber of obs: 1079, groups: ID, 239\nAIC = -192.2, DIC = -272\ndeviance = -239.2\nNote how each wave (here age) has its own mean across our coefficients.\nWe can then plot our population average trajectory:\nnewdata &lt;- nys1 %&gt;%\n  dplyr::select( age_fac ) %&gt;%\n  unique()\nnewdata$ID = -1\nnewdata$ATTIT &lt;- predict(M0, newdata=newdata, re.form=NA)\n\nggplot(newdata, aes(age_fac, ATTIT, group=ID)) +\n  geom_line() + \n  geom_point() + \n  labs(title = \"Population average trajectory of attitude towards deviance over time\",\n       x = \"Age\",\n       y =\"Attitude towards deviance\")",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>A flexible longitudinal model</span>"
    ]
  },
  {
    "objectID": "longitudinal.html#adding-random-slopes",
    "href": "longitudinal.html#adding-random-slopes",
    "title": "34  A flexible longitudinal model",
    "section": "34.2 Adding random slopes",
    "text": "34.2 Adding random slopes\nOur model does not allow for individual trajectories for each student, however. We are only allowing for an intercept shift. This is where the trick comes in: we are going to let each student have their own random slope for growth rate, which will “tilt” our curve for each student. We do this by having a random slope on the continuous age variable, even though our fixed effects are on the factor age variable. We center age around the beginning of the study, so our random intercepts correspond to ATTIT at age 11.\n\nnys1$age_c = nys1$age - 11\nM1 = lmer( ATTIT ~ 0 + age_fac + (1+age_c|ID), data=nys1,\n           control = lmerControl(optimizer = 'bobyqa') )\narm::display( M1 )\n\nlmer(formula = ATTIT ~ 0 + age_fac + (1 + age_c | ID), data = nys1, \n    control = lmerControl(optimizer = \"bobyqa\"))\n          coef.est coef.se\nage_fac11 0.21     0.01   \nage_fac12 0.24     0.01   \nage_fac13 0.33     0.02   \nage_fac14 0.41     0.02   \nage_fac15 0.45     0.02   \n\nError terms:\n Groups   Name        Std.Dev. Corr \n ID       (Intercept) 0.12          \n          age_c       0.05     0.47 \n Residual             0.16          \n---\nnumber of obs: 1079, groups: ID, 239\nAIC = -298.5, DIC = -384\ndeviance = -350.1 \n\n\nWe can then plot individual trajectories to see how this model is working. We first make a set of 20 students to plot:\n\nset.seed( 40440 )\nsmp &lt;- sample( unique( nys1$ID ), 20 )\nsmp_dat &lt;- nys1 %&gt;% filter( ID %in% smp ) %&gt;%\n  complete( ID, age ) %&gt;%\n  mutate( age_fac = as.factor( age ),\n          age_c = age - 11 )\n\nEach student is five rows of data, sometimes with missing values due to the complete() method from above:\n\nfilter( smp_dat, ID == 52 ) %&gt;%\n  dplyr::select( ID, age, age_fac, age_c, ATTIT )\n\n# A tibble: 5 × 5\n     ID   age age_fac age_c ATTIT\n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    52    11 11          0 NA   \n2    52    12 12          1  0.29\n3    52    13 13          2  0.2 \n4    52    14 14          3  0.44\n5    52    15 15          4  0.44\n\n\nWe next predict the values for each student and plot those predicted values:\n\nsmp_dat$pred &lt;- predict(M1, newdata=smp_dat)\n\n# Make a population reference curve\nnewdata$age = 11:15\nnewdata$age_c = 0:4\nnewdata$pred &lt;- predict(M1, newdata=newdata, re.form=NA)\n\nggplot(smp_dat, aes(age, pred)) +\n  geom_line( aes( group=ID ), alpha=0.5) + \n  labs(title = \"Individual trajectories\",\n       x = \"Age\",\n       y =\"Attitude towards deviance\") +\n  geom_line( data = newdata, col=\"red\", linewidth=1 )\n\n\n\n\n\n\n\n\nNote how all our students have the same shape of our overall trajectory, but some are slightly steeper and some more shallow. This allows us to have a shared shape that we can shift (random intercept) and tilt (random slope) to fit the actual data.\nCompare our trajectories to the measured values:\n\nnewdata$ID = NULL\nggplot(smp_dat, aes(age, ATTIT)) +\n  facet_wrap( ~ ID ) +\n  geom_point() + \n  geom_line( aes( y = pred ), col=\"blue\" ) +\n  labs(title = \"Comparing latent curves to observed values\",\n       x = \"Age\",\n       y =\"Attitude towards deviance\") +\n    geom_line( data = newdata, col=\"red\", lty=2 )\n\n\n\n\n\n\n\n\nWe can see how the latent curves are trying to get close to the observed data for each student. Our fit seems reasonable, but not perfect.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>A flexible longitudinal model</span>"
    ]
  },
  {
    "objectID": "longitudinal.html#conclusion",
    "href": "longitudinal.html#conclusion",
    "title": "34  A flexible longitudinal model",
    "section": "34.3 Conclusion",
    "text": "34.3 Conclusion\nHopefully this tool is useful for examining how different waves of data may be different from one another. For example, if COVID happened in the middle of your study, you might expect a big shift in the data. This model allows you to model that shift while still allowing for different individual growth trajectories over time.",
    "crumbs": [
      "MODEL FITTING & INTERPRETATION",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>A flexible longitudinal model</span>"
    ]
  },
  {
    "objectID": "pooling.html",
    "href": "pooling.html",
    "title": "35  Pooling",
    "section": "",
    "text": "35.1 Pooled/unpooled v.s. fixed/random effects\nYou may have noticed that we use a couple of different terms interchangeably in this class when it comes to models. Sometimes we talk about coefficients as being completely pooled/partially pooled/unpooled, and sometimes we talk about coefficients as being random or fixed. Yikes, so confusing! Here’s a quick document explaining what these various terms mean and what sorts of models they represent. We’re only going to be talking about models where the pooling applied to the intercept and slope is the same; most models look like this, and these models are easier to talk about. You should be able to see how you might pool different coefficients differently, though the R code for that can be challenging. We’ll use the HSB data, and all of the models we’ll consider will look at regressions of math achievement on SES.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Pooling</span>"
    ]
  },
  {
    "objectID": "pooling.html#pooledunpooled-v.s.-fixedrandom-effects",
    "href": "pooling.html#pooledunpooled-v.s.-fixedrandom-effects",
    "title": "35  Pooling",
    "section": "",
    "text": "35.1.1 Completely pooled\nA completely pooled model is a model where we assume that every second-level unit (school) has the same intercept and slope (slopes and intercepts are both completely pooled). This doesn’t really have an analog in the fixed/random effects world.\nA completely pooled model in this setting might look like\n\\[\n\\begin{aligned}\n    mathach_i &= \\beta_0 + \\beta_1SES_i + \\varepsilon_i \\\\\n    \\varepsilon_i &\\sim Normal(0, \\sigma^2)\n\\end{aligned}\n\\]\nIn a completely pooled model we’re basically assuming that every school has the same intercept and slope, so we just ignore school membership; notice that we don’t even include the \\(j\\) subscript because we’re ignoring schools completely. How rude!\nWe would fit this model with the classic lm() call of\n\nlm(mathach ~ 1 + ses, data=dat)\n\n\n\n35.1.2 Partially pooled\nA partially pooled model allows for the possibility that different schools might have different slopes and intercepts, but assumes that these slopes and intercepts come from a Normal distribution, which has the effect of pulling them all in towards a grand mean (or partially pooling them). This model can also be called a model with random slopes and random intercepts, since we assume that school intercepts and residuals are random draws from a multivariate distribution with means equal to the grand means (and some possibly non-0 correlation). We don’t try to estimate these by themselves, only their variances and covariance.\nThis model can be represented as\n\\[\\begin{aligned}\nmathach_{ij} &= \\beta_{0j[i]} + \\beta_{1j[i]}SES_i + \\varepsilon_i, \\\\\n    \\beta_{0j} &= \\gamma_{00} + u_{0j},\\\\\n    \\beta_{1j} &= \\gamma_{10} + u_{1j},\\\\\n    \\varepsilon_i &\\sim Normal(0, \\sigma^2_\\varepsilon) \\\\\n    \\begin{pmatrix}\n        u_{0j}\\\\\n        u_{1j}\\\\\n    \\end{pmatrix} &\\sim  N\n    \\begin{bmatrix}\n        \\begin{pmatrix}\n            0\\\\\n            0\n        \\end{pmatrix}\\!\\!,&\n        \\begin{pmatrix}\n            \\sigma^2_0 & \\rho\\sigma_0\\sigma_1\\\\\n            \\rho\\sigma_0\\sigma_1 & \\sigma^2_1\n        \\end{pmatrix}\n    \\end{bmatrix}\n\\end{aligned}\\]\nWe would fit this model with\n\nlmer(mathach ~ 1 + ses + (1 + ses|school), data=dat)\n\n\n\n35.1.3 Unpooled\nIn an unpooled model, we don’t share any information across schools about the slopes and intercepts. Instead, we estimate each one separately in each higher-order unit. This is a fixed-effects model, because the model treats each school-level slope and intercept as a fixed quantity in the population to be estimated directly. In general parlance, to be a little more precise, a fixed-effects model is a model with unpooled intercepts and completely pooled slopes (although in theory the completely pooled model also has only fixed effects, it’s just that those effects are the same in every school; this is why the language of completely pooled, partially pooled, and unpooled coefficients is a little more precise, though it’s also less popular).\nWe could represent an unpooled model as\n\\[\\begin{aligned}\n    mathach_{ij} &= \\beta_{0j[i]} + \\beta_{1j[i]}SES_i + \\varepsilon_i \\\\\n    \\varepsilon_i &\\sim Normal(0, \\sigma^2)\n\\end{aligned}\\]\nWe would fit the model with\n\nlm(mathach ~ 1 + ses*school)\n\nalthough we might get our estimates in a more useful way by specifying an (identical) model which has no reference school, i.e.,\n\nlm(mathach ~ 0 + ses + ses:school)\n\nFor either of these models to fit you need to ensure that school is coded as a factor and not a number; this is not a concern for lmer().",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Pooling</span>"
    ]
  },
  {
    "objectID": "fixed_effects.html",
    "href": "fixed_effects.html",
    "title": "36  Clarification on Fixed Effects and Identification",
    "section": "",
    "text": "36.1 The language of “Fixed Effects”\nPeople will talk about “fixed effects” in (at least) two ways. The first is when you have a dummy variable for each of your clusters, and you are using OLS regression (not multilevel modeling). In this case you are estimating a parameter for each cluster, and we refer to that collection of estimates and parameters that go with these cluster level dummy variables as “fixed effects” and the model is a “fixed effects model.” The second is when you are using multilevel modeling, such as the following:\nM0 &lt;- lmer(Y ~ 1 + var1 + var2 + var3 + (var1|id), data)\nWhen we fit the above model, we will be estimating a grand intercept, and three coefficients for the three variables. Call these \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\). We are also estimating a random intercept and random slope for var1, with each group defined by the id variable having its own random intercept and slope. These are described by a variance-covariance matrix that we have been describing with \\(\\tau_{00}, \\tau_{01}, \\tau_{11}\\).\nNow, the \\(\\beta\\) are the fixed part, or fixed effects, of the model. The \\(\\tau\\) describe the random part or random effects. This is why, in R, we say fixef(M0) to get the \\(\\beta\\). If we say ranef(M0) we get the Empirical Bayes estimates of the random parts for each cluster. If we say coef(M0) R adds all this together to give the sum of the fixed part and random part, for each cluster defined by id.\nRead Gelman and Hill 12.3 for more on this sticky language. G&H do not like “fixed effects” as a description because it is so vague.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Clarification on Fixed Effects and Identification</span>"
    ]
  },
  {
    "objectID": "fixed_effects.html#underidentification",
    "href": "fixed_effects.html#underidentification",
    "title": "36  Clarification on Fixed Effects and Identification",
    "section": "36.2 Underidentification",
    "text": "36.2 Underidentification\nIf we fit a model with a dummy variable for each cluster, and a level to variable that does not vary within cluster, we say our model is “underidentified.” We say it is underidentified because no matter how much data we have, we will always have an infinite number of parameter values that can describe our model equally well. For example, say our level 2 variable is a dummy variable (e.g., sector). Then a model where we add five to the coefficient of the level 2 variable, and subtract five from all of the fixed effects for the clusters with sector=1 will fit our data just as well as one where we don’t. We can’t tell the difference! Hence we do not have enough to “identify” the parameter values.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Clarification on Fixed Effects and Identification</span>"
    ]
  },
  {
    "objectID": "fixed_effects.html#model-syntax-removing-the-main-ses-term-vs-not",
    "href": "fixed_effects.html#model-syntax-removing-the-main-ses-term-vs-not",
    "title": "36  Clarification on Fixed Effects and Identification",
    "section": "36.3 Model syntax: removing the main ses term vs not",
    "text": "36.3 Model syntax: removing the main ses term vs not\nWe talked about both these two models:\n\nM1 = lm( mathach ~ 0 + ses*id, data=dat.ten )\ncoef( M1 )\n\n       ses     id1288     id3533     id3881     id4530     id5761     id6074 \n 3.2554487 13.1149374 10.3671216 11.6441421 10.0390287 12.1419451 14.2022643 \n    id6170     id8800     id9225     id9347 ses:id3533 ses:id3881 ses:id4530 \n15.6332900  9.1573804 13.9360803 12.9702661 -3.5672188 -0.8647429 -1.6080227 \nses:id5761 ses:id6074 ses:id6170 ses:id8800 ses:id9225 ses:id9347 \n-0.1474381 -1.7263610  1.5563357 -0.6873233 -0.3695565 -0.5694548 \n\nM2 = lm( mathach ~ 0 + ses*id - ses, data=dat.ten )\ncoef( M2 )\n\n    id1288     id3533     id3881     id4530     id5761     id6074     id6170 \n13.1149374 10.3671216 11.6441421 10.0390287 12.1419451 14.2022643 15.6332900 \n    id8800     id9225     id9347 ses:id1288 ses:id3533 ses:id3881 ses:id4530 \n 9.1573804 13.9360803 12.9702661  3.2554487 -0.3117701  2.3907058  1.6474260 \nses:id5761 ses:id6074 ses:id6170 ses:id8800 ses:id9225 ses:id9347 \n 3.1080106  1.5290877  4.8117843  2.5681254  2.8858922  2.6859939 \n\n\nNote how when we remove ses via - ses we gain an extra interaction term of ses:id1288. In M1, our ses coefficient is our baseline slope of school 1288. The ses interaction terms are slope changes.\nNote how if we add ses to the changes we get back all the slopes in M2:\n\ncoef( M1 )[12:20] + coef(M1)[[1]]\n\nses:id3533 ses:id3881 ses:id4530 ses:id5761 ses:id6074 ses:id6170 ses:id8800 \n-0.3117701  2.3907058  1.6474260  3.1080106  1.5290877  4.8117843  2.5681254 \nses:id9225 ses:id9347 \n 2.8858922  2.6859939 \n\n\nBottom line: M1 and M2 are exactly the same in what they are describing, they are just parameterized differently. Anything we learn from one we could learn from the other.\n\n36.3.1 Plot our model\nTo plot our model we make a dataset of the intercepts and slopes of each school. Doing this with M2 is much easier than M1, since the coefficients are exactly what we want:\n\nlines = data.frame( id = names( coef(M2) )[1:10],\n                    inter = coef(M2)[1:10],\n                    slope = coef(M2)[11:20] )\n\n# we need to fix our IDs.  :-(\nlines$id = gsub( \"id\", \"\", lines$id)\nhead( lines )\n\n         id    inter      slope\nid1288 1288 13.11494  3.2554487\nid3533 3533 10.36712 -0.3117701\nid3881 3881 11.64414  2.3907058\nid4530 4530 10.03903  1.6474260\nid5761 5761 12.14195  3.1080106\nid6074 6074 14.20226  1.5290877\n\n\n(The gsub “substitutes” (replaces) the string “id” with “” in all of our ids so we get back to the actual school ids. Otherwise we will not be able to connect these data to our raw students as easily.)\nWe now plot!\n\nggplot( dat.ten, aes( ses, mathach ) ) +\n    facet_wrap( ~ id, nrow=2 ) +\n    geom_point( size=0.75, alpha=0.5 ) +\n    geom_abline( data=lines, aes( slope=slope, \n                                  intercept=inter ), \n                 col=\"red\", lwd=1 ) +\n    geom_vline( xintercept = 0 )\n\n\n\n\n\n\n\n\n\n\n36.3.2 What do the intercepts of any of the lines mean?\nThe intercepts predict what math achievement a studnet with ses = 0 going to a given school would have. For example, in school 8800, we predict a student with an ses of 0 would have a math achievement of 9.2.\nNotice that for some schools the intercept is extrapolating. E.g., most of school 8800’s students are below 0 for ses, and the intercept is thus describing what we expect for students at the higher end of their range. For school 9225, we are seeing a prediction for students a bit below the middle of their range.\n\n\n36.3.3 What differences, if any, are there between running a new linear model on each school vs. running the interacted model on the set of 10 schools?\nThe lines would be exactly the same. The standard errors are different. Here is the line on just school 1288:\n\ns1288 = filter( dat.ten, id == \"1288\" )\nM_1288 = lm( mathach ~ 1 + ses, data=s1288 )\nsummary( M_1288 )\n\n\nCall:\nlm(formula = mathach ~ 1 + ses, data = s1288)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.648  -5.700   1.048   4.420   9.415 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   13.115      1.387   9.456 2.17e-09 ***\nses            3.255      2.080   1.565    0.131    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.819 on 23 degrees of freedom\nMultiple R-squared:  0.09628,   Adjusted R-squared:  0.05699 \nF-statistic:  2.45 on 1 and 23 DF,  p-value: 0.1312\n\n\nThe SEs will be different, however. Compare:\n\nsum = summary( M2 )\nsum$coefficients[ c(1, 11 ), ]\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\nid1288     13.114937   1.291495 10.154850 9.121968e-22\nses:id1288  3.255449   1.936454  1.681139 9.349559e-02\n\n\nIn this case the SEs are close, but they could be a lot different if we have a lot of heteroskedasticity or the school has few data points so we do a bad job estimating uncertainty.\nThe key is in the single model we are using all the schools to estimate the residual variance, and this is the number that drives our SE estimates.\n\n\n36.3.4 Do we trust the red lines on the plot? Why or why not?\nWe trust them because they are driven just by the school data, so they are essentially unbiased. But these are small datasets, so they are unstable.\n\n\n36.3.5 What about the variability in the slopes and intercepts of the red lines?\nThe variation is not to be trusted. The slopes are varying because of measurement error. For example, it is unlikely school 3533 really has a negative slope. It is more likely we just got some low performing high ses kids by happenstance in our sample. Similarly, it is unlikely school 6170 has such a steep slope. It has few kids, and the kid with less than -2 ses and a very low math achievment is likely an influential point in that regression.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Clarification on Fixed Effects and Identification</span>"
    ]
  },
  {
    "objectID": "fixed_effects.html#further-reading",
    "href": "fixed_effects.html#further-reading",
    "title": "36  Clarification on Fixed Effects and Identification",
    "section": "36.4 Further Reading",
    "text": "36.4 Further Reading\n(Antonakis, Bastardoz, and Rönkkö 2019)\n\n\n\n\nAntonakis, John, Nicolas Bastardoz, and Mikko Rönkkö. 2019. “On Ignoring the Random Effects Assumption in Multilevel Models: Review, Critique, and Recommendations.” Organizational Research Methods 24 (2): 443–83. https://doi.org/10.1177/1094428119877457.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Clarification on Fixed Effects and Identification</span>"
    ]
  },
  {
    "objectID": "fe_crve.html",
    "href": "fe_crve.html",
    "title": "37  A tour of fixed effects and cluster-robust SEs",
    "section": "",
    "text": "37.1 Aggregation\nOne way forward is to aggregate our HS&B data and merge it into our school-level data, and then analyze the result.\nWe aggregate as so:\ncol.dat = dat %&gt;% group_by( id ) %&gt;% \n    summarize( per.fem = mean(female),\n                 per.min = mean(minority),\n                 mean.ses = mean(ses),\n                 mean.ach = mean(mathach),\n                 n.stud = n() )\n\n# combine our school-level variables (ours and theirs) into one data.frame\nsdat = merge( sdat, col.dat, by=\"id\", all=TRUE )\nhead( sdat )\n\n    id size sector pracad disclim himinty meanses   per.fem    per.min\n1 1224  842      0   0.35   1.597       0  -0.428 0.5957447 0.08510638\n2 1288 1855      0   0.27   0.174       0   0.128 0.4400000 0.12000000\n3 1296 1719      0   0.32  -0.137       1  -0.420 0.6458333 0.97916667\n4 1308  716      1   0.96  -0.622       0   0.534 0.0000000 0.40000000\n5 1317  455      1   0.95  -1.694       1   0.351 1.0000000 0.72916667\n6 1358 1430      0   0.25   1.535       0  -0.014 0.3666667 0.10000000\n     mean.ses  mean.ach n.stud\n1 -0.43438298  9.715447     47\n2  0.12160000 13.510800     25\n3 -0.42550000  7.635958     48\n4  0.52800000 16.255500     20\n5  0.34533333 13.177687     48\n6 -0.01966667 11.206233     30\nWe can now answer our research question with a school-level regression with lm_robust(), that calculates heteroskedastic-robust standard errors:\nlibrary( estimatr )\nMagg = lm_robust( mean.ach ~ 1 + sector + mean.ses, data=sdat )\n\ntidy( Magg )\n\n         term  estimate std.error statistic       p.value   conf.low conf.high\n1 (Intercept) 12.119496 0.1890070 64.121943 1.628760e-114 11.7461711 12.492820\n2      sector  1.221944 0.3226998  3.786627  2.170049e-04  0.5845507  1.859337\n3    mean.ses  5.387377 0.3423555 15.736205  4.289753e-34  4.7111599  6.063594\n   df  outcome\n1 157 mean.ach\n2 157 mean.ach\n3 157 mean.ach\nThe lm_robust() method gives heteroskedastic robust standard errors that take into account possible heteroskedasticity due to, for example, some school outcomes being based on smaller numbers of students (and thus having more variation) than other school outcomes.\nIn this regression we are controlling for school mean SES, not student SES. If anything is going on within school between SES and math achievement, in a way that could be different for different sectors, we might be missing it.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>A tour of fixed effects and cluster-robust SEs</span>"
    ]
  },
  {
    "objectID": "fe_crve.html#cluster-robust-standard-errors",
    "href": "fe_crve.html#cluster-robust-standard-errors",
    "title": "37  A tour of fixed effects and cluster-robust SEs",
    "section": "37.2 Cluster Robust Standard Errors",
    "text": "37.2 Cluster Robust Standard Errors\nInstead of using our aggregated data, we can merge our school-level variables into the student data and run a student level regression:\nThe merge brings in level 2 variables, repeating them for each student in a school:\n\ndat = merge( dat, sdat, by=\"id\" )\nhead( dat )\n\n    id minority female    ses mathach size sector pracad disclim himinty\n1 1224        0      1 -1.528   5.876  842      0   0.35   1.597       0\n2 1224        0      1 -0.588  19.708  842      0   0.35   1.597       0\n3 1224        0      0 -0.528  20.349  842      0   0.35   1.597       0\n4 1224        0      0 -0.668   8.781  842      0   0.35   1.597       0\n5 1224        0      0 -0.158  17.898  842      0   0.35   1.597       0\n6 1224        0      0  0.022   4.583  842      0   0.35   1.597       0\n  meanses   per.fem    per.min  mean.ses mean.ach n.stud\n1  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n2  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n3  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n4  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n5  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n6  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n\n\nIf we run our regression without handling our clustering, we get fine point estimates, but our standard errors are wrong:\n\nMstud = lm( mathach ~ 1 + sector + ses, data = dat )\nbroom::tidy( Mstud ) %&gt;% \n  knitr::kable( digits=2 )\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n11.79\n0.11\n111.15\n0\n\n\nsector\n1.94\n0.15\n12.69\n0\n\n\nses\n2.95\n0.10\n30.14\n0\n\n\n\n\n\nThe standard errors for the above regression, however, is wrong: we are not taking the clustering into account. We can fix this with cluster-robust standard errors. The lm_robust() method comes to the rescue:\n\nMstud &lt;- lm_robust( mathach ~ 1 + sector + ses, \n                       data = dat,\n                       clusters = dat$id )\nbroom::tidy( Mstud ) %&gt;% \n  knitr::kable( digits=2 )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\ndf\noutcome\n\n\n\n\n(Intercept)\n11.79\n0.20\n57.85\n0\n11.39\n12.20\n84.12\nmathach\n\n\nsector\n1.94\n0.32\n6.08\n0\n1.31\n2.56\n141.46\nmathach\n\n\nses\n2.95\n0.13\n22.95\n0\n2.69\n3.20\n132.91\nmathach\n\n\n\n\n\nWe specify the clustering and lm_robust() does the rest; note that we would normally not even run the original lm() command. The lm_robust() command replaces it.\nFor our research question, we see that Catholic schools score about 2 points higher than Public, on average, beyond individual level SES.\nWe can further control for school mean SES, like with aggregation:\n\nMstud2 = lm_robust( mathach ~ 1 + sector + ses + meanses, \n                    data = dat,\n                    cluster = dat$id )\nrs &lt;- broom::tidy( Mstud2 ) \nrs %&gt;% \n  knitr::kable( digits=2 )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\ndf\noutcome\n\n\n\n\n(Intercept)\n12.10\n0.17\n70.65\n0\n11.76\n12.44\n81.55\nmathach\n\n\nsector\n1.28\n0.30\n4.24\n0\n0.68\n1.88\n95.16\nmathach\n\n\nses\n2.19\n0.13\n16.87\n0\n1.93\n2.45\n140.83\nmathach\n\n\nmeanses\n2.97\n0.37\n8.07\n0\n2.24\n3.71\n75.50\nmathach\n\n\n\n\n\nThe contextual value of school mean SES is explaining some of the difference between Catholic and public schools, here: note the reduction of the coefficient for sector. That being said, and still accounting for clustering, sector is still quite significant. The lm_robust() function is also giving us confidence intevals, which is nice: we see anything between 0.7 and 1.9 is possible.\nRelative to the overall standard deviation of math achievement we have:\n\nsd_math = sd( dat$mathach )\nsd_math\n\n[1] 6.878246\n\nrs %&gt;%\n  mutate( estimate = estimate / sd_math,\n          std.error = std.error / sd_math,\n          conf.low = conf.low / sd_math,\n          conf.high = conf.high / sd_math ) %&gt;%\n  knitr::kable( digits = 2 )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\ndf\noutcome\n\n\n\n\n(Intercept)\n1.76\n0.02\n70.65\n0\n1.71\n1.81\n81.55\nmathach\n\n\nsector\n0.19\n0.04\n4.24\n0\n0.10\n0.27\n95.16\nmathach\n\n\nses\n0.32\n0.02\n16.87\n0\n0.28\n0.36\n140.83\nmathach\n\n\nmeanses\n0.43\n0.05\n8.07\n0\n0.33\n0.54\n75.50\nmathach\n\n\n\n\n\n(We have rescaled all our estimates by the standard deviation, which puts things into effect size units, i.e., how many standard deviations large everything is.) We now see the difference between Catholic and public schools is somewhere between 0.10 and 0.27 standard deviations, beyond what can be explained by ses. This is a fairly sizable effect, in education.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>A tour of fixed effects and cluster-robust SEs</span>"
    ]
  },
  {
    "objectID": "fe_crve.html#and-fixed-effects",
    "href": "fe_crve.html#and-fixed-effects",
    "title": "37  A tour of fixed effects and cluster-robust SEs",
    "section": "37.3 And fixed effects?",
    "text": "37.3 And fixed effects?\nWe can combine fixed effects and cluster robust standard errors quite easily, but we cannot combine fixed effects and level 2 covariates at all. We next look at this latter problem, and then see what combining these options looks like when asking questions that do not rely on level 2 variables for main effects.\n\n37.3.1 The problem of fixed effects and level-2 variables\nFixed effects cannot be used to take into account school differences if we are interested in level 2 variables, because the fixed effects and level 2 variables are co-linear. Put another way, if we let each school have its own mean outcome (represented by the coefficient for a dummy variable for that school), then we can’t have a variable like sector to measure how Catholic schools are different from public schools, conditioned on all the school mean outcomes. There is nothing left to explain as, by construction, there are no differences in school mean outcomes once we “control for” the individual school mean outcomes via fixed effects!\nWhat R will do when you give colinear variables is drop the extra ones. Here is a mini-example fake dataset of 4 schools with 3 students in each school:\n\nfake = tibble( id = c( 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4),\n                   mathach = rnorm( 12 ),\n                   ses = rnorm( 12 ),\n                   sector = c( 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1 ) )\nfake$id = as.factor( fake$id )\nfake\n\n# A tibble: 12 × 4\n   id    mathach     ses sector\n   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 1      2.87    1.15        0\n 2 1     -0.475   1.42        0\n 3 1     -0.0762 -1.24        0\n 4 2      0.446  -0.0235      1\n 5 2     -1.20   -1.14        1\n 6 2     -0.308   0.983       1\n 7 3     -0.0716  0.693       0\n 8 3     -0.0275  1.08        0\n 9 3      0.295  -0.598       0\n10 4      0.417   0.571       1\n11 4     -1.14    0.775       1\n12 4     -0.730  -0.345       1\n\n\nAnd our regression model with fixed effects for school plus our school-level ses gives this:\n\nlm( mathach ~ 0 + id + ses + sector, data = fake )\n\n\nCall:\nlm(formula = mathach ~ 0 + id + ses + sector, data = fake)\n\nCoefficients:\n     id1       id2       id3       id4       ses    sector  \n 0.64933  -0.33663  -0.04371  -0.57742   0.27813        NA  \n\n\nNote the NA for sector! We cannot estimate it due to colinearity, so it got dropped.\n\n\n37.3.2 Fixed effects can handle clustering\nThat being said, fixed effects are an excellent way to control for school differences when looking at within-school relationships. For example, we can ask how math relates to SES within schools, controlling for systematic differences across schools.\nHere is the no fixed effect regression, and the fixed effect regression:\n\nMstud3_noFE = lm( mathach ~ 1 + ses, data=dat )\n\ndat$id = as.factor(dat$id)\nMstud3 = lm( mathach ~ 0 + ses + id, data=dat )\nhead( coef( Mstud3 ) )\n\n      ses    id1224    id1288    id1296    id1308    id1317 \n 2.191172 10.667255 13.244353  8.568302 15.098561 12.421003 \n\n\nFor our fixed effect model, we will have lots of coefficients because we have a fixed effect for each school; the head() command is just showing us the first few. We also had to explicitly make our id variable a factor (categorical variable), so R doesn’t think it is a continuous covariate.\nFor our standard errors, etc., we can further account for clustering of our residuals above and beyond what can be explained by our fixed effects (even if we subtract out the mean outcome, we might still have dependencies between students within a given school). So we use our cluster-robust standard errors as so:\n\nMstud3_rob &lt;- lm_robust( mathach ~ 0 + ses + id, \n                         data=dat,\n                         cluster = dat$id )\nhead( tidy( Mstud3_rob ) )\n\n    term  estimate  std.error statistic       p.value  conf.low conf.high\n1    ses  2.191172 0.12984948  16.87471  1.239339e-35  1.934466  2.447878\n2 id1224 10.667255 0.05640441 189.12095 2.389336e-171 10.555746 10.778763\n3 id1288 13.244353 0.01578970 838.79718 2.446556e-262 13.213138 13.275569\n4 id1296  8.568302 0.05525096 155.07971 2.866668e-159  8.459073  8.677531\n5 id1308 15.098561 0.06856053 220.22236 1.252769e-180 14.963020 15.234102\n6 id1317 12.421003 0.04484136 276.99883 1.263605e-194 12.332354 12.509652\n        df outcome\n1 140.8263 mathach\n2 140.8263 mathach\n3 140.8263 mathach\n4 140.8263 mathach\n5 140.8263 mathach\n6 140.8263 mathach\n\n\nWe have again used head() to just get the first lines. The whole printout would be one line per school, plus the ses coefficient!\nLet’s compare our three models (note the way we omit coefficients with id to drop our fixed effects from the table):\n\nlibrary( texreg )\nscreenreg( list( `No FE`=Mstud3_noFE, `FE` = Mstud3, `FE + CRVE`=Mstud3_rob ), \n           omit.coef=\"id\",\n           include.ci = FALSE )\n\n\n==================================================\n             No FE        FE           FE + CRVE  \n--------------------------------------------------\n(Intercept)    12.75 ***                          \n               (0.08)                             \nses             3.18 ***     2.19 ***     2.19 ***\n               (0.10)       (0.11)       (0.13)   \n--------------------------------------------------\nR^2             0.13         0.83         0.83    \nAdj. R^2        0.13         0.82         0.82    \nNum. obs.    7185         7185         7185       \nRMSE                                      6.08    \nN Clusters                              160       \n==================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nA few things to note:\n\nNot having fixed effects means we are getting an estimate of the math-ses relationship including school level context. Note the higher point estimate. Often we want to focus on within-school relationships. Fixed effects does this.\nThe standard errors are larger once we include fixed effects; the fixed effects are partially accounting for clustering.\nThe standard errors are even larger when we include CRVE. It is more fully accounting for the clustering, and the fact that the clusters themselves could vary. In general, one should typically use CRVE in addition to fixed effects, if one wants to view the clusters as representative of a larger population (in this case a larger population of schools).\n\n\n\n37.3.3 Bonus: Interactions with level-2 variables are OK, even with fixed effects\nIf we want to see if the relationship of math and SES is different between schools, we can get tricky like so:\n\nMstud4 = lm_robust( mathach ~ 0 + ses + ses:sector + id, \n                    data=dat,\n                    cluster = id )\nhead( coef( Mstud4 ) )\n\n      ses    id1224    id1288    id1296    id1308    id1317 \n 2.782105 10.923946 13.172496  8.819744 15.498595 12.682641 \n\ntail( coef( Mstud4 ) )\n\n    id9359     id9397     id9508     id9550     id9586 ses:sector \n 14.763044   9.982311  13.772485  10.941590  13.973252  -1.348572 \n\n\nNote interaction terms always get pushed to the end of the list of estimates by R. So we have to pull them out with tail().\nIn the following we compare SEs to if we hadn’t used cluster robust SEs.\n\na &lt;- lm( mathach ~ 0 + ses + ses:sector + id, \n                    data=dat )\nscreenreg( list( wrong=a, adjusted=Mstud4 ), \n           omit.coef=\"id\", single.row = TRUE,\n           include.ci=FALSE )\n\n\n==================================================\n            wrong               adjusted          \n--------------------------------------------------\nses            2.78 (0.14) ***     2.78 (0.16) ***\nses:sector    -1.35 (0.22) ***    -1.35 (0.23) ***\n--------------------------------------------------\nR^2            0.83                0.83           \nAdj. R^2       0.82                0.82           \nNum. obs.   7185                7185              \nRMSE                               6.07           \nN Clusters                       160              \n==================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nIn our second column we are accounting for our clustering with our cluster robust SEs.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>A tour of fixed effects and cluster-robust SEs</span>"
    ]
  },
  {
    "objectID": "fe_crve.html#fixed-effects-vs.-cluster-robust-ses",
    "href": "fe_crve.html#fixed-effects-vs.-cluster-robust-ses",
    "title": "37  A tour of fixed effects and cluster-robust SEs",
    "section": "37.4 Fixed effects vs. cluster robust SEs",
    "text": "37.4 Fixed effects vs. cluster robust SEs\nWhen running a regression with fixed effects and cluster-robust SEs, we might wonder when to use one vs. another, and when to use both. Here’s a breakdown of when to use each:\n\n37.4.1 Fixed Effects\nUse fixed effects when you want to control for unobserved variables that vary across groups (e.g., states, countries) but are constant over time or within those groups. Fixed effects helps eliminate bias from omitted variables that are group-specific and time-invariant.\n\nExample: You are analyzing the effect of tuition fees on graduation rates, but there are unobserved factors (like state policies) that may affect both tuition and graduation rates.\nWhen to use: If you believe that there are unobserved group-level characteristics that need to be controlled for.\n\nImportantly, fixed-effects means you are estimating within group effects: you are no longer comparing one group to another.\nFixed effects by themselves can increase the plausibility of the residual independence assumption within groups. Without FEs (and no cluster-robust SEs) your SEs could be off as they are not accounting for the correlation of units within each group. FEs makes it more easy to believe your individual units are independent. So, roughly speaking, in many cases including fixed effects not only removes bias but also fixes your independence assumption for clustered data!\n\n\n37.4.2 Cluster-Robust Standard Errors\nCluster-robust standard errors are used when you believe that observations within the same group (e.g., individuals within a state or students within a school) may be correlated. This method adjusts standard errors to account for potential intra-group correlation, ensuring more reliable inference.\n\nExample: If students within the same community college may have correlated outcomes due to shared environments.\nWhen to use: When there may be correlation in the error terms within groups, which could lead to underestimated standard errors.\n\nBut we just said fixed effects does this! CRSEs do this in a more robust way, making virtually no assumption on how units within groups might co-vary. But then why would we use both?\n\n\n37.4.3 Using Both\nIf fixed effects account for clustering in your SEs, why bother with cluster-robust standard errors? That is an interesting question. Use both fixed effects and cluster-robust standard errors when:\n\nYou want to control for unobserved, time-invariant group-level factors with fixed effects.\nYou also suspect that there’s within-group correlation in the residuals even when pulling out the common fixed effect. This could be if you had further clustering within the cluster, for example (e.g., your school data was made by sampling a few classes from within the school). If that were happening, your SEs could be biased even when you include fixed effects.\n\n\nExample: In a model of student performance across community colleges in different states, you may use fixed effects to control for state-level policies and cluster-robust standard errors to account for possible correlations between students in the same college.\n\nThere is another reason you might include CRSEs in your fixed effect model: you view your clusters as a sample from some larger population, and you want to get uncertainty estimates that include the question of whether the clusters in your data are representative of this larger population.\nFixed effects only just targets your evaluation sample (the data you have) and holds the clusters as fixed: you are estimating trends for those clusters in your data, and no further. CRSEs will assess cluster variation, and then give you SEs that include how that variation might make you more uncertain as to what you would find if you collected more clusters like the clusters you have.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>A tour of fixed effects and cluster-robust SEs</span>"
    ]
  },
  {
    "objectID": "robust_mlm.html",
    "href": "robust_mlm.html",
    "title": "38  MLM and Cluster-Robust Standard Errors",
    "section": "",
    "text": "38.1 Robust standard errors without multilevel modeling\nBefore diving into MLM, one way to get classic Huber-White / Sandwich / Heteroskedastic-Robust Standard Errors for vanilla OLS for a single school (school 8857) is via the sandwich package.\nFirst we fit our regression, like we would normally:\none.sch = filter( dat, id == \"8857\" )\nnrow( one.sch )\n\n[1] 64\n\nM0 &lt;- lm( mathach ~ 1 + ses, dat) \narm::display( M0 )\n\nlm(formula = mathach ~ 1 + ses, data = dat)\n            coef.est coef.se\n(Intercept) 12.75     0.08  \nses          3.18     0.10  \n---\nn = 7185, k = 2\nresidual sd = 6.42, R-Squared = 0.13\nThen we use the sandwich and lmtest package:\nlibrary( sandwich )\nlibrary( lmtest )\nlmtest::coeftest(M0, type = \"HC1\")\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 12.747396   0.075686 168.424 &lt; 2.2e-16 ***\nses          3.183870   0.097121  32.782 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThis is how you would get cluster robust standard errors when you have clustered data, but have not bothered with multilevel modeling:\nM1 = lm( mathach ~ 1 + ses + sector, data = dat )\nlmtest::coeftest( M1, type = \"CL\", cluster = dat$id )\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.106102 111.150 &lt; 2.2e-16 ***\nses          2.948558   0.097831  30.139 &lt; 2.2e-16 ***\nsector       1.935013   0.152493  12.689 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAs an alternative, the lm_robust method from the estimatr package does these steps in a single step, and is easier to use. But the above sets us up nicely for understanding how to add robustness to a MLM.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>MLM and Cluster-Robust Standard Errors</span>"
    ]
  },
  {
    "objectID": "robust_mlm.html#crse-on-top-of-multilevel-modeling",
    "href": "robust_mlm.html#crse-on-top-of-multilevel-modeling",
    "title": "38  MLM and Cluster-Robust Standard Errors",
    "section": "38.2 CRSE on top of Multilevel Modeling",
    "text": "38.2 CRSE on top of Multilevel Modeling\nOk, so we have seen how to get robust standard errors in the above; how do we combine them with multilevel modeling? First, let’s fit our multilevel model:\n\nM2 = lmer( mathach ~ 1 + ses + sector + (1|id), data=dat )\ndisplay( M2 )\n\nlmer(formula = mathach ~ 1 + ses + sector + (1 | id), data = dat)\n            coef.est coef.se\n(Intercept) 11.72     0.23  \nses          2.37     0.11  \nsector       2.10     0.34  \n\nError terms:\n Groups   Name        Std.Dev.\n id       (Intercept) 1.92    \n Residual             6.09    \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46621.2, DIC = 46601.7\ndeviance = 46606.4 \n\n\nIf we believe all our MLM assumptions, we can get our vanilla standard errors as so:\n\nsummary( M2 )$coef\n\n             Estimate Std. Error   t value\n(Intercept) 11.718908  0.2280585 51.385537\nses          2.374711  0.1054911 22.511017\nsector       2.100837  0.3411243  6.158567\n\n\nIf we don’t believe them fully, we might want to make our inference more robust. Before we turn to this, first note that our point estimates can be different for MLM vs OLS:\n\ncoef( M1 )\n\n(Intercept)         ses      sector \n  11.793254    2.948558    1.935013 \n\nfixef( M2 )\n\n(Intercept)         ses      sector \n  11.718908    2.374711    2.100837 \n\n\nThe assumption of the random effects means we are not weighting all our data the same way. For example, if we find the clusters vary in size a lot, we might weight the clusters more equally when estimating a cluster-level coefficient (e.g., sector) instead of counting on the big clusters more.\nRegardless, we might worry that complex dependencies within our clusters are messing up our standard errors in our MLM. Fixing that is easy:\n\nlibrary( clubSandwich )\nclub &lt;- coef_test( M2,\n           vcov = \"CR1S\",\n           test = \"Satterthwaite\")\nclub\n\n       Coef. Estimate    SE t-stat d.f. (Satt) p-val (Satt) Sig.\n (Intercept)    11.72 0.226  51.86        88.9       &lt;0.001  ***\n         ses     2.37 0.119  19.89       142.8       &lt;0.001  ***\n      sector     2.10 0.347   6.05       149.5       &lt;0.001  ***\n\n\nThe clubSandwich package works for multi-level models fit with either lme4::lmer() or nlme::lme(). Note that coef_test is not the same as coeftest. The vcov = \"CR1S\" replicates the Stata SEs (or so it has been speculated, and assuming they use the same correction as for panel data models).\nWe can compare the SEs as so:\n\nrbind( club$SE, se.fixef(M2) )\n\n     (Intercept)       ses    sector\n[1,]   0.2259713 0.1193704 0.3474632\n[2,]   0.2280585 0.1054911 0.3411243\n\n\nWe see that the SEs did not change much as compared to the vanilla lmer call that assumes homoskedasticity and within-cluster independence of the residuals in this particular circumstance.\n\n38.2.1 What misspecification should we worry about?\nThe sorts of misspecification that we might be worried about are things such as the following:\n\nUsing a random intercept model when the real data-generating process has a random slope;\nUsing a model that assumes homogeneous random effects when the real data-generating process involves heteroskedasticity (e.g., different random effects variances for treatment schools than for control schools);\nUsing a model with a single level of random effects (e.g., school random effects) when the real data-generating process has multiple levels of structure (e.g., school and classroom random effects); or\nAssuming homoscedastic variance for the lowest-level errors when the real process is heteroskedastic or has some other structure.\n\nOf the above (3) could be due to “secret clustering” in your clusters, and in principle result in radically incorrect standard errors. The other options are more violations of homoskedasticity, and are likely to not be as serious of concerns. You can diagnose, in principle, heteroskedasticity with residual plots, checking to see if you have more scatter in your data for some groups or individuals than others.\nRegardless, if you are worried about these things, then the above will give you improved standard errors.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>MLM and Cluster-Robust Standard Errors</span>"
    ]
  },
  {
    "objectID": "robust_mlm.html#some-technical-notes",
    "href": "robust_mlm.html#some-technical-notes",
    "title": "38  MLM and Cluster-Robust Standard Errors",
    "section": "38.3 Some technical notes",
    "text": "38.3 Some technical notes\nSo what is this thing even doing? In the following I describe a rough approximation. The key idea is that a multilevel model specification is specifying a parameterized \\(n \\times n\\) variance-covariance matrix of the residuals of a generic linear model. Due to our assumption of independent clusters, this matrix is block diagonal with blocks \\(V_1, \\ldots, V_J\\), with block \\(V_j\\) corresponding to group \\(j\\). For a random intercept model, for example, block \\(j\\) would be a \\(n_j \\times n_j\\) matrix with \\(\\tau_{00} + \\sigma^2\\) for the diagonal and \\(\\tau_{00}\\) for the off-diagonal, with \\(\\tau_{00}\\) being the variance of the random intercepts and \\(\\sigma^2\\) being the within-block residual variance.\n\nIf we write our multilevel model in reduced form, we can write it as a mini-regression for each group \\(j\\) of: \\[\nY_j = X_j \\vec{\\beta} + Z_j \\vec{r}_j + e_j ,\n\\] where \\(Y_j\\) is the vector of outcomes, \\(X_j\\) and \\(Z_j\\) are mini design matrices of covariates (including a column of 1s for the intercept, normally), with \\(X_j\\) being all the covariates and \\(Z_j\\) being those covariates that have corresponding random effects (also with a column of 1s), \\(\\vec{\\beta}\\) the vector of coefficients (the fixed effects), \\(\\vec{r}_j\\) the vector of random effects for group \\(j\\), and \\(e_j\\) the vector of residuals.\nImportantly, the \\(u_j := Z_j \\vec{u}_j + e_j\\) is all residual, and \\(V_j = Var( u_j )\\): the variance-covariance matrix of the residuals is determined by this structure and our assumptions on \\(\\vec{u}_j\\) being multivariate normal and the \\(e_j\\) being a vector of independent residual draws (the \\(\\epsilon_{ij}\\)).\nNow, given this view of our multilevel model, we can estimated this with generalized least squares. Generalized least squares is a generic regression technique where, if you have a parameterized covariance matrix on the residuals, you can estimate your regression coefficients taking that correlation structure into account. Think of it as a three-step process: first fit the regression without taking the residuals into account, then use the fit model to estimate our big \\(n \\times n\\) variance-covariance matrix, and then use this estimated matrix as a set of weights that we plug back into a least squares estimation.\nIn particular, the estimator for \\(\\vec{\\beta}\\) is weighted least squares: \\[ \\hat{\\beta} = (X'WX)^{-1}X'W Y ,  \\] with \\(W\\) a weight matrix that is a \\(n \\times n\\) block-diagonal matrix formed from the inverses of the estimated \\(V_j\\). Now if the random effects structure (the assumed distribution of the \\(r_j\\)) is misspecified or the residual error structure (on the \\(\\epsilon_{ij}\\)) is wrong, then \\(V_j\\) will be wrong, but \\(\\hat{\\beta}\\) will still be asymptotically consistent (under some conditions).\nCluster-robust methods use the empirical residuals (the \\(\\hat{u}_{j}\\)) to assess the uncertainty in \\(\\hat{\\beta}\\) as an estimate of the \\(\\beta\\) as defined by the implied weights \\(W\\). Even if the random effects part of the model is wrong, the assumption of independent clusters means our inference on this estimand is still right. The key idea is cluster-robust methods take a weighted average of \\(J\\) very badly estimated variance-covariance matrices to get a decent estimate of overall population-level uncertainty.\nThe main advantage of the clubSandwich package is it will take our multilevel model and do this cluster-roboust standard error calculation. Even better, however, is it will (using “CR2” adjustment) try to improve the basic sandwich estimator by 1) adjusting the residuals (the \\(\\hat{u}_j\\)) a bit so that the variance estimator is exactly unbiased if the working model is exactly correct and b) using Satterthwaite degrees of freedom (or generalizations thereof) for tests/confidence intervals, also derived under the assumption that the working model is exactly correct.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>MLM and Cluster-Robust Standard Errors</span>"
    ]
  },
  {
    "objectID": "robust_mlm.html#acknowledgements",
    "href": "robust_mlm.html#acknowledgements",
    "title": "38  MLM and Cluster-Robust Standard Errors",
    "section": "38.4 Acknowledgements",
    "text": "38.4 Acknowledgements\nThanks to James Pustejovsky, the creator of the clubSandwich package, for the help in thinking this through. Much of these notes, in particular the reasons for misspecification and much of the technical notes, are liberally stolen from emails with this fine colleague.",
    "crumbs": [
      "FIXED EFFECTS and FRIENDS",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>MLM and Cluster-Robust Standard Errors</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html",
    "href": "hsb_ex.html",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "",
    "text": "39.1 R Setup\nlibrary(foreign) #this lets us read in spss files\nlibrary(tidyverse) #this is a broad package that allows us to do lots of data management-y things (and ggplot!)\nlibrary(lme4) #this allows us to run MLM\nlibrary(arm) #this allows us to display MLM\nlibrary( lmerTest ) # this puts p-values on the summary() command for fixed effects",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#load-hsb-data",
    "href": "hsb_ex.html#load-hsb-data",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.2 Load HS&B data",
    "text": "39.2 Load HS&B data\n\n# Read student data\nstud.dat = read.spss( \"data/hsb1.sav\", to.data.frame=TRUE )\n\n# Read in school data\nsch.dat = read.spss( \"data/hsb2.sav\", to.data.frame=TRUE )\n\n# Make single data frame with all variables, keep all students even if they\n# don't match to a school\ndat = merge( stud.dat, sch.dat, by=\"id\", all.x=TRUE )",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#table-4.1-descriptive-summaries",
    "href": "hsb_ex.html#table-4.1-descriptive-summaries",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.3 Table 4.1 Descriptive summaries",
    "text": "39.3 Table 4.1 Descriptive summaries\n\n## Get mean and SD of the Level 1 variables, rounded to 2 decimal places\n# math achievement\nround(mean(dat$mathach),2)\n\n[1] 12.75\n\nround(sd(dat$mathach),2)\n\n[1] 6.88\n\n# ses\nround(mean(dat$ses),2)\n\n[1] 0\n\nround(sd(dat$ses),2)\n\n[1] 0.78\n\n## Get mean and SD of Level 2 variables, round to 2 decimal places\n# NOTE: we are getting these from the SCHOOL-LEVEL FILE\n# sector\nround(mean(sch.dat$sector),2) # this answers \"what percent of schools are catholic?\"\n\n[1] 0.44\n\nround(sd(sch.dat$sector),2)\n\n[1] 0.5\n\n# mean ses\nround(mean(sch.dat$meanses),2) # this answers \"what is the average of the school-average SES values?\"\n\n[1] 0\n\nround(sd(sch.dat$meanses),2)\n\n[1] 0.41\n\n# NOTE: if we used the student-level or \"dat\" file, we would be answering the\n# following questions:\n# * what percent of students attend a catholic school?\n# * what is the average student ses? &lt;- this would match what we calculated\n# ourselves if we had the entire school in our sample",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#table-4.2-one-way-anova-i.e-uncontrolled-random-intercept",
    "href": "hsb_ex.html#table-4.2-one-way-anova-i.e-uncontrolled-random-intercept",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.4 Table 4.2: One-Way ANOVA (i.e uncontrolled random intercept)",
    "text": "39.4 Table 4.2: One-Way ANOVA (i.e uncontrolled random intercept)\n\n## Fit the model described \nmod4.2 &lt;- lmer(mathach ~ 1 + (1|id), data=dat)\n# Peek at the results\ndisplay(mod4.2)\n\nlmer(formula = mathach ~ 1 + (1 | id), data = dat)\ncoef.est  coef.se \n   12.64     0.24 \n\nError terms:\n Groups   Name        Std.Dev.\n id       (Intercept) 2.93    \n Residual             6.26    \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 47122.8, DIC = 47114.8\ndeviance = 47115.8 \n\n## Extract the fixed effect coefficient (and it's standard error)\nfixef(mod4.2) # extracts the fixed effect coefficient(s)\n\n(Intercept) \n   12.63697 \n\nse.coef(mod4.2)$fixef #extracts the standard errors for the fixed effect(s)\n\n[1] 0.2443936\n\n## Extract the variance components\n# Note: in the model display, we see the SDs, not the variance\nVarCorr(mod4.2)\n\n Groups   Name        Std.Dev.\n id       (Intercept) 2.9350  \n Residual             6.2569  \n\n# To get the variances, we extract each part and square it\n# variance of random intercept\n(sigma.hat(mod4.2)$sigma$id)^2\n\n(Intercept) \n   8.614025 \n\n# variance of level 1 residual (easier to extract)\nsigma(mod4.2)^2 \n\n[1] 39.14832\n\n# could also use the more complicated formula that we used with the intercept.\n# If we do, we get the same thing\nsigma.hat(mod4.2)$sigma$data^2\n\n[1] 39.14832\n\n# Inference on the need for a random intercept\n# Thus uses the book's way of calculating a test statistic with a\n# chi-squared distribution.\n\nschools = dat %&gt;% group_by( id ) %&gt;%\n  summarise( nj = n(),\n             Y.bar.j = mean( mathach ) )\ngamma.00 = fixef( mod4.2 )[[1]]\nsigma.2 = sigma(mod4.2)^2 \nH = sum( schools$nj * (schools$Y.bar.j - gamma.00)^2 / sigma.2 )\nH\n\n[1] 1660.232\n\n# our p-value\npchisq( H, df = nrow( schools ) - 1, lower.tail = FALSE )\n\n[1] 4.770612e-248\n\n# calculating the ICC\ntau.00 = VarCorr(mod4.2)$id[1,1]\nrho.hat = tau.00 / (tau.00 + sigma.2 )\nrho.hat\n\n[1] 0.1803518\n\n# Calculating reliability for each school mean. (Here it is purely a function of\n# students in the school.  More students, more info, and thus more reliable.)\nsigma.2 = sigma(mod4.2)^2 \ntau.00 = VarCorr(mod4.2)$id[1,1]\nlambda = tau.00 / ( tau.00 + sigma.2 / schools$nj )\nmean( lambda )\n\n[1] 0.9013773\n\n# A bonus graph of the reliabilities\nqplot( lambda )",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#table-4.3-means-as-outcomes-model",
    "href": "hsb_ex.html#table-4.3-means-as-outcomes-model",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.5 Table 4.3 Means as Outcomes Model",
    "text": "39.5 Table 4.3 Means as Outcomes Model\n\n# (i.e. random intercept with Level 2 predictor)\n## Fit the model described \nmod4.3 &lt;- lmer(mathach ~ 1 + meanses + (1|id), data=dat)\n\n# Peek at the results\ndisplay(mod4.3)\n\nlmer(formula = mathach ~ 1 + meanses + (1 | id), data = dat)\n            coef.est coef.se\n(Intercept) 12.65     0.15  \nmeanses      5.86     0.36  \n\nError terms:\n Groups   Name        Std.Dev.\n id       (Intercept) 1.62    \n Residual             6.26    \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46969.3, DIC = 46956.9\ndeviance = 46959.1 \n\n## Extract the fixed effect coefficients (and standard errors/t-statistics)\nfixef(mod4.3) # extracts the fixed effect coefficients\n\n(Intercept)     meanses \n  12.649435    5.863538 \n\n# NOTE: you can call them separately by \"indexing\" them\n# just the intercept\nfixef(mod4.3)[1]\n\n(Intercept) \n   12.64944 \n\n# just coefficient on mean ses\nfixef(mod4.3)[2]\n\n meanses \n5.863538 \n\nse.coef(mod4.3)$fixef #extracts the standard errors for the fixed effect(s)\n\n[1] 0.1492801 0.3614580\n\n## Calculate (or extract) the t-ratio (aka the t-statistic)\n\n# NOTE: the author's don't present this for the intercept, because we often\n# don't care. But it is presented here for completeness\n\n# tstats for intercept\nfixef(mod4.3)[1]/se.coef(mod4.3)$fixef[1]\n\n(Intercept) \n   84.73622 \n\n# tstat mean ses\nfixef(mod4.3)[2]/se.coef(mod4.3)$fixef[2]\n\n meanses \n16.22191 \n\n# tstat extracted - this does both variables at once! \ncoef(summary(mod4.3))[,\"t value\"]\n\n(Intercept)     meanses \n   84.73622    16.22191 \n\n# NOTE: Let's look at what is happening here:\ncoef(summary(mod4.3)) # gives us all the fixed effect statistics we could want\n\n             Estimate Std. Error       df  t value      Pr(&gt;|t|)\n(Intercept) 12.649435  0.1492801 153.7425 84.73622 6.032590e-131\nmeanses      5.863538  0.3614580 153.4067 16.22191  4.267894e-35\n\n# the [ ] is called \"indexing\" - it's a way of subsetting data by telling R\n# which [rows,columns] you want to see we are telling R that we want ALL rows \"[\n# ,\" but only the column labeled \"t value\"\n\n## Extract the variance components\n# Note: in the model display, we see the SDs, not the variance\nVarCorr(mod4.3)\n\n Groups   Name        Std.Dev.\n id       (Intercept) 1.6244  \n Residual             6.2576  \n\n# To get the variances, we extract each part and square it\n# variance of random intercept\n(sigma.hat(mod4.3)$sigma$id)^2\n\n(Intercept) \n   2.638708 \n\n# variance of level 1 residual\nsigma(mod4.3)^2 \n\n[1] 39.15708\n\n# Range of plausible values for school means for schools with mean SES of 0:\n# See page 73-74)\nfixef( mod4.3 )[[1]] + c(-1.96, 1.96) * (sigma.hat(mod4.3)$sigma$id)\n\n[1]  9.465592 15.833279\n\n# Compare to our model without mean ses\nfixef( mod4.2 )[[1]] + c(-1.96, 1.96) * (sigma.hat(mod4.2)$sigma$id)\n\n[1]  6.884441 18.389507\n\n# Proportion reduction in variance or \"variance explained\" at level 2\ntau.00.anova = (sigma.hat(mod4.2)$sigma$id)^2\ntau.00.meanses = (sigma.hat(mod4.3)$sigma$id)^2\n(tau.00.anova-tau.00.meanses) / tau.00.anova\n\n(Intercept) \n   0.693673 \n\n## Inference on the random effects\nschools = merge( schools, sch.dat, by=\"id\" )\ngamma.00 = fixef( mod4.3 )[[1]]\ngamma.01 = fixef( mod4.3 )[[2]]\nschools = mutate( schools, resid = Y.bar.j - gamma.00 - gamma.01*meanses )\nH = sum( schools$nj * schools$resid^2 ) / sigma(mod4.3)^2 \nH\n\n[1] 633.5175\n\npchisq( H, nrow( schools ) - 2, lower.tail = FALSE )\n\n[1] 3.617696e-58\n\n## Reliability revisited (from pg 75)\nmod4.3\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: mathach ~ 1 + meanses + (1 | id)\n   Data: dat\nREML criterion at convergence: 46961.28\nRandom effects:\n Groups   Name        Std.Dev.\n id       (Intercept) 1.624   \n Residual             6.258   \nNumber of obs: 7185, groups:  id, 160\nFixed Effects:\n(Intercept)      meanses  \n     12.649        5.864  \n\nu.hat = coef( mod4.3 )$id\nhead( u.hat )\n\n     (Intercept)  meanses\n1224    12.32688 5.863538\n1288    12.71898 5.863538\n1296    10.70101 5.863538\n1308    12.92208 5.863538\n1317    11.48086 5.863538\n1358    11.73878 5.863538\n\nsigma.2 = sigma(mod4.3)^2 \ntau.00 = VarCorr(mod4.3)$id[1,1]\nsigma.2\n\n[1] 39.15708\n\ntau.00\n\n[1] 2.638708\n\n# These are the individual reliabilities---how well we can separate schools with the same Mean SES\n# (So it is _conditional_ on the mean SES of the schools.)\nlambda.j = tau.00 / (tau.00 + (sigma.2 / schools$nj))\nmean( lambda.j )\n\n[1] 0.7400747",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#table-4.4-random-coefficient-model-i.e.-random-slope",
    "href": "hsb_ex.html#table-4.4-random-coefficient-model-i.e.-random-slope",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.6 Table 4.4 Random coefficient model (i.e. random slope)",
    "text": "39.6 Table 4.4 Random coefficient model (i.e. random slope)\n\n# group-mean center ses  \ndat &lt;- dat %&gt;% group_by( id ) %&gt;% \n  mutate( ses_grpcenter = ses - mean(ses) )\n\n## Fit the model described \nmod4.4 &lt;- lmer(mathach ~ 1 + ses_grpcenter + ( 1 + ses_grpcenter | id ), data=dat)\n# Peek at the results\ndisplay(mod4.4)\n\nlmer(formula = mathach ~ 1 + ses_grpcenter + (1 + ses_grpcenter | \n    id), data = dat)\n              coef.est coef.se\n(Intercept)   12.64     0.24  \nses_grpcenter  2.19     0.13  \n\nError terms:\n Groups   Name          Std.Dev. Corr \n id       (Intercept)   2.95          \n          ses_grpcenter 0.83     0.02 \n Residual               6.06          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46726.2, DIC = 46707.7\ndeviance = 46711.0 \n\n## Extract the fixed effect coefficients (and standard errors/t-statistics)\ncoef(summary(mod4.4)) #this reproduces the whole first panel, though methods used above also work\n\n               Estimate Std. Error       df  t value      Pr(&gt;|t|)\n(Intercept)   12.636193  0.2445047 156.7512 51.68077 2.286893e-100\nses_grpcenter  2.193196  0.1282589 155.2166 17.09976  1.582355e-37\n\n## Extract the variance components\n# Note: in the model display, we see the SDs, not the variance\nVarCorr(mod4.4) \n\n Groups   Name          Std.Dev. Corr \n id       (Intercept)   2.94636       \n          ses_grpcenter 0.83307  0.019\n Residual               6.05807       \n\n# variance of random effects\n(sigma.hat(mod4.4)$sigma$id)^2\n\n  (Intercept) ses_grpcenter \n    8.6810437     0.6939974 \n\n# NOTE: to extract one or the other, you can use indexing\n(sigma.hat(mod4.4)$sigma$id[1])^2 #this is just the intercept random effect\n\n(Intercept) \n   8.681044 \n\n# variance of level 1 residual\nsigma(mod4.4)^2\n\n[1] 36.70019",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#table-4.5-intercepts-and-slopes-as-outcomes-model",
    "href": "hsb_ex.html#table-4.5-intercepts-and-slopes-as-outcomes-model",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.7 Table 4.5 Intercepts and Slopes as Outcomes Model",
    "text": "39.7 Table 4.5 Intercepts and Slopes as Outcomes Model\n\n## Fit the model described \nmod4.5 &lt;- lmer(mathach ~ 1 + meanses + sector + ses_grpcenter*(meanses + sector) + ( 1 + ses_grpcenter | id ), data=dat)\n\n# NOTE: The code above allows the coefficients to appear in the same order as in Table 4.5\n\n# R automatically includes the main effects, so this model can be written more\n# concisely as shown below:\n#\n# lmer(mathach ~ 1 + ses_grpcenter*(meanses + sector) + ( 1 + ses_grpcenter | id ), data=dat)\n\n# Peek at the results\ndisplay(mod4.5)\n\nlmer(formula = mathach ~ 1 + meanses + sector + ses_grpcenter * \n    (meanses + sector) + (1 + ses_grpcenter | id), data = dat)\n                      coef.est coef.se\n(Intercept)           12.10     0.20  \nmeanses                5.33     0.37  \nsector                 1.23     0.31  \nses_grpcenter          2.94     0.16  \nmeanses:ses_grpcenter  1.04     0.30  \nsector:ses_grpcenter  -1.64     0.24  \n\nError terms:\n Groups   Name          Std.Dev. Corr \n id       (Intercept)   1.54          \n          ses_grpcenter 0.32     0.39 \n Residual               6.06          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46523.7, DIC = 46489.2\ndeviance = 46496.4 \n\n## Extract the fixed effect coefficients (and standard errors/t-statistics)\n#this reproduces the whole first panel, though methods used above also work\ncoef(summary(mod4.5))\n\n                       Estimate Std. Error       df   t value      Pr(&gt;|t|)\n(Intercept)           12.095997  0.1987329 159.9143 60.865590 1.625101e-112\nmeanses                5.332898  0.3691567 150.9836 14.446161  2.944282e-30\nsector                 1.226453  0.3062674 149.6139  4.004518  9.756638e-05\nses_grpcenter          2.938785  0.1550889 139.2934 18.949039  2.197507e-40\nmeanses:ses_grpcenter  1.038918  0.2988941 160.5428  3.475873  6.550388e-04\nsector:ses_grpcenter  -1.642619  0.2397854 143.3351 -6.850371  2.009493e-10\n\n# NOTE: there is a slight descrepancy in the estimate for meanses:ses_grpcenter and \n# the t-statistics for meanses:ses_grpcenter and sector:ses_grpcenter; nothing that \n# changes the interpretations, however.\n\n\n# Testing the need for sector  (see page 82)\n# (We use a likelihood ratio test with the anova() function)\nmod4.5.null &lt;- lmer(mathach ~ 1 + meanses + ses_grpcenter*(meanses) + ( 1 + ses_grpcenter | id ), data=dat)\nanova( mod4.5, mod4.5.null )\n\nData: dat\nModels:\nmod4.5.null: mathach ~ 1 + meanses + ses_grpcenter * (meanses) + (1 + ses_grpcenter | id)\nmod4.5: mathach ~ 1 + meanses + sector + ses_grpcenter * (meanses + sector) + (1 + ses_grpcenter | id)\n            npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmod4.5.null    8 46568 46623 -23276    46552                         \nmod4.5        10 46516 46585 -23248    46496 55.941  2  7.122e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Testing the need for random slope  (see page 84)\n# (We use a likelihood ratio test with the anova() function)\nmod4.5.null.slope &lt;- lmer(mathach ~ 1 + meanses + sector + ses_grpcenter*(meanses + sector) + ( 1 | id ), data=dat) \nanova( mod4.5, mod4.5.null.slope )\n\nData: dat\nModels:\nmod4.5.null.slope: mathach ~ 1 + meanses + sector + ses_grpcenter * (meanses + sector) + (1 | id)\nmod4.5: mathach ~ 1 + meanses + sector + ses_grpcenter * (meanses + sector) + (1 + ses_grpcenter | id)\n                  npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod4.5.null.slope    8 46513 46568 -23249    46497                     \nmod4.5              10 46516 46585 -23248    46496 1.0039  2     0.6054",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#figure-4.1",
    "href": "hsb_ex.html#figure-4.1",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.8 Figure 4.1",
    "text": "39.8 Figure 4.1\nNOTE: Figure 4.1 is a graphical display using the results from Model/Table 4.5\nThe solid line represents the slope of the gamma-01 coefficient; this is the same in public and catholic schools. The dotted lines represent the the slope for individual schools with “prototypical” values of meanses (-1,0,1 standard deviations from mean)\n\n# to calculate this, we should note a few values: \navg_meanses &lt;- mean(dat$meanses) #average of mean ses var\nhigh_meanses &lt;- mean(dat$meanses) + sd(dat$meanses) # 1 sd above avg meanses\nlow_meanses &lt;- mean(dat$meanses) - sd(dat$meanses) # 1 sd below avg meanses\n\nfake.students = expand.grid( id = -1,\n                             meanses = c( low_meanses, avg_meanses, high_meanses ),\n                             sector = c( 0, 1 ),\n                             ses_grpcenter = c( -1, 0, 1 ) )\nfake.students = mutate( fake.students, ses = meanses + ses_grpcenter )\nfake.students$mathach = predict( mod4.5, newdata=fake.students, allow.new.levels = TRUE )\nfake.schools = filter( fake.students, ses_grpcenter == 0 )\n\nggplot( fake.students, aes( ses, mathach ) ) + \n  facet_wrap( ~ sector ) +\n  geom_line( aes( group=meanses ), lty = 2 ) +\n  geom_line( data=fake.schools, aes( x = ses, y = mathach ) ) +\n  geom_point( data=fake.schools, aes( x = ses, y = mathach ) )",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#set-up-for-remaining-tablesfigures-of-chapter",
    "href": "hsb_ex.html#set-up-for-remaining-tablesfigures-of-chapter",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.9 Set-up for remaining tables/figures of chapter",
    "text": "39.9 Set-up for remaining tables/figures of chapter\nIn order to create table 4.6 and the following 2 graphs, we will need to prepare a new dataset. These next lines of code do that.\n\n## Start with school level data frame and keep variables interesting to our model comparison\nmod.comp &lt;- dplyr::select( sch.dat, id, meanses, sector )\n\n## Add in number of observations per school \nn_j &lt;- dat %&gt;% group_by( id ) %&gt;%\n  dplyr::summarise(n_j = n())\n\nmod.comp &lt;- merge(mod.comp, n_j, by=\"id\")\nhead( mod.comp )\n\n    id meanses sector n_j\n1 1224  -0.428      0  47\n2 1288   0.128      0  25\n3 1296  -0.420      0  48\n4 1308   0.534      1  20\n5 1317   0.351      1  48\n6 1358  -0.014      0  30\n\n## Run site-specific OLS for each school and save estimates \n\n# Calculate global (not group) centered ses\ndat$ses_centered &lt;- dat$ses - mean(dat$ses)\n\n# This is the \"for loop\" method of generating an estimate for each of many small\n# worlds (schools). See lecture 2.3 code for the \"tidyverse\" way.\nest.ols &lt;- matrix(nrow=160,ncol=2) #create a matrix to store estimates \nse.ols &lt;- matrix(nrow=160,ncol=2) #create matrix to store standard errors\n\nfor (i in 1:length(unique(dat$id))){ #looping across the 160 different values of id\n    id &lt;- unique(dat$id)[i] #pick the value of id we want\n    mod &lt;- lm(mathach ~ 1 + ses_grpcenter, data=dat[dat$id==id,]) #run the model on students in that 1 school\n    est.ols[i,] &lt;- coef( mod ) #save the setimates in the matrix we created\n    se.ols[i,] &lt;- se.coef( mod ) # and the SEs\n}\n\n#convert the matrix to a dataframe and attach the schoolid info\nest.ols &lt;- as.data.frame(est.ols)\nest.ols$id &lt;- sch.dat$id\nnames(est.ols) &lt;- c( 'b0_ols', 'b1_ols', 'id' )\n\n#store standard errors for later\nse.ols &lt;- as.data.frame(se.ols)\nse.ols$id &lt;- sch.dat$id\nnames(se.ols) &lt;- c( 'se_b0_ols', 'se_b1_ols', 'id' )\n\nmod.comp &lt;- merge(mod.comp, est.ols, by='id')\nmod.comp &lt;- merge(mod.comp, se.ols, by='id' )\nhead( mod.comp )\n\n    id meanses sector n_j    b0_ols    b1_ols se_b0_ols se_b1_ols\n1 1224  -0.428      0  47  9.715447 2.5085817 1.0954478  1.765216\n2 1288   0.128      0  25 13.510800 3.2554487 1.3637656  2.079675\n3 1296  -0.420      0  48  7.635958 1.0759591 0.7740752  1.209016\n4 1308   0.534      1  20 16.255500 0.1260242 1.4045813  3.003437\n5 1317   0.351      1  48 13.177688 1.2739128 0.7902486  1.435942\n6 1358  -0.014      0  30 11.206233 5.0680087 0.8994345  1.391550\n\n# We are done running OLS on each of our schools and storing the results.\n\n## Extract site-specific coefficients from \"unconditional model\" (model 4.4)\nest4.4 &lt;- coef(mod4.4)$id\nnames(est4.4) &lt;- c('b0_uncond', 'b1_uncond') #rename\nest4.4$id = rownames( est4.4 )\n\n## Extract site-specific coefficients from the \"conditional model\" (model 4.5)\nest4.5 &lt;- coef(mod4.5)$id\nhead( est4.5 )\n\n     (Intercept)  meanses   sector ses_grpcenter meanses:ses_grpcenter\n1224    12.02263 5.332898 1.226453      2.933689              1.038918\n1288    12.55180 5.332898 1.226453      2.979174              1.038918\n1296    10.38509 5.332898 1.226453      2.744066              1.038918\n1308    12.12710 5.332898 1.226453      2.923822              1.038918\n1317    10.56530 5.332898 1.226453      2.806582              1.038918\n1358    11.60500 5.332898 1.226453      2.961265              1.038918\n     sector:ses_grpcenter\n1224            -1.642619\n1288            -1.642619\n1296            -1.642619\n1308            -1.642619\n1317            -1.642619\n1358            -1.642619\n\nest4.5$id = rownames( est4.5 )\n\n# Now we need to calculate the point estimates using our individual regression equations\n# including our level-2 values for each school\n# (This is a bit of a pain.)\nest4.5 = merge( est4.5, mod.comp, by=\"id\", suffixes = c( \"\", \".v\" ) )\nhead( est4.5 )\n\n    id (Intercept)  meanses   sector ses_grpcenter meanses:ses_grpcenter\n1 1224    12.02263 5.332898 1.226453      2.933689              1.038918\n2 1288    12.55180 5.332898 1.226453      2.979174              1.038918\n3 1296    10.38509 5.332898 1.226453      2.744066              1.038918\n4 1308    12.12710 5.332898 1.226453      2.923822              1.038918\n5 1317    10.56530 5.332898 1.226453      2.806582              1.038918\n6 1358    11.60500 5.332898 1.226453      2.961265              1.038918\n  sector:ses_grpcenter meanses.v sector.v n_j    b0_ols    b1_ols se_b0_ols\n1            -1.642619    -0.428        0  47  9.715447 2.5085817 1.0954478\n2            -1.642619     0.128        0  25 13.510800 3.2554487 1.3637656\n3            -1.642619    -0.420        0  48  7.635958 1.0759591 0.7740752\n4            -1.642619     0.534        1  20 16.255500 0.1260242 1.4045813\n5            -1.642619     0.351        1  48 13.177688 1.2739128 0.7902486\n6            -1.642619    -0.014        0  30 11.206233 5.0680087 0.8994345\n  se_b1_ols\n1  1.765216\n2  2.079675\n3  1.209016\n4  3.003437\n5  1.435942\n6  1.391550\n\nest4.5 = mutate( est4.5, \n                 b0_cond = `(Intercept)` + sector * sector.v + meanses * meanses.v,\n                 b1_cond = ses_grpcenter + `sector:ses_grpcenter` * sector.v + `meanses:ses_grpcenter` * meanses.v )\n\nest4.5 = dplyr::select( est4.5, id, b0_cond, b1_cond )\n\n\n## Combine the MLM estimates into 1 dataset with ids\nest.mlm &lt;- merge( est4.4, est4.5, by=\"id\" )\n\n# Merge all the estimates together by school id\nmod.comp &lt;- merge(mod.comp,est.mlm,by = 'id',all=TRUE)\n\nhead( mod.comp )\n\n    id meanses sector n_j    b0_ols    b1_ols se_b0_ols se_b1_ols b0_uncond\n1 1224  -0.428      0  47  9.715447 2.5085817 1.0954478  1.765216  9.956953\n2 1288   0.128      0  25 13.510800 3.2554487 1.3637656  2.079675 13.386036\n3 1296  -0.420      0  48  7.635958 1.0759591 0.7740752  1.209016  8.039091\n4 1308   0.534      1  20 16.255500 0.1260242 1.4045813  3.003437 15.622073\n5 1317   0.351      1  48 13.177688 1.2739128 0.7902486  1.435942 13.132771\n6 1358  -0.014      0  30 11.206233 5.0680087 0.8994345  1.391550 11.387452\n  b1_uncond   b0_cond  b1_cond\n1  2.262837  9.740146 2.489033\n2  2.375964 13.234409 3.112156\n3  1.872247  8.145275 2.307720\n4  2.050193 16.201317 1.835985\n5  1.997129 13.663596 1.528623\n6  2.738390 11.530341 2.946721",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#table-4.6-comparing-site-specific-estimates-from-different-models",
    "href": "hsb_ex.html#table-4.6-comparing-site-specific-estimates-from-different-models",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.10 Table 4.6 Comparing site-specific estimates from different models",
    "text": "39.10 Table 4.6 Comparing site-specific estimates from different models\n\n## Create the list of rows that B&R include in the table p. 87\nkeeprows &lt;- c(4, 15, 17, 22, 27, 53, 69, 75, 81, 90, 135, 153)\n\n## Limit data to the rows of interest, and print the columns in Table 4.6 in the correct order\ntab4.6 &lt;- mod.comp[keeprows, c('b0_ols','b1_ols','b0_uncond','b1_uncond','b0_cond','b1_cond','n_j','meanses','sector') ]\n\n\n## Print Table 4.6 -- the Empirical Bayes from conditional model (b0_cond, b1_cond) are waaaaaay off\nround(tab4.6,2)\n\n    b0_ols b1_ols b0_uncond b1_uncond b0_cond b1_cond n_j meanses sector\n4    16.26   0.13     15.62      2.05   16.20    1.84  20    0.53      1\n15   15.98   2.15     15.74      2.19   16.01    1.84  53    0.52      1\n17   18.11   0.09     17.41      1.95   17.25    3.71  29    0.69      0\n22   11.14  -0.78     11.22      1.15   10.89    0.63  67   -0.62      1\n27   13.40   4.10     13.32      2.54   12.95    3.00  38   -0.06      0\n53    9.52   3.74      9.76      2.75    9.37    2.42  51   -0.64      0\n69   11.47   6.18     11.64      2.72   11.92    3.03  25    0.08      0\n75    9.06   1.65      9.28      2.01    9.30    0.67  63   -0.59      1\n81   15.42   5.26     15.25      3.14   15.53    1.91  66    0.43      1\n90   12.14   1.97     12.18      2.14   12.34    3.03  50    0.19      0\n135   4.55   0.25      6.42      1.92    8.55    2.63  14    0.03      0\n153  10.28   0.76     10.71      2.06    9.67    2.37  19   -0.59      0",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#figure-4.2-scatter-plots-of-the-estimates-from-2-unconstrained-models",
    "href": "hsb_ex.html#figure-4.2-scatter-plots-of-the-estimates-from-2-unconstrained-models",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.11 Figure 4.2 : Scatter plots of the estimates from 2 unconstrained models",
    "text": "39.11 Figure 4.2 : Scatter plots of the estimates from 2 unconstrained models\n\n## Panel (a) and Panel (b) are plotted on the same graph \nggplot(data=mod.comp,aes()) + \n  geom_point(aes(x=b1_ols,y=b0_ols),color='black',alpha=0.7) + \n  geom_point(aes(x=b1_uncond,y=b0_uncond),color='blue',alpha=0.7) + \n  labs(title=\"Black=OLS; Blue=Unconditional EB\") +\n  xlim(-5,8) + ylim(2,20)",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#figure-4.3-scatter-plots-of-residuals-from-the-ols-constrained-mlm-model",
    "href": "hsb_ex.html#figure-4.3-scatter-plots-of-residuals-from-the-ols-constrained-mlm-model",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.12 Figure 4.3 : Scatter plots of residuals from the OLS & Constrained MLM model",
    "text": "39.12 Figure 4.3 : Scatter plots of residuals from the OLS & Constrained MLM model\n\n## Luke: Equation 4.271 and 4.27b (p. 92) are allegedly how we calculate the intercept and slope residuals \n## But I'm not sure where the estimates for the gamma-hat terms come from; the OLS model only includes\n## individual-level ses\n\n# trying it here with the predictions from conditional EB\nfes = fixef( mod4.5 )\nfes\n\n          (Intercept)               meanses                sector \n            12.095997              5.332898              1.226453 \n        ses_grpcenter meanses:ses_grpcenter  sector:ses_grpcenter \n             2.938785              1.038918             -1.642619 \n\nmod.comp = mutate( mod.comp,\n                   u0_ols = b0_ols - (fes[1] + fes[2]*meanses + fes[3]*sector),\n                   u1_ols = b1_ols - (fes[4] + fes[5]*meanses + fes[6]*sector)  )\n\n\n## Panel (a) and (b) plotted on same graph\n\nmod.comp = mutate( mod.comp, \n                   u0_cond = b0_cond - (fes[1] + fes[2]*meanses + fes[3]*sector),\n                   u1_cond = b1_cond - (fes[4] + fes[5]*meanses + fes[6]*sector)  )\n\nhead( mod.comp )\n\n    id meanses sector n_j    b0_ols    b1_ols se_b0_ols se_b1_ols b0_uncond\n1 1224  -0.428      0  47  9.715447 2.5085817 1.0954478  1.765216  9.956953\n2 1288   0.128      0  25 13.510800 3.2554487 1.3637656  2.079675 13.386036\n3 1296  -0.420      0  48  7.635958 1.0759591 0.7740752  1.209016  8.039091\n4 1308   0.534      1  20 16.255500 0.1260242 1.4045813  3.003437 15.622073\n5 1317   0.351      1  48 13.177688 1.2739128 0.7902486  1.435942 13.132771\n6 1358  -0.014      0  30 11.206233 5.0680087 0.8994345  1.391550 11.387452\n  b1_uncond   b0_cond  b1_cond      u0_ols      u1_ols     u0_cond      u1_cond\n1  2.262837  9.740146 2.489033 -0.09807014  0.01445354 -0.07337107 -0.005095579\n2  2.375964 13.234409 3.112156  0.73219201  0.18368221  0.45580075  0.040389349\n3  1.872247  8.145275 2.307720 -2.22022179 -1.42648036 -1.71090544 -0.194719195\n4  2.050193 16.201317 1.835985  0.08528223 -1.72492410  0.03109939 -0.014963432\n5  1.997129 13.663596 1.528623 -2.01661001 -0.38691354 -1.53070178 -0.132203129\n6  2.738390 11.530341 2.946721 -0.81510320  2.14376861 -0.49099553  0.022480378\n\nnrow( mod.comp )\n\n[1] 160\n\nggplot(data=mod.comp, aes( pch=as.factor(sector)) ) + \n         geom_point(aes(x=u1_ols, y=u0_ols),color='black', alpha=0.7) +   \n         geom_point(aes(x=u1_cond, y=u0_cond),color='blue', alpha=0.7) + \n         labs(title = \"Black: OLS, Blue: Conditional EB\") + \n         xlim(-6,6) + ylim(-8,8)\n\n\n\n\n\n\n\n# To get in two-panel format we need to get our data to long format\nmod.comp.ols = data.frame( sector = mod.comp$sector,\n                           u0 = mod.comp$u0_ols,\n                           u1 = mod.comp$u1_ols )\nmod.comp.EB = data.frame(  sector = mod.comp$sector,\n                           u0 = mod.comp$u0_cond,\n                           u1 = mod.comp$u1_cond )\nmod.comp.l = bind_rows( ols=mod.comp.ols, cond = mod.comp.EB, .id = \"method\" )\n\nggplot(data=mod.comp.l, aes( u1, u0, pch=as.factor(sector)) ) + \n  facet_wrap( ~ method ) +\n  geom_point()",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "hsb_ex.html#table-4.7-pg-94",
    "href": "hsb_ex.html#table-4.7-pg-94",
    "title": "39  Code for HSB Example in Chapter 4 of R&B",
    "section": "39.13 Table 4.7 : pg 94",
    "text": "39.13 Table 4.7 : pg 94\n\n# This section is not very good--I would skip.\n# Generating confidence intervals for individual random intercepts and slopes is a weird business.\n\n# OLS First:\n\n# Doing it by fitting OLS on our subset\nsch.2305 = filter( dat, id == 2305 )\nhead( sch.2305 )\n\n# A tibble: 6 × 13\n# Groups:   id [1]\n  id    minority female    ses mathach  size sector pracad disclim himinty\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 2305         1      1 -0.738   16.4    485      1   0.69   -1.38       1\n2 2305         1      1 -1.18    12.8    485      1   0.69   -1.38       1\n3 2305         1      1 -0.308   15.3    485      1   0.69   -1.38       1\n4 2305         1      1 -0.358   12.7    485      1   0.69   -1.38       1\n5 2305         1      1 -1.52    10.2    485      1   0.69   -1.38       1\n6 2305         1      1 -0.518    8.94   485      1   0.69   -1.38       1\n# ℹ 3 more variables: meanses &lt;dbl&gt;, ses_grpcenter &lt;dbl&gt;, ses_centered &lt;dbl&gt;\n\nM.2305 = lm( mathach ~ ses_grpcenter, data=sch.2305 )\nM.2305\n\n\nCall:\nlm(formula = mathach ~ ses_grpcenter, data = sch.2305)\n\nCoefficients:\n  (Intercept)  ses_grpcenter  \n      11.1378        -0.7821  \n\nconfint( M.2305 )\n\n                  2.5 %    97.5 %\n(Intercept)    9.911824 12.363698\nses_grpcenter -2.665989  1.101767\n\nsch.8367 = filter( dat, id == 8367 )\nhead( sch.8367 )\n\n# A tibble: 6 × 13\n# Groups:   id [1]\n  id    minority female    ses mathach  size sector pracad disclim himinty\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 8367         0      0 -0.228  15.9     153      0      0    1.79       0\n2 8367         0      1 -0.208   1.86    153      0      0    1.79       0\n3 8367         1      0  0.532  -2.83    153      0      0    1.79       0\n4 8367         0      1  0.662   2.12    153      0      0    1.79       0\n5 8367         0      0 -0.228   6.76    153      0      0    1.79       0\n6 8367         0      0 -1.08    0.725   153      0      0    1.79       0\n# ℹ 3 more variables: meanses &lt;dbl&gt;, ses_grpcenter &lt;dbl&gt;, ses_centered &lt;dbl&gt;\n\nM.8367 = lm( mathach ~ ses_grpcenter, data=sch.8367 )\nM.8367\n\n\nCall:\nlm(formula = mathach ~ ses_grpcenter, data = sch.8367)\n\nCoefficients:\n  (Intercept)  ses_grpcenter  \n       4.5528         0.2504  \n\nconfint( M.8367 )\n\n                  2.5 %   97.5 %\n(Intercept)    1.872974 7.232598\nses_grpcenter -3.431096 3.931845\n\n# Use SE from earlier to get confint\nest4.7 &lt;- mod.comp[c(22,135),]\nest4.7\n\n      id meanses sector n_j    b0_ols     b1_ols se_b0_ols se_b1_ols b0_uncond\n22  2305  -0.622      1  67 11.137761 -0.7821112 0.6138468  0.943289 11.222551\n135 8367   0.032      0  14  4.552786  0.2503748 1.2299413  1.689668  6.423938\n    b1_uncond   b0_cond   b1_cond    u0_ols    u1_ols    u0_cond     u1_cond\n22   1.149555 10.886600 0.6276417  1.132373 -1.432070  0.8812116 -0.02231763\n135  1.924903  8.549007 2.6307047 -7.713864 -2.721656 -3.7176433 -0.34132569\n\n# CI for intercept and slope using our normal and stored SEs.\n# (Not taking t distribution into account changes things, as does not\n# taking the uncertainty in the fixed effects for the EB CIs.  So this is\n# very approximate.)\nse_uncond = as.data.frame( se.coef(mod4.4)$id )\nhead( se_uncond )\n\n     (Intercept) ses_grpcenter\n1224   0.8464092     0.7189592\n1288   1.1205609     0.7593421\n1296   0.8382669     0.7111100\n1308   1.2307722     0.8005012\n1317   0.8382675     0.7377054\n1358   1.0354852     0.7489241\n\nnames( se_uncond ) = c(\"se_b0_uncond\",\"se_b1_uncond\" )\nse_cond = as.data.frame( se.coef(  mod4.5 )$id )\nnames( se_cond ) = c(\"se_b0_cond\",\"se_b1_cond\" )\nhead( se_cond )\n\n     se_b0_cond se_b1_cond\n1224  0.7662313  0.2929654\n1288  0.9521965  0.2988437\n1296  0.7601221  0.2923332\n1308  1.0176181  0.3025414\n1317  0.7603073  0.2940970\n1358  0.8982481  0.2971694\n\nse_uncond$id = rownames( se_uncond )\nse_cond$id = rownames( se_cond )\nest4.7 = merge( est4.7, se_uncond, by=\"id\" )\nest4.7 = merge( est4.7, se_cond, by=\"id\" )\n\nest4.7.int = mutate( est4.7, \n                 CI.low.ols = b0_ols + - 1.96 * se_b0_ols,\n                 CI.high.ols = b0_ols + 1.96 * se_b0_ols,\n                 CI.low.uncond = b0_uncond + - 1.96 * se_b0_uncond,\n                 CI.high.uncond = b0_uncond + 1.96 * se_b0_uncond,\n                 CI.low.cond = b0_cond + - 1.96 * se_b0_cond,\n                 CI.high.cond = b0_cond + 1.96 * se_b0_cond )\n\ndplyr::select( est4.7.int, starts_with(\"CI\" ) )\n\n  CI.low.ols CI.high.ols CI.low.uncond CI.high.uncond CI.low.cond CI.high.cond\n1   9.934621   12.340901      9.815648      12.629455    9.579800     12.19340\n2   2.142101    6.963471      3.642797       9.205078    6.361492     10.73652\n\nest4.7.slope = mutate( est4.7, \n                     CI.low.ols = b1_ols + - 1.96 * se_b1_ols,\n                     CI.high.ols = b1_ols + 1.96 * se_b1_ols,\n                     CI.low.uncond = b1_uncond + - 1.96 * se_b1_uncond,\n                     CI.high.uncond = b1_uncond + 1.96 * se_b1_uncond,\n                     CI.low.cond = b1_cond + - 1.96 * se_b1_cond,\n                     CI.high.cond = b1_cond + 1.96 * se_b1_cond )\n\ndplyr::select( est4.7.slope, starts_with(\"CI\" ) )\n\n  CI.low.ols CI.high.ols CI.low.uncond CI.high.uncond CI.low.cond CI.high.cond\n1  -2.630958    1.066735    -0.1675367       2.466647   0.0629627     1.192321\n2  -3.061375    3.562124     0.3960110       3.453795   2.0356645     3.225745",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Code for HSB Example in Chapter 4 of R&B</span>"
    ]
  },
  {
    "objectID": "faraway_ex.html",
    "href": "faraway_ex.html",
    "title": "40  Code for Faraway Example",
    "section": "",
    "text": "40.1 R Setup\nlibrary( arm )\nlibrary( ggplot2 )\nlibrary( plyr )\n\n# Install package from textbook to get the data by \n# running this line once.\n#install.packages( \"faraway\" )",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Code for Faraway Example</span>"
    ]
  },
  {
    "objectID": "faraway_ex.html#first-example",
    "href": "faraway_ex.html#first-example",
    "title": "40  Code for Faraway Example",
    "section": "40.2 First Example",
    "text": "40.2 First Example\n\n# load the data\nlibrary(faraway)\ndata(psid)\nhead(psid)\n\n  age educ sex income year person\n1  31   12   M   6000   68      1\n2  31   12   M   5300   69      1\n3  31   12   M   5200   70      1\n4  31   12   M   6900   71      1\n5  31   12   M   7500   72      1\n6  31   12   M   8000   73      1\n\n# Make log-transform of income\npsid$log_income = with( psid, log( income + 100 ) )\n                            \n                            \n# Look at some plots\npsid.sub = subset( psid, person &lt; 21 )\nggplot( data=psid.sub, aes( x=year, y=income ) ) +\n    facet_wrap( ~ person ) +\n    geom_line()\n\n\n\n\n\n\n\nggplot( data=psid.sub, aes( x=year, y=log_income, group=person ) ) +\n    facet_wrap( ~ sex ) +\n    geom_line()\n\n\n\n\n\n\n\n# Simple regression on a single person\nlmod &lt;- lm( log_income ~ I(year-78), subset=(person==1), psid)\ncoef(lmod)\n\n (Intercept) I(year - 78) \n  9.40910950   0.08342068 \n\n# Now do linear regression on everyone\nsum.stat = ddply( psid, .(person), function( dat ) {\n    lmod &lt;- lm(log(income) ~ I(year-78), data=dat )\n    cc = coef(lmod)\n    names(cc) = c(\"intercept\",\"slope\")\n    c( cc, sex=dat$sex[[1]] )\n} )\nhead( sum.stat )\n\n  person intercept       slope sex\n1      1  9.399957  0.08426670   2\n2      2  9.819091  0.08281031   2\n3      3  7.893863  0.03131149   1\n4      4  7.853027  0.07585135   1\n5      5  8.033453 -0.04738677   1\n6      6  9.673443  0.08953380   2\n\nplot( slope ~ intercept, data=sum.stat, xlab=\"Intercept\",ylab=\"Slope\")\n\n\n\n\n\n\n\nboxplot( slope ~ sex, data=sum.stat )\n\n\n\n\n\n\n\n# Is rate of income growth different by sex?\nt.test( slope ~ sex, data=sum.stat )\n\n\n    Welch Two Sample t-test\n\ndata:  slope by sex\nt = 2.3786, df = 56.736, p-value = 0.02077\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n 0.00507729 0.05916871\nsample estimates:\nmean in group 1 mean in group 2 \n     0.08903346      0.05691046 \n\n# Is initial income different by sex?\nt.test( intercept ~ sex, data=sum.stat )\n\n\n    Welch Two Sample t-test\n\ndata:  intercept by sex\nt = -8.2199, df = 79.719, p-value = 3.065e-12\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.4322218 -0.8738792\nsample estimates:\nmean in group 1 mean in group 2 \n       8.229275        9.382325",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Code for Faraway Example</span>"
    ]
  },
  {
    "objectID": "faraway_ex.html#fitting-the-model",
    "href": "faraway_ex.html#fitting-the-model",
    "title": "40  Code for Faraway Example",
    "section": "40.3 Fitting the model",
    "text": "40.3 Fitting the model\n\n# Fitting our model\nlibrary(lme4)\npsid$cyear &lt;- psid$year-78\nmmod &lt;- lmer(log(income) ~ cyear*sex + age + educ + (cyear|person), psid)\ndisplay(mmod)\n\nlmer(formula = log(income) ~ cyear * sex + age + educ + (cyear | \n    person), data = psid)\n            coef.est coef.se\n(Intercept)  6.67     0.54  \ncyear        0.09     0.01  \nsexM         1.15     0.12  \nage          0.01     0.01  \neduc         0.10     0.02  \ncyear:sexM  -0.03     0.01  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n person   (Intercept) 0.53          \n          cyear       0.05     0.19 \n Residual             0.68          \n---\nnumber of obs: 1661, groups: person, 85\nAIC = 3839.8, DIC = 3751.2\ndeviance = 3785.5 \n\n# refit with the lmerTest library to get p-values\nlibrary( lmerTest )\nmmod &lt;- lmer(log(income) ~ cyear*sex + age + educ + (cyear|person), psid)\nsummary(mmod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: log(income) ~ cyear * sex + age + educ + (cyear | person)\n   Data: psid\n\nREML criterion at convergence: 3819.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-10.2310  -0.2134   0.0795   0.4147   2.8254 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n person   (Intercept) 0.2817   0.53071      \n          cyear       0.0024   0.04899  0.19\n Residual             0.4673   0.68357      \nNumber of obs: 1661, groups:  person, 85\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  6.674211   0.543323 81.176972  12.284  &lt; 2e-16 ***\ncyear        0.085312   0.008999 78.915123   9.480 1.14e-14 ***\nsexM         1.150312   0.121292 81.772542   9.484 8.06e-15 ***\nage          0.010932   0.013524 80.837434   0.808   0.4213    \neduc         0.104209   0.021437 80.722319   4.861 5.65e-06 ***\ncyear:sexM  -0.026306   0.012238 77.995359  -2.150   0.0347 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) cyear  sexM   age    educ  \ncyear       0.020                            \nsexM       -0.104 -0.098                     \nage        -0.874  0.002 -0.026              \neduc       -0.597  0.000  0.008  0.167       \ncyear:sexM -0.003 -0.735  0.156 -0.010 -0.011",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Code for Faraway Example</span>"
    ]
  },
  {
    "objectID": "faraway_ex.html#model-diagnostics",
    "href": "faraway_ex.html#model-diagnostics",
    "title": "40  Code for Faraway Example",
    "section": "40.4 Model Diagnostics",
    "text": "40.4 Model Diagnostics\n\n# First add our residuals and fitted values to our original data\n# (We can do this since we have no missing data so the rows will line up\n# correctly)\npsid = transform( psid,  resid=resid( mmod ),\n                  fit = fitted( mmod ) )\nhead( psid )\n\n  age educ sex income year person log_income cyear       resid      fit\n1  31   12   M   6000   68      1   8.716044   -10  0.06719915 8.632316\n2  31   12   M   5300   69      1   8.594154    -9 -0.13201639 8.707478\n3  31   12   M   5200   70      1   8.575462    -8 -0.22622748 8.782641\n4  31   12   M   6900   71      1   8.853665    -7 -0.01852759 8.857804\n5  31   12   M   7500   72      1   8.935904    -6 -0.01030887 8.932967\n6  31   12   M   8000   73      1   8.999619    -5 -0.02093325 9.008130\n\n# Here is a qqplot for each sex\nggplot( data=psid ) +\n    facet_wrap( ~ sex ) +\n    stat_qq( aes( sample=resid ) )\n\n\n\n\n\n\n\n# If you want to add the lines, you have to do a little more work\nslopes = ddply( psid, .(sex), function( dat ) {\n    y &lt;- quantile(dat$resid, c(0.25, 0.75))\n    x &lt;- qnorm(c(0.25, 0.75))\n    slope &lt;- as.numeric( diff(y)/diff(x) )\n    int &lt;- y[[1]] - slope * x[[1]]\n    c( slope=slope, int=int )\n} )\nslopes\n\n  sex     slope        int\n1   F 0.4324568 0.10579138\n2   M 0.2473357 0.03321435\n\nggplot( data=psid ) +\n    facet_wrap( ~ sex ) +\n    stat_qq( aes( sample=resid ) ) +\n    geom_abline( data=slopes, aes( slope=slope, intercept=int ) )\n\n\n\n\n\n\n\n\nAnd a residual plot\n\npsid$educ_levels = cut(psid$educ, c(0,8.5,12.5,20), labels=c( \"Less than HS\", \"HS\", \"Beyond HS\" ) )\nggplot( data=psid, aes( x=fit, y=resid ) ) +\n    facet_wrap( ~ educ_levels ) +\n    geom_point()\n\n\n\n\n\n\n\n\n\n40.4.1 Lattice code\nFor reference, we can also do this:\n\n# This is doing it from the lattice package\nlibrary( lattice )\nqqmath(~resid(mmod) | sex, psid)\n\n\n\n\n\n\n\n# fancier with some lines.  The points should lie on the line\n# if we have normal residuals.  (We don't.)\nqqmath(~ resid(mmod)  | sex, data = psid,\n       panel = function(x, ...) {\n           panel.qqmathline(x, ...)\n           panel.qqmath(x, ...)\n       })",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Code for Faraway Example</span>"
    ]
  },
  {
    "objectID": "perf_ex.html",
    "href": "perf_ex.html",
    "title": "41  Example of a three-level model of clustered data",
    "section": "",
    "text": "41.1 Load the data\nWe first load the data. In the following we load the data and look at the first few lines. We see that each subject had two measurements from the standard and from the mini Wright flow meter.\npefr = read.dta( \"data/pefr.dta\" )\n\nhead( pefr )\n\n  id wp1 wp2 wm1 wm2\n1  1 494 490 512 525\n2  2 395 397 430 415\n3  3 516 512 520 508\n4  4 434 401 428 444\n5  5 476 470 500 500\n6  6 557 611 600 625",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Example of a three-level model of clustered data</span>"
    ]
  },
  {
    "objectID": "perf_ex.html#reshape-the-data-optional-section",
    "href": "perf_ex.html#reshape-the-data-optional-section",
    "title": "41  Example of a three-level model of clustered data",
    "section": "41.2 Reshape the data (Optional section)",
    "text": "41.2 Reshape the data (Optional section)\nThis section illustrates some advanced reshaping techniques. In particular we reshape the data twice to deal with the time and the device as different levels.\nHere we go:\n\ndat &lt;- pefr %&gt;%\n  pivot_longer(cols = c(wp1, wm1, wp2, wm2), \n               names_to = c( \"device_time\" ),\n               values_to = \"flow\" ) %&gt;%\n  separate_wider_position( device_time, \n                           widths = c( device = 2, time = 1 ) )\n\ndat\n\n# A tibble: 68 × 4\n      id device time   flow\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1     1 wp     1       494\n 2     1 wm     1       512\n 3     1 wp     2       490\n 4     1 wm     2       525\n 5     2 wp     1       395\n 6     2 wm     1       430\n 7     2 wp     2       397\n 8     2 wm     2       415\n 9     3 wp     1       516\n10     3 wm     1       520\n# ℹ 58 more rows\n\n\nLet’s see what we got:\n\nhead( dat )\n\n# A tibble: 6 × 4\n     id device time   flow\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1     1 wp     1       494\n2     1 wm     1       512\n3     1 wp     2       490\n4     1 wm     2       525\n5     2 wp     1       395\n6     2 wm     1       430\n\nsubset( pefr, id==1 )\n\n  id wp1 wp2 wm1 wm2\n1  1 494 490 512 525\n\n\nWe see the measurements correspond to the first row of the original pefr data.\nLet’s also check our second person to see if the measurements have the appropriate labels. They do.\n\nsubset( dat, id==2 )\n\n# A tibble: 4 × 4\n     id device time   flow\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1     2 wp     1       395\n2     2 wm     1       430\n3     2 wp     2       397\n4     2 wm     2       415\n\nsubset( pefr, id==2)\n\n  id wp1 wp2 wm1 wm2\n2  2 395 397 430 415\n\n\nAnother sanity check:\n\ntable( dat$id )\n\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 \n 4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4 \n\n\nWe have four measurements, still, for each person.\nWhen reshaping data, one typically has to fiddle with all of the commands and check the results a few times to get it right. ChatGPT or similar is really good at helping with this.",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Example of a three-level model of clustered data</span>"
    ]
  },
  {
    "objectID": "perf_ex.html#plot-the-data",
    "href": "perf_ex.html#plot-the-data",
    "title": "41  Example of a three-level model of clustered data",
    "section": "41.3 Plot the data",
    "text": "41.3 Plot the data\nWe can look at the data. The following illustrates getting different colors and symbols depending on covariate information:\n\ndat$id = as.factor( dat$id )\ndat$device = as.factor( dat$device )\ndat$time = as.factor( dat$time )\nggplot( data=dat, aes( x=id, y=flow, col=device, pch=time ) ) + \n    geom_jitter( width=0.2, height=0, size = 2 ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe see lots of subject variation. It is unclear if one device is systematically higher or lower than the other, but it does look like the devices are often more similar to each other, indicating individual-level device bias.",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Example of a three-level model of clustered data</span>"
    ]
  },
  {
    "objectID": "perf_ex.html#the-mathematical-model",
    "href": "perf_ex.html#the-mathematical-model",
    "title": "41  Example of a three-level model of clustered data",
    "section": "41.4 The mathematical model",
    "text": "41.4 The mathematical model\nLevel 1: We have, for individual \\(i\\) using machine \\(j\\) at time \\(t\\):\n\\[ Y_{tji} = \\beta_{0ji} + \\beta_{1} t + \\epsilon_{tji} .\\] Note the using the subscript of time as a covariate. Some might prefer \\(time_{tji} = 1\\) and \\(time_{tji}=2\\) and then writing \\(\\beta_1 time_{tji}\\).\nThe \\(\\beta_{1}\\) allows for a time effect of the second measurement being systematically lower or higher than the first. We pool this across all subjects and machines.\nLevel 2: Our machine-level intercepts for each subject are\n\\[ \\beta_{0ji} = \\gamma_{0i} + \\gamma_1 D_j + u_{ji} \\]\nwith \\(D_j = 1\\{ j = wp \\}\\) being an indicator (dummy variable) for the second machine. The \\(\\gamma_1\\) allows a systematic bias for the two machines (so the wp machine could tend to give larger readings than the wm machine, for example). Overall, the above says each machine expected reading varies around the subject’s lung capacity, but that these expected readings will vary around the subjects true capacity by the \\(u_{ji}\\). Actual readings for subject \\(i\\) on machine \\(j\\) will hover around \\(\\beta_{ji}\\) if we had the subject test over and over, according to our model (not including fatigue captured by the time coefficient).\nLevel 3: Finally our subject intercepts are \\[ \\gamma_{0i} = \\mu + w_{i} . \\] The overall population lung capacity is \\(\\mu\\). Subjects have larger or smaller lung capacity depending on their \\(w_{i}\\). This is the subject-to-subject variability.\nThe \\(u_{ji}\\) and \\(w_i\\) are each normally distributed, and independent from each other. The \\(w_i\\) are how the subjects vary (i.e., their different lung capacities). The \\(u_{ji}\\) are the individual biases of a machine for a given subject. Looking at our plot, we see that subjects vary a lot, and machines vary sometimes within a subject (the centers of the pairs of colored points tend to be close, but not always), and the residual variance tends to be small (colored points are close together). We should see this in our model output. Let’s find out!",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Example of a three-level model of clustered data</span>"
    ]
  },
  {
    "objectID": "perf_ex.html#fit-the-model",
    "href": "perf_ex.html#fit-the-model",
    "title": "41  Example of a three-level model of clustered data",
    "section": "41.5 Fit the model",
    "text": "41.5 Fit the model\nWe have a classic three-level model with time and device as covariates:\n\nlibrary( lme4 )\nM1 = lmer( flow ~ 1 + device + time + (1|id) + (1|device:id), data=dat )\ndisplay( M1 )\n\nlmer(formula = flow ~ 1 + device + time + (1 | id) + (1 | device:id), \n    data = dat)\n            coef.est coef.se\n(Intercept) 454.43    27.84 \ndevicewp     -6.03     8.05 \ntime2        -1.03     4.37 \n\nError terms:\n Groups    Name        Std.Dev.\n device:id (Intercept)  19.72  \n id        (Intercept) 111.99  \n Residual               18.01  \n---\nnumber of obs: 68, groups: device:id, 34; id, 17\nAIC = 682.8, DIC = 709.1\ndeviance = 689.9 \n\n\nWe interact device and id to generate unique ids for all the device groups nested within subject.\nNow let’s connect some pieces:\n\nThe main effects estimate \\(\\mu = 455.46\\) (average measured lung capacity) and \\(\\gamma_1 = -6.03\\) (the wp device’s bias vs. the wm device) and \\(\\beta_1 = -1.03\\) (reduction in lung capacity in second measurement occasion).\n\nThe z-score of \\(z = -6.03 / 8.05 &lt; 1\\) means there is no evidence of systematic bias of one machine compared to the other.\nThe estimated standard deviation of actual lung capacity is 112. Some people have much larger capacity than other people.\nThe estimated standard deviation of how two different machines will measure the same person is \\(19.72\\). Different machines will tend to systematically give different average measurements for the same subject. I.e., some subjects will look good on a wm machine, and some on a wp machine.\nThe estimated standard deviation of how much a repeated measurement of the same machine on the same person will vary is 18. The machines are relatively precise, given the variation in the population.\nThe amount of variance explained by lung variation is \\(112^2 / (19.72^2 + 111.99^2 + 18.01^2) = 0.94636\\), i.e., most of it.",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Example of a three-level model of clustered data</span>"
    ]
  },
  {
    "objectID": "kenya_ex.html",
    "href": "kenya_ex.html",
    "title": "42  Example of a three-level longitudinal model",
    "section": "",
    "text": "42.1 Load the data\nIn the following we load the data and look at the first few lines. Lots of variables! The main ones are id (the identifier of the kid), treatment (the kind of treatment given to the school), schoolid (the identifier of the school), gender (the gender of the kid), and rn (the time variable). Our outcome is ravens (Raven’s colored progressive matrices assessment).\nkenya = read.dta( \"data/kenya.dta\" )\n\n# look at first 9 variables\nhead( kenya[1:9], 3 )\n\n  id schoolid rn relyear ravens arithmetic vmeaning dstotal age_at_time0\n1  1        2  1   -0.15     15          5       25       6         7.19\n2  1        2  2    0.14     19          7       39       8         7.19\n3  1        2  3    0.46     21          7       33       7         7.19\n\n# what times do we have?\ntable( kenya$rn ) #time\n\n\n  1   2   3   4   5 \n546 546 546 546 546 \n\nlength( unique( kenya$id ) )\n\n[1] 546\n\nlength( unique( kenya$schoolid) )\n\n[1] 12\nWe see we have 546 kids and 12 schools.",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Example of a three-level longitudinal model</span>"
    ]
  },
  {
    "objectID": "kenya_ex.html#plot-and-prep-the-data",
    "href": "kenya_ex.html#plot-and-prep-the-data",
    "title": "42  Example of a three-level longitudinal model",
    "section": "42.2 Plot and prep the data",
    "text": "42.2 Plot and prep the data\nWe can look at the data.\n\nggplot( data=kenya, aes( x=rn, y=ravens, group=id )  )+ \n            facet_wrap( ~ gender ) + \n            geom_line( alpha=0.3 )\n\nWarning: Removed 114 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nor we can wrap by school to clean up our plot:\n\nggplot( data=kenya, aes( x=rn, y=ravens, group=id )  )+ \n            facet_wrap( ~ schoolid ) + \n            geom_line( alpha=0.3 )\n\nWarning: Removed 114 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nOr we can look at a sample of 12 individual children:\n\nid.sub = sample( unique( kenya$id), 12 )\nken.sub = subset( kenya, id %in% id.sub )\nggplot( data=ken.sub, aes( x=rn, y=ravens, group=id )  )+ \n            facet_wrap( ~ id ) + \n            geom_line( alpha=0.3 )\n\n\n\n\n\n\n\n\nWe have lots of noise! But there is also a trend.\nUsing the mosaic package we can easily calculate the progression of marginal means, and again see there is growth over time, on average:\n\nmosaic::favstats( ravens ~ rn, data=kenya )\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n  rn min Q1 median Q3 max mean   sd   n missing\n1  1   1 16     17 19  28 17.3 2.56 537       9\n2  2   0 16     17 19  28 17.7 2.79 529      17\n3  3   0 16     18 20  30 18.3 3.04 523      23\n4  4   4 17     18 20  30 18.6 2.97 513      33\n5  5   7 17     19 21  31 19.5 3.10 496      50\n\n\nThe above also shows that we have some missing data, more as the study progresses.\nWe drop these missing observations:\n\nkenya = subset( kenya, !is.na( ravens ) & !is.na( rn ) )\n\nWe have some treatments, which we order so control is first\n\nstr( kenya$treatment )\n\n Factor w/ 4 levels \"meat\",\"milk\",..: 1 1 1 1 1 4 4 4 4 4 ...\n\nlevels( kenya$treatment )\n\n[1] \"meat\"    \"milk\"    \"calorie\" \"control\"\n\nkenya$treatment = relevel( kenya$treatment, ref = \"control\" )\nlevels( kenya$treatment )\n\n[1] \"control\" \"meat\"    \"milk\"    \"calorie\"",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Example of a three-level longitudinal model</span>"
    ]
  },
  {
    "objectID": "kenya_ex.html#the-mathematical-model",
    "href": "kenya_ex.html#the-mathematical-model",
    "title": "42  Example of a three-level longitudinal model",
    "section": "42.3 The mathematical model",
    "text": "42.3 The mathematical model\nLet’s fit a random slope model, letting kids grow over time.\nLevel 1: We have for individual \\(i\\) in school \\(j\\) at time \\(t\\): \\[ Y_{ijt} = \\beta_{0ij} + \\beta_{1ij} (t-L) + \\epsilon_{ijt} \\]\nLevel 2: Each individual has their own growth curve. Their curve’s slope and intercepts varies around the school means: \\[ \\beta_{0ij} = \\gamma_{00j} + \\gamma_{01} gender_{ij} + u_{0ij} \\] \\[ \\beta_{1ij} = \\gamma_{10j} + \\gamma_{11} gender_{ij} + u_{1ij} \\] We also have that \\((u_{0ij}, u_{1ij})\\) are normally distributed with some 2x2 covariance matrix. We are forcing the impact of gender to be constant across schools, but are allowing girls and boys to grow at different rates. The average growth rate of a school can be different, as represented by the \\(\\gamma_{10j}\\).\nLevel 3: Finally our school mean slope and intercepts are \\[ \\gamma_{0j} = \\mu_{00} + w_{0i} \\] \\[ \\gamma_{1j} = \\mu_{10} + \\mu_{11} meat_j + \\mu_{12} milk_j + \\mu_{13} calorie_j + w_{1i}  \\] For the rate of growth at a school we allow different slopes for different treatments (compared to baseline). The milk, meat, and calorie are the three different treatments applied. Due to random assignment, we do not expect treatment to be related to baseline outcome, so we do not have the treatment in the intercept term–this is rather unstandard and we would typically allow baseline differences to account for random imbalance in the treatment assignment. But we are following the textbook example here.\nWe also have that \\((w_{0j}, w_{1j})\\) are normally distributed with some 2x2 covariance matrix:\n\\[  \\begin{pmatrix}w_{j0}\\\\\nw_{j1}\n\\end{pmatrix} \\sim N\\left(\\left(\\begin{array}{c}\n0 \\\\\n0\n\\end{array}\\right), \\left[\n\\begin{array}{cc}\n\\tau_{00} & \\tau_{01} \\\\\n& \\tau_{11}\n\\end{array}\n\\right]\\right) = N\\left(\\left(\\begin{array}{c}\n0 \\\\\n0\n\\end{array}\\right), \\Sigma_{sch} \\right) \\]\nThe \\(\\mu_0\\) and \\(\\mu_1\\) are the slope and intercept for the overall population growth (this is what defines our marginal model).\nWe will use \\(L = 1\\) to center the data at the first time point (so our intercept is expected ravens score at onset of the study).\nConceptual question: What would changing \\(L\\) do to our model and the reasoning about not having treatment in the intercept for school?",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Example of a three-level longitudinal model</span>"
    ]
  },
  {
    "objectID": "kenya_ex.html#fit-the-model",
    "href": "kenya_ex.html#fit-the-model",
    "title": "42  Example of a three-level longitudinal model",
    "section": "42.4 Fit the model",
    "text": "42.4 Fit the model\n\nlibrary( lme4 )\nkenya$rn = kenya$rn - 1 # center by L=1\nM1 = lmer( ravens ~ 1 + rn + gender*rn + treatment:rn + (1+rn|schoolid) + (1+rn|id:schoolid), \n           data=kenya )\n\nboundary (singular) fit: see help('isSingular')\n\ndisplay( M1 )\n\nlmer(formula = ravens ~ 1 + rn + gender * rn + treatment:rn + \n    (1 + rn | schoolid) + (1 + rn | id:schoolid), data = kenya)\n                    coef.est coef.se\n(Intercept)         17.41     0.19  \nrn                   0.59     0.08  \ngendergirl          -0.30     0.20  \nrn:gendergirl       -0.14     0.08  \nrn:treatmentmeat     0.17     0.09  \nrn:treatmentmilk    -0.13     0.09  \nrn:treatmentcalorie -0.02     0.09  \n\nError terms:\n Groups      Name        Std.Dev. Corr  \n id:schoolid (Intercept) 1.40           \n             rn          0.43     -0.09 \n schoolid    (Intercept) 0.45           \n             rn          0.09     -1.00 \n Residual                2.31           \n---\nnumber of obs: 2598, groups: id:schoolid, 546; schoolid, 12\nAIC = 12545.9, DIC = 12474\ndeviance = 12496.0 \n\n\nNow let’s connect some pieces:\n\n\\(\\mu_{00} = 17.41\\) and \\(\\mu_{11} = 0.59\\). The initial score for boys is 17.4, on average, with an average gain of 0.59 per year for control schools.\n\\(\\gamma_{01} = -0.30\\) and \\(\\gamma_{11} = -0.14\\), giving estimates that girls score lower and gain slower than boys.\nThe school-level variation in initial expected Raven scores is 0.45 (this is the standard deviation of \\(w_{0i}\\)), relatively small compared to the individual variation of 1.40 (this is the standard deviation of \\(u_{0ij}\\)).\nThe correlation of the \\(u_{0ij}\\) and \\(u_{1ij}\\) is basically zero (estimated at -0.09).\nThe random effects for school has a covariance matrix \\(\\Sigma_{sch}\\) of \\[ \\widehat{\\Sigma}_{sch} = \\left[\n\\begin{array}{cc}\n0.45^2 & 0.45 \\times 0.09 \\times -0.99 \\\\\n. & 0.09^2\n\\end{array}\n\\right] \\] The very negative correlation suggests an extrapolation effect, and that perhaps we could drop the random slope for schools.\nThe treatment effects are estimated as \\(\\mu_{11}=0.17, \\mu_{12}=-0.13\\), and \\(\\mu_{13}=-0.02\\).\n\nP-values for these will not be small, however, as the standard errors are all 0.09.\n\nWe could try to look at uncertainty on our parameters using the confint( M1 ) command, but it turns out that it crashes for this model. This can happen, and our -0.99 correlation gives a hint as to why. Let’s first drop the random slope at the school level and then try:\n\nM1B = lmer( ravens ~ rn + gender*rn + treatment:rn + (1|schoolid) + (1+rn|id:schoolid), \n           data=kenya )\ndisplay( M1B )\n\nlmer(formula = ravens ~ rn + gender * rn + treatment:rn + (1 | \n    schoolid) + (1 + rn | id:schoolid), data = kenya)\n                    coef.est coef.se\n(Intercept)         17.39     0.17  \nrn                   0.57     0.08  \ngendergirl          -0.30     0.20  \nrn:gendergirl       -0.14     0.08  \nrn:treatmentmeat     0.22     0.10  \nrn:treatmentmilk    -0.09     0.10  \nrn:treatmentcalorie  0.02     0.10  \n\nError terms:\n Groups      Name        Std.Dev. Corr  \n id:schoolid (Intercept) 1.42           \n             rn          0.44     -0.11 \n schoolid    (Intercept) 0.33           \n Residual                2.31           \n---\nnumber of obs: 2598, groups: id:schoolid, 546; schoolid, 12\nAIC = 12544.4, DIC = 12478\ndeviance = 12498.9 \n\nconfint( M1B )\n\n                      2.5 %   97.5 %\n.sig01               1.1657  1.65435\n.sig02              -0.3544  0.32871\n.sig03               0.3075  0.54179\n.sig04               0.0000  0.60033\n.sigma               2.2312  2.39580\n(Intercept)         17.0605 17.71696\nrn                   0.4091  0.72814\ngendergirl          -0.6870  0.09145\nrn:gendergirl       -0.2879  0.00772\nrn:treatmentmeat     0.0164  0.40811\nrn:treatmentmilk    -0.2876  0.09811\nrn:treatmentcalorie -0.1775  0.20453\n\n\nWe then have to puzzle out which confidence interval goes with what. The .sig01 is the variance of the kid (id:schoolid), which we can tell by the range it covers. Then the next must be correlation, and then the slope. This tells us we have no confidence the school random intercept is away from 0 (.sig04).",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Example of a three-level longitudinal model</span>"
    ]
  },
  {
    "objectID": "kenya_ex.html#some-quick-plots",
    "href": "kenya_ex.html#some-quick-plots",
    "title": "42  Example of a three-level longitudinal model",
    "section": "42.5 Some quick plots",
    "text": "42.5 Some quick plots\nWe can look at the Empirical Bayes intercepts:\n\nschools = data.frame( resid = ranef( M1 )$schoolid$`(Intercept)` )\nkids = data.frame( resid = ranef( M1 )$id$`(Intercept)` )\nresid = data.frame( resid = resid( M1 ) )\nresids = bind_rows( school=schools, child=kids, residual=resid, .id=\"type\" )\nresids$type = factor( resids$type, levels = c(\"school\",\"child\", \"residual\" ) )\n\nggplot( resids, aes( x = type, y = resid ) ) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n\nThis shows that the variation in occasion is much larger than kid, which is much larger than school.\nWe can calculate all the individual growth curves and plot those:\n\nkenya$predictions = predict( M1 )\nggplot( data=kenya, aes( x=rn, y=predictions, group=id ) ) +\n    facet_wrap( ~ schoolid, labeller = label_both) +\n    geom_line( alpha=0.3 )\n\n\n\n\n\n\n\n\nGenerally individual curves are estimated to have positive slopes. The schools visually look quite similar; any school variation is small compared to individual variation.",
    "crumbs": [
      "WORKED EXAMPLES",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Example of a three-level longitudinal model</span>"
    ]
  },
  {
    "objectID": "icc_viz.html",
    "href": "icc_viz.html",
    "title": "43  ICC Visualization",
    "section": "",
    "text": "The intraclass correlation (ICC) is the ratio of the level 2 variance to the total variance. It is a measure of between group differences, as it answers the question, “what proportion of total variance is accounted for by the level 2 units?” It is simultaneously a measure of within group similarity, answering the question, “what is the expected correlation in the outcome between pairs of observations drawn from the same cluster?” The ICC is also closely related to reliability in measurement, with a high ICC indicating more reliable measures (e.g., student test scores, inter-rater reliability, etc.)\nThe shiny app below allows you to set the number of level 1 units, level 2 units, and the ICC, using students in schools as the context. To build your intuitions, make a prediction for what the graph will look like with an ICC of 0, and an ICC of 1, then test them out by manipulating the ICC slider.",
    "crumbs": [
      "VISUALIZATIONS",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>ICC Visualization</span>"
    ]
  },
  {
    "objectID": "rand_slopes_viz.html",
    "href": "rand_slopes_viz.html",
    "title": "44  Random Slopes Visualization",
    "section": "",
    "text": "The shiny app below allows you to set the various parameters of a random slopes multilevel model to see how they are reflected in the data. I often find it easiest to think of random slopes models in longitudinal contexts, so the app here asks for “people” and “timepoints”, but the same logic applies to cross sectional contexts, such as students nested within schools.\nExercises:\n\nBefore manipulating a slider, make a prediction for what will change in the graph, then verify by moving the slider\nWhat parameter needs to be changed (and to what value) to generate a random intercepts model? Verify using the app.\nSelect parameters to generate a collection of students ranging from no growth to strong positive growth, where the lower growth schools had higher initial achievement.",
    "crumbs": [
      "VISUALIZATIONS",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Random Slopes Visualization</span>"
    ]
  },
  {
    "objectID": "rewb_viz.html",
    "href": "rewb_viz.html",
    "title": "45  Within vs Between / Contextual Effects Visualization",
    "section": "",
    "text": "Within, between, and contextual effects can be a challenge to think about or visualize. This shiny app allows you to explore different effects to build intuition about what these relationships mean, substantively, using students in schools as a context. In the graph, the big dots and black dashed line represent school means, that is, the between effect. The small dots and the multicolored lines represent individual schools, or, the within effect. The contextual effect is the difference in within and between effects (sort of like an interaction representing a “difference in slopes”). By default, it is set to 0, so the within and between effects are the same, which is the assumption of the random intercepts model. (NB The “offset” parameter is not a real MLM parameter; it just spreads out the school means on the x-axis to make the visualization more powerful)\nExercises:\n\nBefore manipulating a slider, make a prediction for what will change in the graph, then verify by moving the slider\nSelect parameters to generate a between effect of 0 and a within effect of 0.5\nSelect parameters to generate a between effect of 0.5 and a within effect of 0\nImagine that the clusters are people and the observations are measurements. What would the graph look like if the x-axis represented typing speed and the y-axis represented typing accuracy?\nUsing the within/between HSB examples, input the parameter estimates from the Mundlak model into the shiny app and compare that to our visualization of the data here.",
    "crumbs": [
      "VISUALIZATIONS",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Within vs Between / Contextual Effects Visualization</span>"
    ]
  },
  {
    "objectID": "centering_viz.html",
    "href": "centering_viz.html",
    "title": "46  Centering Visualization",
    "section": "",
    "text": "Centering of predictors is an important issue in multilevel modeling. In contrast to single-level regression, how we center variables can really change the interpretation of the slope coefficients. The three main options are:\n\nNo centering: leave \\(X\\) as it is\nGrand mean centering: subtract \\(\\bar{X}\\) from every \\(X\\) so that a value of 0 represents the grand mean. The slope has the same interpretation, but our estimates of random effects may change.\nGroup mean centering: subtract \\(\\bar{X}_j\\) from every \\(X\\) so that a value of 0 represents the cluster mean. The slope now represents the within-group relationship (just like fixed effects) because we have removed all between group variation from \\(X\\).\n\nThe visualization below helps us think about centering and why it matters.",
    "crumbs": [
      "VISUALIZATIONS",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Centering Visualization</span>"
    ]
  },
  {
    "objectID": "latent_logit_viz.html",
    "href": "latent_logit_viz.html",
    "title": "47  Latent Logit/LPM Visualization",
    "section": "",
    "text": "Often in education research, dichotomous variables are not really dichotomies (e.g., struck by lightning, not struck by lightning), but rather, dichotomized continuous variables, such as passing or failing a test. That is, a test score is a continuous measure of proficiency, but we can define a cut score above which you “pass” and below which you “fail”. This practice, while common, has many pitfalls (Ho, 2008) and can distort our understanding of trends and relationships. Fortunately the logit model (and its cousin the probit model) can help un-distort our vision!\nIn the shiny app below, we are imagining a distribution of test scores that rises over time. When the distribution is normal, on the left, the observed proportion of passing scores is non-linear, even though the trend in the test scores themselves is linear. Because a normal distribution has most of its mass in the center, this results in the classic s-shape of the logit model. When we fit a linear regression, we are implicitly assuming that the underlying (“latent”) distribution is uniform, resulting in the graph on the right. This is rarely the case empirically.\nWhen the cut score is at the average (0 in this case), this doesn’t make much of a difference. But things really start to break down when we shift the cut score to a more extreme value. Try moving it to a standard deviation of +1, and see which model performs better!",
    "crumbs": [
      "VISUALIZATIONS",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Latent Logit/LPM Visualization</span>"
    ]
  },
  {
    "objectID": "icc_derivation.html",
    "href": "icc_derivation.html",
    "title": "48  ICC Derivation",
    "section": "",
    "text": "Often, the ICC is described as the correlation between observations that share the same group membership. While you can look at the visualizer to get some intuition on what this means, here is a short proof adapted from S52 materials.\nConsider the variance components model (this is the random intercept model with no covariates):\n\\[\ny_{ij} = \\beta_0 + \\zeta_j + \\varepsilon_{ij}\n\\]\nThe correlation between an observation \\(y\\) and an observation from the same group \\(y'\\) is the standardized covariance:\n\\[\n\\rho(y, y') = \\frac{cov(y,y')}{\\sqrt{var(y)var(y')}}\n\\]\nWe can expand the numerator, the covariance between \\(y\\) and \\(y'\\) and substitute in the definition of \\(y\\) from our model:\n\\[\ncov(y,y') = cov(\\beta_0 + \\zeta_j + \\varepsilon_{ij}, \\beta_0 + \\zeta_j + \\varepsilon'_{ij})\n\\]\nBy definition, \\(\\beta_0\\) is the same for everyone (i.e., the “constant” term), and \\(\\zeta_j\\) will be the same for both observations because we are looking within a single cluster. The only difference between the two groups are the individual level error terms, \\(\\varepsilon_{ij}\\). The rules of covariance tell us that the constant drops out and the \\(\\varepsilon\\) too because it is independent of \\(\\zeta_j\\), we can simplify our equation:\n\\[\ncov(y,y') = cov(\\zeta_j, \\zeta_j)\n\\]\nThe covariance of a variable with itself is the variance:\n\\[\ncov(y,y') = cov(\\zeta_j, \\zeta_j) = var(\\zeta_j) = \\sigma^2_\\zeta\n\\]\nConceptually, the \\(\\zeta_j\\) represents the shared influences on \\(y\\) that would cause the similarity between observations in the same group.\nWe know from our variance decomposition in the ICC formula that \\(var(y)\\) is the sum of the between-group and within-group variance components (note the independence of random effects assumption is key here):\n\\[\nvar(y) = var(y') = \\sigma^2_\\zeta + \\sigma^2_\\varepsilon\n\\]\nWe can substitute these quantities into the original formula:\n\\[\n\\rho(y, y') = \\frac{cov(y,y')}{\\sqrt{var(y)var(y')}} = \\frac{\\sigma^2_\\zeta}{\\sigma^2_\\zeta + \\sigma^2_\\varepsilon} = ICC\n\\] Thus, the ICC is both the proportion of total variance accounted for by group membership and the correlation between pairs of observations drawn from the same group. QED!",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>ICC Derivation</span>"
    ]
  },
  {
    "objectID": "inflated_variance.html",
    "href": "inflated_variance.html",
    "title": "49  Inflated Variance Derivation",
    "section": "",
    "text": "Say we want to estimate the variability of mean math achievement across schools. I.e., each school has some average math achievement of its students, and we want to know how different schools are.\nThe naïve way of doing this is to estimate the mean math achievement for a sample of schools, and take the standard deviation (square root of variance) of this sample as a reasonable estimate. In math terms, we would calculate \\(\\bar{Y}_j\\) for each school \\(j\\) and then use as our estimate \\[\\widehat{\\tau^2} = var( \\bar{Y}_j ) = \\frac{1}{J-1} \\sum_{j=1}^J (\\bar{Y}_j - \\bar{Y})^2,\\]\nwhere \\(\\bar{Y}\\) is the average of the \\(\\bar{Y}_j\\) across our \\(J\\) schools. Our estimate, above, will give a number that is too big because the overall variance includes the uncertainty in estimating the individual \\(\\bar{Y}_j\\). The following is a math derivation on a simple scenario that illustrates why, and by how much.\nFirst, pretend our Data Generation Process (DGP) is Mother Nature making a bunch of schools, and then for each school making a bunch of kids. Our model is that the schools are represented by school-level true mean math achievement, and the kids are made by adding an individual kid effect to the mean math achievement of their schools.\nSo we have \\[\\alpha_j \\sim N( \\mu, \\tau^2 )\\] meaning each school is a random draw from a normal distribution with a mean \\(\\mu\\) and a standard deviation \\(\\tau\\). These are the true means of the schools. We wish we knew them, but we do not. Instead we see a sample of kids from the school and we hope the mean of the kids is close to this true mean \\(\\alpha_j\\).\nFor any kid \\(i\\) we have \\[Y_i  = \\alpha_{j[i]} + \\epsilon_i\\] with \\[\\epsilon_i \\sim N( 0, \\sigma^2 ).\\] These \\(\\epsilon_i\\) are the classic residuals we are used to.\nFor the moment, assume each school \\(j\\) has \\(n\\) kids. Then the average observed math achievement is \\[\\bar{Y}_j = \\frac{1}{n} \\sum_{i : j[i] = j} Y_i ,\\] the average of all kids in the school. Note the “\\(i : j[i] = j\\)” term, which reads as “\\(i\\) for those \\(i\\) where \\(j[i] = j\\)” meaning “sum over all students which go to school \\(j\\).”\nOk, so now we have math achievement for school \\(j\\). We then have \\[\\bar{Y}_j = \\frac{1}{n} \\sum_{i : j[i] = j} Y_i  = \\frac{1}{n} \\sum_{i : j[i] = j} \\alpha_{j[i]} + \\epsilon_i =   \\frac{1}{n} \\sum_{i : j[i] = j} \\alpha_j +  \\frac{1}{n} \\sum_{i : j[i] = j} \\epsilon_i = \\alpha_j +  \\frac{1}{n} \\sum_{i : j[i] = j}\\epsilon_i = \\alpha_j + \\bar{\\epsilon}_j .\\] Here we have \\(\\bar{\\epsilon}_j = \\bar{Y}_j - \\alpha_j\\), i.e., we have a school-level residual, the error in our estimate of \\(\\alpha_j\\) using \\(\\bar{Y}_j\\). This residual is the sum of a bunch of student residuals, which we assume are all independent of each other. When you average a bunch of independent, identically distributed (i.i.d.) residuals, each with variance \\(\\sigma^2\\), you get something which still has the same mean (of 0) but a smaller variance by a factor of \\(n\\): \\[var\\{ \\bar{\\epsilon_j} \\} = var\\{  \\frac{1}{n} \\sum_{i : j[i] = j\\}\\epsilon_i } = \\frac{1}{n^2} \\sum_{i : j[i] = j} var\\{ \\epsilon_i \\} =  \\frac{1}{n} \\sigma^2\\] This is the familiar result that the mean of a bunch of variables has a standard deviation \\(1/\\sqrt{n}\\) of the original standard deviation (part of the Central Limit Theorem).\nWe can think of \\(\\bar{Y}_j\\) as a random quantity, random for two reasons: school \\(j\\) is a randomly made school, and the students in school \\(j\\) are randomly made students. Under the assumption that the students’ error terms are independent of the school’s mean math achievement we can easily calculate the variance of our estimator: \\[var\\{ \\bar{Y}_j \\} = var\\{ \\alpha_j \\} + var\\{ \\bar{\\epsilon}_j \\} =   \\tau^2 +  \\frac{1}{n} \\sigma^2\\]\nThis is bigger than our target of \\(\\tau^2\\), the true variability in mean math achievement across schools. The uncertainty in estimating the \\(\\alpha_j\\) has entered into the variability.\nOur estimate \\(\\widehat{\\tau^2}\\) will be an unbiased estimate of \\(\\tau^2 + \\frac{1}{n} \\sigma^2\\). One way to fix is to estimate \\(\\sigma^2\\) and then adjust our estimate of the variance of \\(\\tau^2\\) by subtracting \\(\\frac{1}{n} \\hat{\\sigma}^2\\). Another is to use multilevel modeling, which does this for us, in effect.",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Inflated Variance Derivation</span>"
    ]
  },
  {
    "objectID": "cov_matrix_derivation.html",
    "href": "cov_matrix_derivation.html",
    "title": "50  Covariance Derivation",
    "section": "",
    "text": "50.1 The student-level residual matrix\nFollowing Packet 7.1, let’s think about a generic regression equation for a linear growth model with 5 timepoints (this is a simplified version of the NYS model, where each time point is a year of age, 11–15).\nIn particular, consider \\[\nY_{ti} = \\beta_0 + \\beta_1 age_{ti} + u_{ti}\n\\] where \\(age_{ti}\\) is our age from 11 (so an observation at 11 years old would have age11 = 0). This means our intercept correspond to our first timepoint, with \\(a_1 = 0, a_2 = 1, ..., a_5 = 4\\). I.e., our \\(age_{ti}\\) is number of years since onset of study.\nIn this model, \\(\\beta_{0}\\) is the average outcome across our population at the onset of the study and \\(\\beta_{1}\\) is the average rate of growth (per year) in the population.\nNow we have 5 observations for each student \\(i\\), so the residuals \\((u_{1i}, \\ldots, u_{5i})\\) are likely correlated with each other. For example, a student might just generally have higher levels of outcome, or lower levels, which means the overall residual of one time point would be related to the reisduals of other time points. In math, we can write that for any randomly subject \\(i\\), the covariance matrix of their residuals is \\[\\begin{aligned}\n\\begin{pmatrix} u_{i1} \\\\\nu_{i2} \\\\\nu_{i3} \\\\\nu_{i4} \\\\\nu_{i5}\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\\\\\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\delta_{11} & \\delta_{12} & \\delta_{13} & \\delta_{14} & \\delta_{15} \\\\\n           & \\delta_{22} & \\delta_{23} & \\delta_{24} & \\delta_{25} \\\\\n         &              & \\delta_{33} & \\delta_{34} & \\delta_{35} \\\\\n         &              &             & \\delta_{44} & \\delta_{45} \\\\\n         &              &              &            & \\delta_{55}\n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned} = N( 0, \\Sigma_i)\\]\nThis matrix of residuals for student \\(i\\) is one of the blocks in our \\(N \\times N\\) block-diagonal matrix for all our residuals (this would be the giant matrix plugged into our sandwich formula to get standard errors for \\(\\beta_0\\) and \\(\\beta_1\\)), where \\(N\\) is the number of observations. Assuming 5 observations per student, multilevel and generalized linear modeling (which we are talking about here) make the assumption that this matrix is the same across students; cluster robust standard errors would not make this assumption. (More broadly, MLM and generalized linear modeling make the assumption that we can represent all the \\(\\Sigma_i\\) in terms of measured covariates and pre-specified parameters, but in this case we end up with the same matrix for all students with 5 time points. Students with fewer than 5 would be subsets of this matrix corresponding to the time points observed.)\nThe diagonal of \\(\\Sigma_i\\) are our variances at each timepoint (this means, for example, that if our model is good, that if we took the variance of all the observations where \\(t=5\\) across our dataset we should get something close to \\(\\delta_{55}\\)).\nIn the remainder of this document, we look at how MLM gives expressions for this matrix.",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Covariance Derivation</span>"
    ]
  },
  {
    "objectID": "cov_matrix_derivation.html#covariance-matrix-for-a-random-intercept-model",
    "href": "cov_matrix_derivation.html#covariance-matrix-for-a-random-intercept-model",
    "title": "50  Covariance Derivation",
    "section": "50.2 Covariance matrix for a random intercept model",
    "text": "50.2 Covariance matrix for a random intercept model\nFollowing Packet 7.1, we start with a random intercept model with a completely pooled growth component with 5 timepoints (this is a simplified version of the NYS model, where each time point is a year of age, 11–15). In particular, take the model represented by this lmer() command:\n\nM = lmer( Y ~ 1 + age11 + (1|id), data=nys )\n\nwhere age11 is our age from 11 (so an observation at 11 years old would have age11 = 0).\nIn math, this model is \\[\\begin{aligned}\nY_{ti} &= \\pi_{0i} + \\beta_{1} age_ + \\epsilon_{ti} \\\\\n\\epsilon_{ti} &\\sim N( 0, \\sigma^2 ) \\\\\n\\pi_{0i} &=  \\beta_{0} + r_{0i} \\\\\nr_{0j} & \\sim N( 0, \\tau_{00} ) \\\\\n\\end{aligned}\\]\nThe reduced form is \\[\\begin{aligned}\nY_{ti} &= \\beta_{0}  + \\beta_{1}  a_t  + r_{0i} + \\epsilon_{ti} \\\\\n&= \\beta_{0}  + \\beta_{1}  a_t  + u_{ti}\n\\end{aligned}\\] with \\(u_{ti} = r_{0i} + \\epsilon_{ti}\\).\nNote that \\(\\epsilon_{ti}\\) is the specific time-individual residual after the individual random effects, and \\(u_{ti}\\) is the overall residual (deviation from what we expect from the population, or the difference between our observed outcome and the population model, not student latent growth curve).\nUsing our model, let’s calculate some variances and covariances of the residuals.\nFirst the variance of a residual at time point \\(t\\): \\[\n\\begin{aligned}\nvar( u_{ti} ) &= var( r_{i} + \\epsilon_{ti} ) \\\\\n  &= var( r_{i} ) + var( \\epsilon_{ti} ) + cov( r_i, \\epsilon_{ti} ) \\\\\n  &= \\tau_{00} + \\sigma^2\n\\end{aligned}\n\\] because the residuals are independent, so all covariances of different residuals, such as \\(r_i\\) and \\(\\epsilon_{ti}\\) are 0. The second line is using the identity \\(Var( A + B ) = Var( A ) + Var( B ) + 2 Cov( A, B )\\).\nSecond, the covariance of \\(u_{1i}\\) and \\(u_{2i}\\), i.e., time 1 and time 2 for the same person: \\[\n\\begin{aligned}\ncov( u_{1i}, u_{2i} ) &= cov( r_{i} + \\epsilon_{1i}, r_{i} + \\epsilon_{2i},  ) \\\\\n  &= cov( r_{i}, r_{i} ) + cov( r_{i}, \\epsilon_{2i} ) + cov( \\epsilon_{1i}, r_i ) + cov( \\epsilon_{1i}, \\epsilon_{2i} ) \\\\\n  &= \\tau_{00}\n\\end{aligned}\n\\] The last bit is again because the covariances of different residuals are 0. The covariance of something with itself is just the variance. The second line comes from \\[cov( A + B, C + D ) = cov( A, C ) + cov( A, D ) + cov( B, C ) + cov( B, D ),\\] i.e., you multiply all the bits out. The above clearly generalizes so the covariance of any two time points within a student has covariance of \\(\\tau_{00}\\).\nFinally, looking at two different students, we have \\[\n\\begin{aligned}\ncov( u_{ti}, u_{t'j} ) &= cov( r_{i} + \\epsilon_{ti}, r_{j} + \\epsilon_{t'j},  ) \\\\\n  &= cov( r_{i}, r_{j} ) + cov( r_{i}, \\epsilon_{t'j} ) + cov( \\epsilon_{ti}, r_j ) + cov( \\epsilon_{ti}, \\epsilon_{t'j} ) \\\\\n  &= 0 ,\n\\end{aligned}\n\\] because all of the residuals are independent, according to our model. This says that all our population residuals from different students are not correlated. This gives us our block diagonal structure on our \\(N \\times N\\) matrix of residuals. For student \\(i\\), the first two results tell us that: \\[\\begin{aligned}\n\\begin{pmatrix} u_{i1} \\\\\nu_{i2} \\\\\nu_{i3} \\\\\nu_{i4} \\\\\nu_{i5}\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\\\\\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} + \\sigma^2 & \\tau_{00} & \\tau_{00} & \\tau_{00} & \\tau_{00} \\\\\n           & \\tau_{00} + \\sigma^2 & \\tau_{00} & \\tau_{00} & \\tau_{00} \\\\\n         &              & \\tau_{00} + \\sigma^2 & \\tau_{00} & \\tau_{00} \\\\\n         &              &             & \\tau_{00} + \\sigma^2 & \\tau_{00} \\\\\n         &              &              &            & \\tau_{00} + \\sigma^2\n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned} .\\]\nOur multilevel model has given us a specific structure for our student-level residual covariance matrix \\(\\Sigma_i\\). We could just fit a regression at the population level with this matrix specified, without talking about random intercepts or anything. We can also tweak this matrix in ways that capture other kinds of variation. This is the key to this approach to modeling clustered or non-independent data.\nIn the next section we repeat this for a random slope model. Same idea, more messy math.",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Covariance Derivation</span>"
    ]
  },
  {
    "objectID": "cov_matrix_derivation.html#covariance-matrix-for-a-random-slope-model",
    "href": "cov_matrix_derivation.html#covariance-matrix-for-a-random-slope-model",
    "title": "50  Covariance Derivation",
    "section": "50.3 Covariance matrix for a random slope model",
    "text": "50.3 Covariance matrix for a random slope model\nTake a random slopes model with 5 timepoints (this is the NYS model, each time point is a year of age, 11–15):\n\\[\n\\begin{aligned}\nY_{ti} &= \\pi_{0i} + \\pi_{1i} age_{ti} + \\epsilon_{ti} \\\\\n\\pi_{0i} &=  \\beta_{0} + r_{0i} \\\\\n\\pi_{1i} &= \\beta_{1} + r_{1i} \\\\\n\\begin{pmatrix} r_{0j} \\\\\nr_{1j}\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{01} \\\\\n        & \\tau_{11}\n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned}\n\\]\nLet \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). Let our intercept correspond to our first timepoint, so \\(a_1 = 0, a_2 = 1, ..., a_5 = 4\\). I.e., our \\(age_{ti}\\) is number of years since onset of study. Then \\(\\beta_{0}\\) is the average outcome at onset of the study and \\(\\beta_{1}\\) is the rate of growth (per year) in the population.\nThe reduced form is \\[\\begin{aligned}\nY_{ti} &= \\beta_{0}  + \\beta_{1}  age_{ti}  + r_{0i} + r_{1i} age_{ti} + \\epsilon_{ti} \\\\\n&= \\beta_{0}  + \\beta_{1}  age_{ti}  + u_{ti}\n\\end{aligned}\\] with \\(u_{ti} = r_{0i} + r_{1i} age_{ti} + \\epsilon_{ti}\\).\nNow let’s use this definition of \\(u_{ti}\\) to calculate all the \\(\\delta_{tt'}\\) values in the student level covariance matrix \\(\\Sigma_i\\).\n\n50.3.1 Calculating the covariances\nLet’s calculate \\(\\delta_{13} = cov( \\epsilon_{i1}, \\epsilon_{i2} )\\).\nFirst we need a math fact about random quantities \\(A\\), \\(B\\), and \\(C\\): \\[cov( A + B, C ) = cov( A, C ) + cov( B, C ) .\\] Also if you multiply something by a constant \\(k\\) you have \\[cov( k_1 A, k_2 B ) = k_1 k_2 cov( A, B ) .\\]\nAlso note that \\(a_1 = 0\\) and \\(a_3 = 2\\), given our coding of age ($a_1$ is the time covariate at age 11, which is 0, for example). Then we have, plugging in those values: \\[\\begin{aligned}\n\\delta_{13} &= cov( u_{i1}, u_{i3} ) \\\\\n   &= cov(  r_{0i} + r_{1i} a_1 + \\epsilon_{1i},  r_{0i} + r_{1i} a_3 + \\epsilon_{3i} ) \\\\\n   &= cov(  r_{0i}  + \\epsilon_{1i},  r_{0i} + 2 r_{1i} + \\epsilon_{3i} ) \\\\\n   &= cov(  r_{0i}, r_{0i} ) + cov( r_{0i}, 2 r_{1i} ) + cov( r_{0i}, \\epsilon_{3i} ) + cov( \\epsilon_{1i}, r_{0i}) + cov( \\epsilon_{1i}, 2 r_{1i} )  + cov( \\epsilon_{1i}, \\epsilon_{3i}) \\\\\n   &= \\tau_{00} + 2\\tau_{01} + 0 + 0 + 0 + 0 \\\\\n   &= \\tau_{00} + 2\\tau_{01}\n\\end{aligned}\\]\nNote how we multiple out the individual components, and this gives an expression for the overall covariance of our two residuals. If we did this for each \\(\\delta_{tt'}\\) we could fill in our \\(5 \\times 5\\) matrix. Fun!\nA core idea here is the independence of the different residual pieces makes a lot of the terms go to 0, giving short(er) expressions than we might have otherwise. The random slope model dictates the overall covariance of the residuals.\n\n\n50.3.2 Calculating the diagonal terms.\nFor the variances, you would just calculate covariance of a quantity with itself. Let’s do \\(\\delta_{11}\\), the variance of timepoint 1: \\[\\begin{aligned}\n\\delta_{11} &= var( u_{1i} ) = cov( u_{1i}, u_{1i} ) \\\\\n   &= cov(  r_{0i} + r_{1i} a_1 + \\epsilon_{1i},  r_{0i} + r_{1i} a_1 + \\epsilon_{1i} ) \\\\\n   &= cov(  r_{0i}  + \\epsilon_{1i},  r_{0i} + \\epsilon_{1i} ) \\\\\n   &= cov(  r_{0i}, r_{0i} ) + cov( r_{0i},  \\epsilon_{1i} ) + cov( \\epsilon_{1i}, r_{0i}) + cov( \\epsilon_{1i},\\epsilon_{1i} )  \\\\\n   &= \\tau_{00} + 0 + 0 + \\sigma^2 =  \\tau_{00} + \\sigma^2\n\\end{aligned}\\]\nNow let’s do \\(\\delta_{55}\\), the variance of timepoint 5: \\[\\begin{aligned}\n\\delta_{55} &= var( u_{5i} ) = cov( u_{5i}, u_{5i} ) \\\\\n   &= cov(  r_{0i} + r_{1i} a_5 + \\epsilon_{5i},  r_{0i} + r_{5i} a_5 + \\epsilon_{5i} ) \\\\\n   &= cov(  r_{0i} + 4 r_{1i} + \\epsilon_{5i},  r_{0i} + 4 r_{1i} + \\epsilon_{5i} ) \\\\\n   &= cov(  r_{0i}, r_{0i} ) + cov(  r_{0i}, 4 r_{1i} )  + cov( r_{0i},  \\epsilon_{5i} ) + \\\\\n    &\\qquad cov( 4 r_{1i}, r_{0i} ) + cov( 4 r_{1i}, 4 r_{1i} )  + cov( 4 r_{1i}, \\epsilon_{5i} ) \\\\\n    & \\qquad cov( \\epsilon_{1i}, r_{0i}) + cov( \\epsilon_{1i}, 4 r_{1i} )  + cov( \\epsilon_{1i},\\epsilon_{1i} )  \\\\\n   &= \\tau_{00} + 4 \\tau_{01} + 0 + 4 \\tau_{01} + 16 \\tau_{11} + 0 + 0 + 0 + \\sigma^2 \\\\\n   &= \\tau_{00} + 16 \\tau_{11} + 8 \\tau_{01} + \\sigma^2 .\n\\end{aligned}\\]\nNote how the variance around the intercept (at time 1 where \\(a_1 = 0\\)) looks like it would be smaller than the variance further out. That being said, the covariance \\(\\tau_{01}\\) could be large and negative, causing the variance at the intercept to be less. But, if \\(\\tau_{01}\\) is positive, the overall variance increases as we move away from the intercept point.\nOne interesting aspect of random slope models is the marginal (at each time point) variance changes at each time point. This is heteroskedasticity: the variances are each time point can be different because the lines can spread or gather.",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Covariance Derivation</span>"
    ]
  },
  {
    "objectID": "complex_error.html",
    "href": "complex_error.html",
    "title": "51  An overview of complex error structures",
    "section": "",
    "text": "51.1 National Youth Survey running example\nOur running example is the data as described in Raudenbush and Bryk, and we follow the discussion on page 190. These data are the first cohort of the National Youth Survey (NYS). This data comes from a survey in which the same students were asked yearly about their acceptance of 9 “deviant”^[Wow, has the way we talk about things changed over the years.] behaviors (such as smoking marijuana, stealing, etc.). The study began in 1976, and followed two cohorts of children, starting at ages 11 and 14 respectively. We will analyze the first 5 years of data.\nAt each time point, we have measures of:\nFor each student, we also have:\nOne reasonable research question would to describe how the cohort evolved. For this question, the parameters of interest would be the average attitudes at each age. Standard deviations and intrasubject correlations are, as is often but not always the case, simply nuisance parameters. Still, the better we can do at realistically modeling these nuisance parameters, the more precision we will have for the measures of interest, and the power we will have to test relevant hypotheses.",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>An overview of complex error structures</span>"
    ]
  },
  {
    "objectID": "complex_error.html#national-youth-survey-running-example",
    "href": "complex_error.html#national-youth-survey-running-example",
    "title": "51  An overview of complex error structures",
    "section": "",
    "text": "ATTIT, the attitude towards deviance, with higher numbers implying higher tolerance for deviant behaviors.\nEXPO, the “exposure”, based on asking the children how many friends they had who had engaged in each of the behaviors. Both of these numbers have been transformed to a logarithmic scale to reduce skew.\n\n\n\nGender (binary)\nMinority status (binary)\nFamily income, in units of $10K.\n\n\n\n51.1.1 Getting the data ready\nWe’ll focus on the first cohort, from ages 11-15. First, let’s read the data.\nNote that this table is in “wide format”. That is, there is only one row for each student, with all the different observations for that student in different columns of that one row.\n\nnyswide = read.csv(\"data/nyswide.csv\")\nhead(nyswide)\n\n  ID ATTIT.11 EXPO.11 ATTIT.12 EXPO.12 ATTIT.13 EXPO.13 ATTIT.14 EXPO.14\n1  3     0.11   -0.37     0.20   -0.27     0.00   -0.37     0.00   -0.27\n2  8     0.29    0.42     0.29    0.20     0.11    0.42     0.51    0.20\n3  9     0.80    0.47     0.58    0.52     0.64    0.20     0.75    0.47\n4 15     0.44    0.07     0.44    0.32     0.89    0.47     0.75    0.26\n5 33     0.20   -0.27     0.64   -0.27     0.69   -0.27       NA      NA\n6 45     0.11    0.26     0.37   -0.17     0.37    0.14     0.37    0.14\n  ATTIT.15 EXPO.15 FEMALE MINORITY INCOME\n1     0.11   -0.17      1        0      3\n2     0.69    0.20      0        0      4\n3     0.98    0.47      0        0      3\n4     0.80    0.47      0        0      4\n5     0.11    0.07      1        0      4\n6     0.69    0.32      1        0      4\n\n\nFor our purposes, we want it in “long format.” The pivot_longer() command does this for us:\n\nnys1.na &lt;- nyswide %&gt;%\n  pivot_longer(\n    cols = c(ATTIT.11:ATTIT.15, EXPO.11:EXPO.15),\n    names_to = c(\".value\", \"AGE\"),\n    names_sep = \"\\\\.\",\n    values_to = c(\"ATTIT\", \"EXPO\")\n  )\n\n## Drop missing ATTIT values\nnys1 = nys1.na[!is.na(nys1.na$ATTIT),] \n\n## Make age a number\nnys1$AGE = as.numeric(nys1$AGE)\nhead( nys1 )\n\n# A tibble: 6 × 7\n     ID FEMALE MINORITY INCOME   AGE ATTIT  EXPO\n  &lt;int&gt;  &lt;int&gt;    &lt;int&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     3      1        0      3    11  0.11 -0.37\n2     3      1        0      3    12  0.2  -0.27\n3     3      1        0      3    13  0    -0.37\n4     3      1        0      3    14  0    -0.27\n5     3      1        0      3    15  0.11 -0.17\n6     8      0        0      4    11  0.29  0.42\n\n\nWe also need to make our age a factor so it is treated appropriately as an indicator of what wave the data was collected in.\n\nnys1$agefac = as.factor(nys1$AGE) \n\nJust to get a sense of the data, let’s plot each age as a boxplot\n\nggplot( nys1, aes( agefac, ATTIT ) ) +\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote some features of the data: First, we see that ATTIT goes up over time. Second, we see the variation of points also goes up over time. This is heteroskedasticity.\nIf we plot individual lines we have\n\nnys1$AGEjit = jitter(nys1$AGE)\nnys1$ATTITjit = jitter(nys1$ATTIT, amount=0.05)\nggplot( filter( nys1, complete.cases(nys1) ), aes( AGEjit, ATTITjit, group=ID ) ) +\n  geom_line( alpha=0.2 ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote how we have correlation of residuals, in that some students are systematically low and some are systematically higher (although there is a lot of bouncing around).",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>An overview of complex error structures</span>"
    ]
  },
  {
    "objectID": "complex_error.html#representation-of-error-structure",
    "href": "complex_error.html#representation-of-error-structure",
    "title": "51  An overview of complex error structures",
    "section": "51.2 Representation of error structure",
    "text": "51.2 Representation of error structure\nIn our data, we have 5 observations \\(y_{it}\\) for each subject i at 5 fixed times \\(t=1\\) through \\(t=5\\). Within each person \\(i\\) (where person is our Level-2 group, and time is our Level-1), we can write\n\\[\\begin{pmatrix}y_{i1}\\\\\ny_{i2}\\\\\ny_{i3}\\\\\ny_{i4}\\\\\ny_{i5}\n\\end{pmatrix} = \\left(\\begin{array}{c}\n\\mu_{1i}\\\\\n\\mu_{2i}\\\\\n\\mu_{3i}\\\\\n\\mu_{4i}\\\\\n\\mu_{5i}\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\right)\\] where our set of 5 residuals are the random part, distributed as\n\\[\\left(\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n0\n\\end{array}\\right),\\left(\\begin{array}{ccccc}\n\\tau_{11} & \\tau_{12} & \\tau_{13} & \\tau_{14} & \\tau_{15}\\\\\n. & \\tau_{22} & \\tau_{23}& \\tau_{24} & \\tau_{25}\\\\\n. & . & \\tau_{33}& \\tau_{34} & \\tau_{35}\\\\\n. & . & . & \\tau_{44} & \\tau_{45}\\\\\n. & . & . & . & \\tau_{55}\n\\end{array}\\right)\\right] = N( 0, \\Sigma ). \\]\nThe key part is the correlation between the residuals at different times. We call our entire covariance matrix \\(\\Sigma\\). This matrix describes how the residuals within a single individual (with 5 time points of observation) are correlated.\nOur regression model gives us the mean vector for any given student (e.g., \\((\\mu_{1i}, \\ldots, \\mu_{5i})\\) would be \\(X_i'\\beta\\), where \\(X_i\\) is a \\(5 \\times p\\) matrix of covariates for student \\(i\\), and \\(\\beta\\) is our fixed effect parameter vector. \\(X_i\\) would have one row per time point and time would be one of the columns, to give our predictions for our 5 time points.\nOur error structure model gives us the distribution of the \\((\\epsilon_{1i}, \\ldots, \\epsilon_{5i})\\) for student \\(i\\). Different ideas about the data generating process lead to different correlation structures here. We saw a couple of those in class.",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>An overview of complex error structures</span>"
    ]
  },
  {
    "objectID": "complex_error.html#reproducing-rbs-chapter-6-examples",
    "href": "complex_error.html#reproducing-rbs-chapter-6-examples",
    "title": "51  An overview of complex error structures",
    "section": "51.3 Reproducing R&B’s Chapter 6 examples",
    "text": "51.3 Reproducing R&B’s Chapter 6 examples\nThe above provides a framework for thinking about grouped data: each group (i.e., student) is a small world with a linear prediction line and a collection of residuals around that line. Under this view, we specify a specific structure on how the residuals relate to each other. (E.g., for classic OLS we would have i.i.d. normally distributed residuals, represented as our \\(\\Sigma\\) being a diagonal matrix with \\(\\sigma^2\\) along the diagonal and 0s everywhere else). In R, once we determine what structure we want, we can fit models based on parameterized correlation matrices using the lme command from the nlme package (You may need to first call install.packages(\"nlme\") to get this package), or the gls package.\nLet’s load the nlme package now:\n\nlibrary(nlme)\n\nRecall that all of these models include a linear term on age and an intercept (so two fixed effects and no covariate adjustment).\n\n51.3.1 Compound symmetry (random intercept model)\nA “compound symmetry” residual covariance structure (all diagonal elements equal, all off-diagonal elements equal) is actually equivalent to a random intercepts model. Thus, there are 2 ways to get this same model:\n\nmodelRE = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~1|ID )\n\nand\n\nmodelCompSymm = gls(ATTIT ~ AGE,\n                    data=nys1,\n                    correlation=corCompSymm(form=~AGE|ID) )\n\nFor reference, using the lme4 package we again have (we use lme4:: in front of lmer to avoid loading the lme4 package fully):\n\nmodelRE.lme4 = lme4::lmer(ATTIT ~ AGE + (1|ID), data=nys1 )\n\nWe can get the correlation matrix for individuals #3:\n\nmyVarCovs = getVarCov(modelRE,type=\"marginal\", individual=3)\nmyVarCovs\n\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.066450 0.034113 0.034113 0.034113 0.034113\n2 0.034113 0.066450 0.034113 0.034113 0.034113\n3 0.034113 0.034113 0.066450 0.034113 0.034113\n4 0.034113 0.034113 0.034113 0.066450 0.034113\n5 0.034113 0.034113 0.034113 0.034113 0.066450\n  Standard Deviations: 0.25778 0.25778 0.25778 0.25778 0.25778 \n\n\nIf we look at an individual #5, who only has 4 timepoints we get a \\(4 \\times 4\\) matrix:\n\ngetVarCov(modelRE,type=\"marginal\", individual=5)\n\nID 33 \nMarginal variance covariance matrix\n         1        2        3        4\n1 0.066450 0.034113 0.034113 0.034113\n2 0.034113 0.066450 0.034113 0.034113\n3 0.034113 0.034113 0.066450 0.034113\n4 0.034113 0.034113 0.034113 0.066450\n  Standard Deviations: 0.25778 0.25778 0.25778 0.25778 \n\n\nOther individuals are the same, if they have the same number of time points, given our model. So in this model, we are saying the residuals of a student have the same distribution as any another student with the same number of time points.\n\n51.3.1.1 Comparing the models\nThese are two very different ways of specifying the same thing, and the parameter estimates we get out are also not the same. Compare the two summary printouts:\n\nsummary(modelRE)\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -204.9696 -185.0418 106.4848\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept)  Residual\nStdDev:   0.1846979 0.1798237\n\nFixed effects:  ATTIT ~ AGE \n                 Value  Std.Error  DF   t-value p-value\n(Intercept) -0.5099954 0.05358498 839 -9.517505       0\nAGE          0.0644387 0.00398784 839 16.158810       0\n Correlation: \n    (Intr)\nAGE -0.969\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.90522949 -0.64353962 -0.01388485  0.60377631  3.26938845 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\n\nand\n\nsummary(modelCompSymm)\n\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n        AIC       BIC   logLik\n  -204.9696 -185.0418 106.4848\n\nCorrelation Structure: Compound symmetry\n Formula: ~AGE | ID \n Parameter estimate(s):\n      Rho \n0.5133692 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.5099954 0.05358498 -9.517505       0\nAGE          0.0644387 0.00398784 16.158810       0\n\n Correlation: \n    (Intr)\nAGE -0.969\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.77123071 -0.77132300 -0.06434029  0.71151900  3.38387884 \n\nResidual standard error: 0.2577787 \nDegrees of freedom: 1079 total; 1077 residual\n\n\nThese do not look very similar, do they? But wait:\n\nlogLik(modelCompSymm)\n\n'log Lik.' 106.4848 (df=4)\n\nlogLik(modelRE)\n\n'log Lik.' 106.4848 (df=4)\n\nlogLik(modelRE.lme4)\n\n'log Lik.' 106.4848 (df=4)\n\nAIC( modelCompSymm )\n\n[1] -204.9696\n\nAIC( modelRE )\n\n[1] -204.9696\n\nAIC( modelRE.lme4 )\n\n[1] -204.9696\n\n\nIn fact, they have the same AIC, etc., because they are equivalent models.\nThe lesson is that it’s actually quite hard to see the correspondence between a familiar random-effects model and an equivalent model expressed in terms of a covariance matrix. Sure, we could do a bunch of math and see that in the end they are the same; but that math is already daunting here, and this is the simplest possible situation. The fitted parameters of a covariance-based model are just really hard to interpret in familiar terms.\n\n\n\n51.3.2 Autoregressive error structure (AR[1])\nOne typical structure used for longitudinal data is the “autoregressive” structure. The idea is threefold:\n\n\\(Var(u_{it}) = \\sigma^2\\) - that is, overall marginal variance is staying constant.\n\\(Cor(u_{it},u_{i(t-1)}) = \\rho\\) - that is, residuals are a little bit “sticky” over time so residuals from nearby time points tend to be similar.\n\\(E(u_{it}|u_{i(t-1)},u_{i(t-2)}) = E(u_{it}|u_{i(t-1)})\\) - that is, the only way the two-periods-ago measurement tells you anything about the current one is through the intermediate one, with no longer-term effects or “momentum”.\n\nIn this case, the unconditional two-step correlation \\(Cor(u_{it},u_{i(t-2)})\\) is also easy to calculate. Intuitively, we can say that a portion \\(\\rho\\) of the residual “is the same” after each step, so that after two steps the portion that “is the same” is \\(\\rho\\) of \\(\\rho\\), or \\(\\rho^2\\). Clearly, then, after three steps the correlation will be \\(\\rho^3\\), and so on. In other words, the part that “is the same” is decaying in an exponential pattern. Indeed, one could show that (3.), above, requires the correlated part to decay in a memoryless pattern, leaving the Exponential and Hypergeometric distributions (which both show exponential decay) among the few options.\nThus, the within-subject correlation structure implied by these postulates is:\n\\[\\left(\\begin{array}{c}\nu_{1i}\\\\\nu_{2i}\\\\\nu_{3i}\\\\\nu_{4i}\\\\\n...\\\\\nu_{ni}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n...\\\\\n0\n\\end{array}\\right),\\sigma^2\\left(\\begin{array}{cccccc}\n1 & \\rho  & \\rho^2 & \\rho^3 & ... & \\rho^{n-1}\\\\\n. & 1 & \\rho & \\rho^2 & ... & \\rho^{n-2}\\\\\n. & . & 1& \\rho  & ... & \\rho^{n-3}\\\\\n. & . & . & 1 & ... & \\rho^{n-4} \\\\\n... & ... & ... & ... & ... & ... \\\\\n. & . & . & . & ... & 1\n\\end{array}\\right)\\right]\\\\\\]\nAs you can see, this structure takes advantage of the temporal nature of the data sequence to parameterize the covariance matrix with only two underlying parameters: \\(\\sigma\\) and \\(\\rho\\). By contrast, a random intercept model needs the overall \\(\\sigma\\) and variance of intercepts \\(\\tau\\)—also two parameters! Same complexity, different structure.\n\n51.3.2.1 Fitting the AR[1] covariance structure\nTo get a true AR[1] residual covariance structure, we need to leave the world of hierarchical models, and thus use the command gls. This is just what we’ve discussed in class. However, later on in this document, we’ll see how to add AR[1] structure on top of a hierarchical model, which is messier from a theoretical point of view, but often more useful and interpretable in practice.\n\nmodelAR1 = gls(ATTIT ~ AGE, \n                    data=nys1,\n                    correlation=corAR1(form=~AGE|ID) )\n\nsummary(modelAR1)\n\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n        AIC       BIC   logLik\n  -250.4103 -230.4826 129.2051\n\nCorrelation Structure: ARMA(1,0)\n Formula: ~AGE | ID \n Parameter estimate(s):\n     Phi1 \n0.6159857 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.4534647 0.07515703 -6.033564       0\nAGE          0.0601205 0.00569797 10.551218       0\n\n Correlation: \n    (Intr)\nAGE -0.987\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.75013168 -0.81139621 -0.03256558  0.74814629  3.40350724 \n\nResidual standard error: 0.2561765 \nDegrees of freedom: 1079 total; 1077 residual\n\n\nYou have to dig around in the large amount of output to find the parameter estimates, but they are there. Phi1 is the auto-correlation parameter. And the covariance of residuals:\n\ngetVarCov(modelAR1,type=\"marginal\")\n\nMarginal variance covariance matrix\n          [,1]     [,2]     [,3]     [,4]      [,5]\n[1,] 0.0656260 0.040425 0.024901 0.015339 0.0094485\n[2,] 0.0404250 0.065626 0.040425 0.024901 0.0153390\n[3,] 0.0249010 0.040425 0.065626 0.040425 0.0249010\n[4,] 0.0153390 0.024901 0.040425 0.065626 0.0404250\n[5,] 0.0094485 0.015339 0.024901 0.040425 0.0656260\n  Standard Deviations: 0.25618 0.25618 0.25618 0.25618 0.25618 \n\nsummary(modelAR1)$AIC\n\n[1] -250.4103\n\n\nNote that the AIC of our AR[1] model is lower by about 45 than the random intercept model; clearly far superior because it is getting nearby residuals being more correlated, while the random intercept model does not do this. Also see the banding structure of the residual correlation matrix.\n\n\n\n51.3.3 Random slopes\nIn theory, a random slopes model could be done with gls as well as with lme by building the final residual matrices as a function of the random slope parameters; in practice, it’s much more practical just to do it as a hierarchical model with lme:\n\nmodelRS = lme(ATTIT ~ 1 + AGE, \n              data=nys1,\n              random=~AGE|ID )\n\nWe have separated our fixed and random components with lme(). We first include a formula with only fixed effects, and then give a right-side-only formula with terms similar to what you’d put in parentheses with lmer() for the random effects.\nOur results:\n\nsummary(modelRS)\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n       AIC       BIC   logLik\n  -310.125 -280.2334 161.0625\n\nRandom effects:\n Formula: ~AGE | ID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.51024132 (Intr)\nAGE         0.05038614 -0.98 \nResidual    0.16265429       \n\nFixed effects:  ATTIT ~ 1 + AGE \n                 Value  Std.Error  DF  t-value p-value\n(Intercept) -0.5133250 0.05834087 839 -8.79872       0\nAGE          0.0646849 0.00492904 839 13.12323       0\n Correlation: \n    (Intr)\nAGE -0.981\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.87852414 -0.55971196 -0.07521191  0.57495075  3.45648134 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\ngetVarCov(modelRS,type=\"marginal\", individual=3)\n\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.039649 0.015922 0.018650 0.021379 0.024108\n2 0.015922 0.047646 0.026457 0.031725 0.036992\n3 0.018650 0.026457 0.060720 0.042070 0.049876\n4 0.021379 0.031725 0.042070 0.078872 0.062760\n5 0.024108 0.036992 0.049876 0.062760 0.102100\n  Standard Deviations: 0.19912 0.21828 0.24641 0.28084 0.31953 \n\nsummary(modelRS)$AIC\n\n[1] -310.125\n\n\nThe first thing to note is the residual covariance matrix comes from the structure of the random intercept and random slope. If you squint hard enough at it, you can begin to see the linear structures in its diagonal and off-diagonal elements. If you graphed it, those structures would jump out more clearly. But in practice, it’s much easier to think of things in terms of the hierarchical model, not in terms of linear structures in a covariance matrix.\nNote also that the AIC has dropped by another 60 points or so; we’re continuing to improve the model.\nAlso note that this is just using a different package to fit the exact same model we would fit using lmer; so far we haven’t taken advantage of the lme command’s additional flexibility.\n\n\n51.3.4 Random slopes with heteroskedasticity\nRelaxing the homoskedasticity assumption in the random slopes model leaves us a bit in between worlds. We’re not fully into the world of GLS, because there are still random effects; but we’re not fully in the world of hierarchical models because there is structure in the residuals within groups. We’ll talk more about this compromise below; for now, let’s just do it.\n\nmodelRSH = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~AGE|ID,\n              weights=varIdent(form=~1|agefac) )\n\nThe key line is the varIdent line: we are saying each age factor level gets its own weight (rescaling) of the residuals—this is heteroskedasticity. In particular, the above says our residual variance will be weighted by a weight for each age factor, so each age level effectively gets its own variance. This is where these models start to get a bit exciting—we have random slopes, and then heteroskedastic residuals (homoskedastic for any given age level), all together. Our fit model:\n\nsummary(modelRSH)\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -312.5801 -262.7608 166.2901\n\nRandom effects:\n Formula: ~AGE | ID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.57693602 (Intr)\nAGE         0.05431367 -0.979\nResidual    0.14054184       \n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | agefac \n Parameter estimates:\n       11        12        13        14        15 \n1.0000000 1.1956071 1.3095864 1.1255177 0.9802311 \nFixed effects:  ATTIT ~ AGE \n                 Value  Std.Error  DF   t-value p-value\n(Intercept) -0.4929012 0.05715889 839 -8.623351       0\nAGE          0.0631404 0.00483385 839 13.062122       0\n Correlation: \n    (Intr)\nAGE -0.981\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.9163540 -0.5498217 -0.0758348  0.5482942  3.2370312 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\n\nNote how we have 5 parameter estimates for the residuals, listed under agefac. It appears as if we have more variation in age 13 than other ages. Age 11, the baseline, is 1.0; it is our reference scaling. These numbers are all scaling the overall residual variance parameter \\(\\sigma^2\\) of \\(0.1405^2\\).\nFor looking at the covariance structure of the residuals, we use getVarCov() again:\n\nmyVarCov = getVarCov(modelRSH,type=\"marginal\", individual=3)\nmyVarCov\n\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.034915 0.016947 0.018731 0.020516 0.022300\n2 0.016947 0.049916 0.026415 0.031150 0.035884\n3 0.018731 0.026415 0.067975 0.041784 0.049468\n4 0.020516 0.031150 0.041784 0.077440 0.063052\n5 0.022300 0.035884 0.049468 0.063052 0.095615\n  Standard Deviations: 0.18685 0.22342 0.26072 0.27828 0.30922 \n\n\nWe get lists of matrices back from our call. We can convert any one to a correlation matrix:\n\ncov2cor(myVarCov[[1]])\n\n          1         2         3         4         5\n1 1.0000000 0.4059446 0.3844935 0.3945453 0.3859525\n2 0.4059446 1.0000000 0.4534860 0.5010159 0.5194173\n3 0.3844935 0.4534860 1.0000000 0.5759089 0.6136046\n4 0.3945453 0.5010159 0.5759089 1.0000000 0.7327497\n5 0.3859525 0.5194173 0.6136046 0.7327497 1.0000000\n\n\nNo amount of squinting will show the structure in the original covariance matrix. But when you convert to a correlation matrix, you can again squint and begin to see the linear structures in its diagonal and off-diagonal elements. The same comment as above still applies: in practice, it’s much easier to think of things in terms of the hierarchical model, and only read the diagonals of the covariance matrix.\nWe can also get our AIC:\n\nsummary(modelRSH)$AIC\n\n[1] -312.5801\n\n\nThe AIC has dropped by only another 2.5 points or so; that corresponds to the idea that if one of these two models were exactly true, the odds are about \\(e^{2.5/2}\\cong 3.5\\) in favor of the more complex model. Aside from the fact that that premise is silly – we are pretty sure that neither of these models is the exact truth; and in that case, something like BIC would probably be better than AIC – those odds are also pretty weak; the simpler model is probably better here.\nHere’s the reported BICs, by the way: -280.2334145 for the homoskedastic one, and -262.7607678 for the heteroskedastic. As we expected, the simpler model wins that fight. (Though what \\(N\\) to use for BIC is sometimes not obvious with hierarchical models, so you can’t trust those numbers too much; see the unit on AIC and BIC and model building.)\n\n\n51.3.5 Fully unrestricted model\nOK, let’s go whole hog, and fit the unrestricted model. Again, this means leaving the world of hierarchical models and using gls.\n\nmodelUnrestricted = gls(ATTIT ~ AGE, \n               data=nys1,\n               correlation=corSymm(form=~1|ID),\n               weights=varIdent(form=~1|agefac) )\n\n\nsummary(modelUnrestricted)\n\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n       AIC       BIC  logLik\n  -319.262 -234.5691 176.631\n\nCorrelation Structure: General\n Formula: ~1 | ID \n Parameter estimate(s):\n Correlation: \n  1     2     3     4    \n2 0.458                  \n3 0.372 0.511            \n4 0.441 0.437 0.663      \n5 0.468 0.443 0.597 0.764\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | agefac \n Parameter estimates:\n      11       12       13       14       15 \n1.000000 1.118479 1.414269 1.522510 1.560074 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.4557090 0.05465564 -8.337822       0\nAGE          0.0597274 0.00458344 13.031145       0\n\n Correlation: \n    (Intr)\nAGE -0.979\n\nStandardized residuals:\n         Min           Q1          Med           Q3          Max \n-1.482606297 -0.809004080 -0.006791942  0.840804584  4.082258054 \n\nResidual standard error: 0.1903187 \nDegrees of freedom: 1079 total; 1077 residual\n\n\nAnd our residual structure:\n\nmyvc = getVarCov(modelUnrestricted,type=\"marginal\", individual=3)\nmyvc\n\nMarginal variance covariance matrix\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 0.036221 0.018541 0.019071 0.024335 0.026466\n[2,] 0.018541 0.045313 0.029251 0.026924 0.027989\n[3,] 0.019071 0.029251 0.072448 0.051703 0.047736\n[4,] 0.024335 0.026924 0.051703 0.083962 0.065764\n[5,] 0.026466 0.027989 0.047736 0.065764 0.088156\n  Standard Deviations: 0.19032 0.21287 0.26916 0.28976 0.29691 \n\n\nAnd AIC:\n\nAIC( modelUnrestricted )\n\n[1] -319.262\n\n\nThis unrestricted covariance and correlation matrices have the same structures discussed in the book and in class. The AIC has improved by another 6 or 7 points; that’s marginally “significant”, but in practice probably not substantial enough to make up for the massive loss of interpretability. The lesson we should take from that is that there’s not a whole lot of room for improvement just by tinkering with the residual covariance structure; if we want a much better model, we would have to add new fixed or random effects; perhaps other covariates or perhaps a quadratic term in time.",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>An overview of complex error structures</span>"
    ]
  },
  {
    "objectID": "complex_error.html#having-both-ar1-and-random-slopes",
    "href": "complex_error.html#having-both-ar1-and-random-slopes",
    "title": "51  An overview of complex error structures",
    "section": "51.4 Having both AR[1] and Random Slopes",
    "text": "51.4 Having both AR[1] and Random Slopes\nLet’s look at an AR1 residual structure along with some covariates in our main model. The following has AR[1] and also a random intercept and slope:\n\nnys1$AGE11 = nys1$AGE - 11\nctrl &lt;- lmeControl(opt='optim');\nmodel1 = lme(fixed=ATTIT ~ AGE11 + EXPO + FEMALE + MINORITY + log(INCOME + 1), \n              data=nys1,\n              random=~1 + AGE11|ID,\n              correlation=corAR1(),\n             control = ctrl )\n\nsummary(model1)\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n       AIC       BIC   logLik\n  -444.203 -389.4427 233.1015\n\nRandom effects:\n Formula: ~1 + AGE11 | ID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.07999781 (Intr)\nAGE11       0.03332789 0.718 \nResidual    0.16897903       \n\nCorrelation Structure: AR(1)\n Formula: ~1 | ID \n Parameter estimate(s):\n      Phi \n0.1868886 \nFixed effects:  ATTIT ~ AGE11 + EXPO + FEMALE + MINORITY + log(INCOME + 1) \n                      Value  Std.Error  DF   t-value p-value\n(Intercept)      0.26483469 0.04070122 838  6.506800  0.0000\nAGE11            0.04972825 0.00463402 838 10.731129  0.0000\nEXPO             0.31452987 0.02489980 838 12.631823  0.0000\nFEMALE          -0.01661080 0.02027639 235 -0.819219  0.4135\nMINORITY        -0.06296592 0.02676263 235 -2.352755  0.0195\nlog(INCOME + 1) -0.00943584 0.02359588 235 -0.399893  0.6896\n Correlation: \n                (Intr) AGE11  EXPO   FEMALE MINORI\nAGE11           -0.123                            \nEXPO            -0.064 -0.225                     \nFEMALE          -0.181 -0.038  0.171              \nMINORITY        -0.491  0.008  0.011 -0.030       \nlog(INCOME + 1) -0.923 -0.004  0.084 -0.054  0.407\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.61795250 -0.58685208 -0.08121038  0.57649581  2.82037550 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\n\nIn order to get this model to converge, we had to use the lmeControl command above; without it, the model doesn’t converge due to not reaching a max in the given number of iterations. The lmeControl with optim apparently turns up the juice so it converges without complaint.\nLet’s compare our fit model to the same model without AR1 correlation\n\nmodel1simple = lme(fixed=ATTIT ~ AGE11 + EXPO + FEMALE + MINORITY + log(INCOME + 1), \n             data=nys1,\n             random=~1 + AGE11|ID )\nscreenreg( list( AR=model1, noAR=model1simple ) )\n\n\n=========================================\n                 AR           noAR       \n-----------------------------------------\n(Intercept)         0.26 ***     0.26 ***\n                   (0.04)       (0.04)   \nAGE11               0.05 ***     0.05 ***\n                   (0.00)       (0.00)   \nEXPO                0.31 ***     0.32 ***\n                   (0.02)       (0.02)   \nFEMALE             -0.02        -0.02    \n                   (0.02)       (0.02)   \nMINORITY           -0.06 *      -0.06 *  \n                   (0.03)       (0.03)   \nlog(INCOME + 1)    -0.01        -0.01    \n                   (0.02)       (0.02)   \n-----------------------------------------\nAIC              -444.20      -436.80    \nBIC              -389.44      -387.02    \nLog Likelihood    233.10       228.40    \nNum. obs.        1079         1079       \nNum. groups: ID   239          239       \n=========================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nThe AR1 model has a notably lower AIC and thus is significantly better:\n\nanova( model1simple, model1 )\n\n             Model df       AIC       BIC   logLik   Test  L.Ratio p-value\nmodel1simple     1 10 -436.8027 -387.0206 228.4014                        \nmodel1           2 11 -444.2030 -389.4427 233.1015 1 vs 2 9.400292  0.0022\n\n\nAutoregression involves only a single extra parameter–the autoregressive correlation coefficient.\nOur hybrid model is actually kind of mixed up, conceptually. We allowed a random slope on age, and also an autoregressive component by age. Thus, we effectively allowed the covariance matrix to vary in two different ways, at two different levels of our modeling.\nIn fact, as we’ve seen in class, any random effects, whether they be on slope or intercept, are actually equivalent to certain ways of varying the variance-covariance matrix of the residuals within each group. For instance, random intercepts are equivalent to compound symmetry. Thus, by including both random intercepts and AR1 correlation in the above model, we’ve effectively fit a model that allows any covariance matrix that can be expressed as a sum of a random slope covariance matrix (with 2 parameters plus a scaling factor) and an AR1 covariance matrix (with 1 parameter plus a scaling factor). That makes 5 degrees of freedom total for our covariance matrix. This is many fewer than the 15 for a fully unconstrained matrix, for comparison.\nConceptually this model is nice: people have linear growth trends, but vary around those growth trends in an autoregressive way.",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>An overview of complex error structures</span>"
    ]
  },
  {
    "objectID": "complex_error.html#the-kitchen-sink-building-complex-models",
    "href": "complex_error.html#the-kitchen-sink-building-complex-models",
    "title": "51  An overview of complex error structures",
    "section": "51.5 The Kitchen sink: building complex models",
    "text": "51.5 The Kitchen sink: building complex models\nWhich brings us to the next point: how do you actually use this stuff in practice? Ideally, you’d like both the interpretability (and robustness against MAR missingness) of hierarchical models, along with the ability to add additional residual structure such as AR[1] and/or heteroskedastic residuals. The good news is, you can get both. The bad news is, there’s a bit of a potential for bias due to overfitting.\nFor instance, imagine you use both random effects and AR[1]. Say that for a given subject you have 5 time points, and all of them are above the values you would have predicted based on fixed effects alone. That might be explained by an above-average random effect, or by a set of correlated residuals that all came in high. Whichever one of these is the “true” explanation, the MLE will tend to parcel it out between the two. This can lead to downward bias in variance and/or correlation parameter estimates, especially with small numbers of observations per subject–the variation gets pushed into just assuming the residuals are correlated due to the auto-regressive structure.\nStill, as long as your focus is on location parameters such as true means or slopes, having hybrid models can be a good way to proceed. Let’s explore this by first fitting a “kitchen sink” model for this data, in which we use all available covariates; and seeing how adding heteroskedasticity, AR[1] structure, or both changes it (or doesn’t).\nWhat do we want in this “kitchen sink” model? Let’s first fit a very simple random intercept model with fixed effects for gender, minority status, “exposure”, and log(income), to see which of these covariates to focus on. We use the lmerTest package to get some early \\(p\\)-values for these fixed effects.\n\nmodelKS0 = lmerTest::lmer(ATTIT ~ FEMALE + MINORITY + log(INCOME + 1) + EXPO + (1|ID), data=nys1)\nsummary(modelKS0, correlation=FALSE)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ATTIT ~ FEMALE + MINORITY + log(INCOME + 1) + EXPO + (1 | ID)\n   Data: nys1\n\nREML criterion at convergence: -265.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1840 -0.5922 -0.0797  0.6043  2.6319 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.01756  0.1325  \n Residual             0.03444  0.1856  \nNumber of obs: 1079, groups:  ID, 239\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)      3.480e-01  4.195e-02  2.301e+02   8.297    9e-15 ***\nFEMALE          -1.835e-02  2.094e-02  2.327e+02  -0.876   0.3819    \nMINORITY        -5.698e-02  2.789e-02  2.279e+02  -2.043   0.0422 *  \nlog(INCOME + 1)  2.102e-03  2.449e-02  2.272e+02   0.086   0.9317    \nEXPO             4.516e-01  2.492e-02  1.041e+03  18.122   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n(The correlation=FALSE shortens the printout.)\nApparently, MINORITY and EXPO are the covariates with significant effects; minority status is correlated with a lower tolerance for deviance, while “deviant” friends are of course correlated positively with tolerance of deviance. Let’s build a few hierarchical models including these in various specifications (can you identify what models are what? Some of these models are not necessarily good choices). We first center our age so we have meaningful intercepts.\n\nnys1$age13 = nys1$AGE - 13\n\nmodelKS1 = lme(ATTIT ~ MINORITY + age13,\n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS2 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS3 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~EXPO|ID )\n\nmodelKS4 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID )\n\nAnd now we examine them:\n\nlibrary( texreg )\nscreenreg( list( modelKS1, modelKS2, modelKS3, modelKS4 ))\n\n\n===================================================================\n                 Model 1      Model 2      Model 3      Model 4    \n-------------------------------------------------------------------\n(Intercept)         0.31 ***     0.31 ***     0.29 ***     0.34 ***\n                   (0.01)       (0.01)       (0.01)       (0.01)   \nMINORITY           -0.05 *      -0.05 *      -0.04        -0.06 *  \n                   (0.02)       (0.02)       (0.03)       (0.03)   \nage13               0.06 ***     0.06 ***     0.05 ***     0.05 ***\n                   (0.00)       (0.00)       (0.00)       (0.00)   \nEXPO                                                       0.37 ***\n                                                          (0.02)   \n-------------------------------------------------------------------\nAIC              -374.18      -374.18      -312.13      -394.06    \nBIC              -324.37      -324.37      -277.27      -364.18    \nLog Likelihood    197.09       197.09       163.07       203.03    \nNum. obs.        1079         1079         1079         1079       \nNum. groups: ID   239          239          239          239       \n===================================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nOK, Number 4 seems like a pretty good model. Let’s see how much it improves when we add AR[1]:\n\nmodelKS5 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID) )\nAIC( modelKS5 )\n\n[1] -433.5943\n\nfixef( modelKS4 )\n\n(Intercept)    MINORITY       age13        EXPO \n 0.34054593 -0.05606947  0.04830698  0.36778951 \n\nfixef( modelKS5 )\n\n(Intercept)    MINORITY       age13        EXPO \n 0.34083507 -0.05704474  0.04733879  0.35207989 \n\n\nNote that the estimates for all the effects are essentially unchanged. However, the AIC is almost 40 points better. Also, because the model has done a better job explaining residual variance, the \\(p\\)-value for the coefficient on MINORITY has dropped from 0.032 to 0.029, as we can see on the summary display below. This is not a large drop, but a noticeable one:\n\nsummary( modelKS5 )\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -433.5943 -398.7338 223.7972\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept)  Residual\nStdDev:   0.1186009 0.1864592\n\nCorrelation Structure: ARMA(1,0)\n Formula: ~AGE | ID \n Parameter estimate(s):\n     Phi1 \n0.3212696 \nFixed effects:  ATTIT ~ MINORITY + age13 + EXPO \n                 Value   Std.Error  DF   t-value p-value\n(Intercept)  0.3408351 0.011858053 838 28.742920   0.000\nMINORITY    -0.0570447 0.025968587 237 -2.196683   0.029\nage13        0.0473388 0.004523745 838 10.464512   0.000\nEXPO         0.3520799 0.024451130 838 14.399330   0.000\n Correlation: \n         (Intr) MINORI age13 \nMINORITY -0.457              \nage13    -0.013  0.010       \nEXPO      0.006 -0.001 -0.224\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.82097989 -0.65136103 -0.08846076  0.61819746  2.77303796 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\n\nIs any of this drop in the \\(p\\)-value due to overfitting? Given the size of the change in AIC, it seems doubtful that that’s a significant factor.\nLet’s try including heteroskedasticity, without AR[1]:\n\nmodelKS6 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS6 )\n\n[1] -389.5696\n\n\nThis did not improve AIC in this case, so we can avoid looking at this model further.\nFor completeness, let’s look at a model with both AR(1) and heteroskedasticity:\n\nmodelKS7 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID),\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS7 )\n\n[1] -431.1943\n\n\nAgain, no improvement. So we settle with our AR[1] model with a random intercept to get overall level of a student.",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>An overview of complex error structures</span>"
    ]
  },
  {
    "objectID": "cluster_demo.html",
    "href": "cluster_demo.html",
    "title": "52  Walk-through of calculating robust standard errors",
    "section": "",
    "text": "52.1 Robust errors (no clustering)\nThe (no clustering, ordinary) linear regression model assumes that\n\\[y = X\\beta + \\varepsilon\\]\nwith the \\(\\varepsilon\\)’s independently and identically normally distributed with variance \\(\\sigma^2\\). Here \\(\\beta\\) is a column vector of regression coefficients, \\((\\beta_0, \\beta_1)\\) in our example. \\(y\\) is a vector of the outcomes and \\(\\varepsilon\\) is a vector of the residuals. \\(X\\) is a \\(n\\) by \\(p\\) matrix referred to as the model matrix (p is the number of predictors, including the intercept). In this example, the first column of the matrix is all 1’s, for the intercept, and the second column is each person’s value for ses. The third is each person’s value for sector (which will be the same for all students in a single school).\ndat = read.spss( \"data/hsb1.sav\", to.data.frame=TRUE )\nsdat = read.spss( \"data/hsb2.sav\", to.data.frame=TRUE )\ndat = merge( dat, sdat, by=\"id\", all.x=TRUE )\ndat = dat[ c( \"id\", \"mathach\", \"ses\", \"sector\" ) ]\ndat$id &lt;- factor( dat$id ) ### make the school variable a factor\nhead( dat )\n\n    id mathach    ses sector\n1 1224   5.876 -1.528      0\n2 1224  19.708 -0.588      0\n3 1224  20.349 -0.528      0\n4 1224   8.781 -0.668      0\n5 1224  17.898 -0.158      0\n6 1224   4.583  0.022      0\nMaking a model matrix from a regression\nX &lt;- model.matrix( mathach ~ ses + sector, data = dat )\nhead( X )\n\n  (Intercept)    ses sector\n1           1 -1.528      0\n2           1 -0.588      0\n3           1 -0.528      0\n4           1 -0.668      0\n5           1 -0.158      0\n6           1  0.022      0\n\ny &lt;- dat$mathach\nhead( y )\n\n[1]  5.876 19.708 20.349  8.781 17.898  4.583\nWith these assumptions, our estimate for \\(\\beta\\) using the OLS criterion is \\(\\hat{\\beta} = (X^TX)^{-1}X^Ty\\). We can calculate this directly with R.\nsolve(t(X) %*% X) %*% t(X) %*% y ##(X'X)^{-1}X'y\n\n                 [,1]\n(Intercept) 11.793254\nses          2.948558\nsector       1.935013\nCompare with lm: they are the same!\nmod = lm(mathach ~ ses + sector, data = dat)\nmod \n\n\nCall:\nlm(formula = mathach ~ ses + sector, data = dat)\n\nCoefficients:\n(Intercept)          ses       sector  \n     11.793        2.949        1.935\nWe can also estimate standard errors for the coefficients by taking \\(\\sqrt{\\hat{\\sigma}^2diag((X^TX)^{-1})}\\).\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\npreds &lt;- X %*% beta_hat\nresids &lt;- y - preds\nsigma_2_hat &lt;- sum(resids^2)/(nrow(X)-3) ### estimate of the residual variance\nsqrt(sigma_2_hat * diag(solve(t(X) %*% X))) ### using the matrix algebra\n\n(Intercept)         ses      sector \n 0.10610213  0.09783058  0.15249341\nAgain, compare:\nlibrary( arm )\ndisplay( mod ) ### same results\n\nlm(formula = mathach ~ ses + sector, data = dat)\n            coef.est coef.se\n(Intercept) 11.79     0.11  \nses          2.95     0.10  \nsector       1.94     0.15  \n---\nn = 7185, k = 3\nresidual sd = 6.35, R-Squared = 0.15\nBut notice that this assumes that the residuals have a single variance, \\(\\sigma^2\\). Frequently this assumption is implausible, in which case the standard errors we derive may not be correct. It would be useful to have a way to derive standard errors which does not require us to assume that the residuals are homoscedastic. This is where heteroscedasticity-robust standard errors, or Huber-White standard errors, come in. Huber-White standard errors are asymptotically correct, even if the residual variance is not constant at all values of the predictor.\nThe basic idea behind Huber-White standard errors is that we let each individual residual serve as an estimate of the variance of the residuals at that value of the predictors. If we let \\(V = (X^TX)^{-1},\\) \\(N\\) be the number of observations, and \\(K\\) be the number of predictors, including the intercept, then the formula for the standard errors is\n\\[ SE^2 = \\frac{N}{N-K} \\cdot diag\\left( V \\cdot \\left( \\sum X_i X_i^T \\varepsilon_i^2 \\right) \\cdot V\\right) \\]\nThis is called a sandwich estimator, where \\(V\\) is the bread and \\(\\sum X_i X_i^T \\varepsilon_i^2\\) (which is a \\(K\\) by \\(K\\) matrix) is the meat. Below, we implement this in R.\nN &lt;- nrow(dat) ### number of observations\nK &lt;- 3 ### number of regression coefficients, including the intercept\nV &lt;- solve(t(X) %*% X) ### the bread\nV\n\n              (Intercept)           ses        sector\n(Intercept)  2.796108e-04  3.460078e-05 -0.0002847979\nses          3.460078e-05  2.377141e-04 -0.0000702375\nsector      -2.847979e-04 -7.023750e-05  0.0005775742\n\nmeat &lt;- matrix(0, nrow = K, ncol = K) ### we'll build the meat as we go, iterating over the \n                                      ### individual rows\nfor(i in 1:nrow(dat)){\n  this_point &lt;- X[i, ] %*% t(X[i, ]) * resids[i]^2 ### the contribution of this particular \n                                                   ### point\n  meat &lt;- meat + this_point ### take the current meat, and add this point's contribution\n}\nmeat\n\n     (Intercept)        ses     sector\n[1,]  289161.019  -3048.176 133136.299\n[2,]   -3048.176 159558.729   9732.201\n[3,]  133136.299   9732.201 133136.299\n\nSEs = sqrt(diag(N/(N-K) * V %*% meat %*% V)) ### standard errors\nSEs\n\n(Intercept)         ses      sector \n 0.11021454  0.09487279  0.15476724\nNotice that the estimated standard errors haven’t changed much, so whatever heteroscedasticity is present in this association doesn’t seem to be affecting them.\nCombining the above steps in a tidy bit of code gives:\nmod &lt;- lm(mathach ~ ses + sector, data = dat)\nresids = resid( mod )\n\nX &lt;- model.matrix(mathach ~ ses + sector, data = dat)\n\nV &lt;- solve(t(X) %*% X) ### the bread\nvcov_hw = V %*% t(X) %*% diag(resids^2) %*% X %*% V\n\nvcov_hw\n\n             (Intercept)          ses       sector\n(Intercept)  0.012142174  0.001957716 -0.012535538\nses          0.001957716  0.008997088 -0.003992666\nsector      -0.012535538 -0.003992666  0.023942897\n\nsqrt(diag(vcov_hw)) ### standard errors\n\n(Intercept)         ses      sector \n 0.11019153  0.09485298  0.15473493 \n\nsqrt( diag( vcov( mod ) ) )\n\n(Intercept)         ses      sector \n 0.10610213  0.09783058  0.15249341",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Walk-through of calculating robust standard errors</span>"
    ]
  },
  {
    "objectID": "cluster_demo.html#robust-errors-no-clustering",
    "href": "cluster_demo.html#robust-errors-no-clustering",
    "title": "52  Walk-through of calculating robust standard errors",
    "section": "",
    "text": "52.1.1 R Packages to do all this for you\nThere is an R package to do all of this for us. The following gives us the “Variance Covariance” matrix:\n\nlibrary(sandwich)\nvc &lt;- vcovHC( mod, type = \"HC0\")\nprint( vc, digits=3 )\n\n            (Intercept)      ses   sector\n(Intercept)     0.01214  0.00196 -0.01254\nses             0.00196  0.00900 -0.00399\nsector         -0.01254 -0.00399  0.02394\n\n\nThe square root of the diagonal are our standard errors\n\nsqrt( diag( vc ) )\n\n(Intercept)         ses      sector \n 0.11019153  0.09485298  0.15473493 \n\n\nThey are what we hand-calculated above (up to some rounding error). Observe how the differences are all very close to zero:\n\nsqrt( diag( vc ) ) - SEs\n\n  (Intercept)           ses        sector \n-2.301170e-05 -1.980850e-05 -3.231386e-05 \n\n\nWe can use them for testing as follows\n\nlibrary( lmtest )\ncoeftest( mod, vcov. = vc )\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.110192 107.025 &lt; 2.2e-16 ***\nses          2.948558   0.094853  31.086 &lt; 2.2e-16 ***\nsector       1.935013   0.154735  12.505 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n(Note the weird “.”. I don’t know why it is part of the name.)\nIn fact, these packages play well together, so you can tell lmtest to use the vcovHC function as follows:\n\ncoeftest( mod, vcov. = vcovHC )\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.110237 106.981 &lt; 2.2e-16 ***\nses          2.948558   0.094913  31.066 &lt; 2.2e-16 ***\nsector       1.935013   0.154801  12.500 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAll this is well and good, but everything we have done so far is WRONG because we have failed to account for the clustering of students within schools. Huber-White (Sandwich) corrections only deal with heteroskedasticity, not clustering. We extend these ideas to do clustering next.",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Walk-through of calculating robust standard errors</span>"
    ]
  },
  {
    "objectID": "cluster_demo.html#cluster-robust-standard-errors",
    "href": "cluster_demo.html#cluster-robust-standard-errors",
    "title": "52  Walk-through of calculating robust standard errors",
    "section": "52.2 Cluster Robust Standard Errors",
    "text": "52.2 Cluster Robust Standard Errors\nThe next step is to get standard errors which allow the residuals to be correlated within clusters and to have non-0 means within clusters (which violates the assumption of independence of residuals). The math here is harder to explain. We start by calculating \\(X*\\varepsilon\\), multiplying each row in \\(X\\) by the associated residual. Then we take the column sum of \\(X\\) within each cluster. This is easiest to understand for the intercept column, where the sum is simply equal to the sum of the residuals in that cluster. If all of the residuals in a cluster are large and positive (or large and negative), then this sum will be very large; if the residuals are close to mean 0 in a cluster, the sum will be small. We then bind the results into a \\(M\\) by \\(K\\) matrix, where \\(M\\) is the number of clusters, each row corresponds to a cluster, and each column corresponds to a coefficient, which we’ll call \\(U\\). This is the meat which we sandwich with \\(V\\). Finally, we take\n\\[\\sqrt{ diag( \\frac{M}{M-1}\\frac{N-1}{N-K} VU^TUV)}\\]\nwhich gives us estimated standard errors for the regression coefficients.\nThe intuition isn’t so clear here, but notice that the more highly correlated residuals are within clusters (especially clusters with extreme values of the predictors), the larger \\(U^TU\\) will be, and the less precise our estimates.\nHere’s a “by hand” implementation in R.\n\ncluster &lt;- dat$id\nM &lt;- length(unique(cluster))\nweight_mat &lt;- as.vector(resids) * X ### start by calculating for each X predictor values \n                                    ### weighted by the residuals\nhead( weight_mat )\n\n  (Intercept)        ses sector\n1   -1.411858  2.1573194      0\n2    9.648498 -5.6733165      0\n3   10.112584 -5.3394444      0\n4   -1.042618  0.6964687      0\n5    6.570618 -1.0381576      0\n6   -7.275123 -0.1600527      0\n\nu_icept &lt;- tapply(weight_mat[, '(Intercept)'], cluster, sum) ### sum up the weighted \n                                                             ### intercepts in each cluster\nu_ses &lt;- tapply(weight_mat[, 'ses'], cluster, sum) ### sum up the weighted slopes in \n                                                       ### each cluster\nu_sector &lt;- tapply(weight_mat[, 'sector'], cluster, sum)\n\nu &lt;- cbind(u_icept, u_ses, u_sector)\n\n### cluster-robust standard errors\nSE.adj.hand = sqrt((M/(M-1))*((N-1)/(N-K)) * diag(V %*% t(u) %*% u %*% V)) \nSE.adj.hand\n\n(Intercept)         ses      sector \n  0.2031455   0.1279373   0.3171766 \n\n\nThese are a lot higher than before; there’s a lot of within-cluster correlation, and our OLS-based estimated standard errors are unrealistically small.\nYou can use these standard errors in general if you’re not interested in modeling what’s happening at the cluster level and just want to get the right standard errors for your fixed effects.\n\n52.2.1 Using R Packages\nThere is a package that gives you the cluster-robust estimate of the variance-covariance matrix. You can then use this matrix to get your adjusted standard errors:\n\nlibrary( multiwayvcov )\n\nm1 &lt;- lm( mathach ~ ses + sector, data=dat )\nvcov_id &lt;- cluster.vcov(m1, dat$id)\ncoeftest(m1, vcov_id)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.79325    0.20315 58.0532 &lt; 2.2e-16 ***\nses          2.94856    0.12794 23.0469 &lt; 2.2e-16 ***\nsector       1.93501    0.31718  6.1007 1.111e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCompare to if we ignored clustering:\n\ncoeftest( m1 )  ## BAD!!\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.106102 111.150 &lt; 2.2e-16 ***\nses          2.948558   0.097831  30.139 &lt; 2.2e-16 ***\nsector       1.935013   0.152493  12.689 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can look at how much bigger they are:\n\nSE.adj = sqrt( diag( vcov_id ) )\nSE.bad = sqrt( diag( vcov( m1 ) ) )\nSE.adj / SE.bad\n\n(Intercept)         ses      sector \n   1.914623    1.307743    2.079937 \n\n\nMore than 100% bigger for our sector variable and intercept. The ses variable is less so, since it varies within cluster.\nFinally, we check to see that our hand-calculation is the same as the package:\n\nSE.adj.hand - SE.adj\n\n  (Intercept)           ses        sector \n 1.296185e-14 -3.025358e-15 -2.997602e-15 \n\n\nUp to rounding errors, we are the same!\n\n\n52.2.2 Aside: Making your own function\nThe following is code to generate the var-cor matrix more efficiently. For reference (or to ignore):\n\n cl &lt;- function(dat, fm, cluster){\n   attach(dat, warn.conflicts = F)\n   require(sandwich)\n   require(lmtest)\n   M &lt;- length(unique(cluster))\n   N &lt;- length(cluster)\n   K &lt;- fm$rank\n   dfc &lt;- (M/(M-1))*((N-1)/(N-K))\n   uj  &lt;- apply(estfun(fm), 2, function(x) \n                       tapply(x, cluster, sum));\n   vcovCL &lt;- dfc*sandwich(fm, meat=crossprod(uj)/N)\n   coeftest(fm, vcovCL)\n }\n \ncl(dat, mod, dat$id)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.79325    0.20315 58.0532 &lt; 2.2e-16 ***\nses          2.94856    0.12794 23.0469 &lt; 2.2e-16 ***\nsector       1.93501    0.31718  6.1007 1.111e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "MATH DERIVATIONS",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Walk-through of calculating robust standard errors</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Antonakis, John, Nicolas Bastardoz, and Mikko Rönkkö. 2019. “On\nIgnoring the Random Effects Assumption in Multilevel Models: Review,\nCritique, and Recommendations.” Organizational Research\nMethods 24 (2): 443–83. https://doi.org/10.1177/1094428119877457.\n\n\nAsparouhov, Tihomir, and Bengt Muthen. 2006. “Multilevel\nModeling of Complex Survey Data.”\nASA Section on Survey Research Methods, 2718–26.\n\n\nCarle, Adam C. 2009. “Fitting Multilevel Models in Complex Survey\nData with Design Weights: Recommendations.” BMC\nMedical Research Methodology 9 (49): 1–13. https://doi.org/10.1186/1471-2288-9-49.\n\n\nLaukaityte, Inga, and Marie Wiberg. 2018. “Importance of Sampling\nWeights in Multilevel Modeling of International Large-Scale Assessment\nData.” Communications in Statistics-Theory and Methods\n47 (20): 4991–5012.\n\n\nLorah, Julie. 2020. “Estimating a Multilevel Model with Complex\nSurvey Data: Demonstration Using TIMSS.” Journal of Modern\nApplied Statistical Methods 18 (2): 24.\n\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2006. “Multilevel\nModelling of Complex Survey Data.” Journal of the Royal\nStatistical Society, 805–27.",
    "crumbs": [
      "References"
    ]
  }
]